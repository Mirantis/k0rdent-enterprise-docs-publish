{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Mirantis k0rdent Enterprise","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Mirantis k0rdent Enterprise is a Kubernetes-native platform engineering solution that provides open source composability to simplify the creation of use case-specific developer platforms at scale. It delivers the open source k0rdent Distributed Container Management Environment (DCME) via a secure software supply chain, with components fully validated by Mirantis and enhanced with additional Enterprise-class features and 24/7 Enterprise or Fully-Managed Support.</p> <p>Mirantis k0rdent Enterprise (and k0rdent) are based on the premise that:</p> <ul> <li>Kubernetes and its ecosystem are mature and inherently stable.</li> <li>Large scale adoption of Kubernetes means that Mirantis k0rdent Enterprise can run anywhere.</li> <li>Community standards and open source projects ensure support and reduce adoption risk.</li> </ul> <p>The goal of Mirantis k0rdent Enterprise is to provide platform engineers with means to deliver a distributed container management environment (DCME) and enable them to compose unique internal developer platforms (IDP) to support a diverse range of complex modern application workloads.</p> <p>Another way to think of Mirantis k0rdent Enterprise is as a \"super control plane\" designed to ensure the consistent provisioning and lifecycle management of kubernetes clusters and the services that make them useful.</p> <p>In short: Kubernetes clusters at scale, managed centrally, template driven, based on open community driven standards, enabling Golden Paths ... Mirantis k0rdent Enterprise aspires to do all of that.</p> <p>Whether you want to manage Kubernetes clusters on-premises, in the cloud, or a combination of both, Mirantis k0rdent Enterprise provides a consistent way to do it. With full life-cycle management, including provisioning, configuration, and maintenance, Mirantis k0rdent Enterprise is designed to be a repeatable and secure way to manage your Kubernetes clusters in a central location.</p>"},{"location":"#structure-and-history","title":"Structure and History","text":"<p>As noted above, Mirantis k0rdent Enterprise delivers open source k0rdent (with additional features) for Enterprise use, via a tightly-controlled software supply chain and validation process. k0rdent comprises:</p> <ul> <li> <p>k0rdent: the overall project</p> <ul> <li> <p>k0rdent Cluster Manager (KCM)</p> <p>Deployment and life-cycle management of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>k0rdent State Manager (KSM)</p> <p>Installation and life-cycle management of deployed services.</p> <ul> <li>This is currently rolled into kcm, but may be split out in the future</li> <li>ksm leverages Project Sveltos     for an increasing amount of functionality</li> </ul> </li> <li> <p>k0rdent Observability and FinOps (KOF)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul> </li> </ul> <p>There are a few historical names that may show up in the code and in older docs, including:</p> <ul> <li>Project 2A: the original codename of k0rdent, 2A references the hexadecimal 0x2A,    or 42, which encompasses our hopes for the project.</li> <li>HMC or hmc: the original repository name for k0rdent and KCM   development</li> <li>motel: the original repository and codename for KOF</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the Mirantis k0rdent Enterprise Quick Start Guide to get started with a small deployment of Mirantis k0rdent Enterprise.</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>Mirantis k0rdent Enterprise leverages the Cluster API provider ecosystem; the following providers have had <code>ProviderTemplates</code> created and validated, and more are in the works. </p> <ul> <li>AWS</li> <li>Azure</li> <li>Bare Metal</li> <li>vSphere</li> <li>OpenStack</li> <li>GCP</li> </ul> <p>Mirantis k0rdent Enterprise also includes a way to add custom providers, so it's possible to integrate it with other hyperscalers,  or even into an existing local infrastructure.</p>"},{"location":"#development-documentation","title":"Development Documentation","text":"<p>Documentation related to the development process and developer-specific notes is located in the main k0rdent open source repository.</p>"},{"location":"#where-to-go-from-here","title":"Where to go from here","text":"<p>This documentation covers all aspects of administering, using, and contributing to Mirantis k0rdent Enterprise, documentation for Mirantis k0rdent Enterprise AddOns, plus where to get support and services. It includes:</p> <ul> <li>Mirantis k0rdent Enterprise concepts</li> <li>QuickStarts</li> <li>Administrator Guide</li> <li>User Guide</li> <li>Mirantis k0rdent Enterprise Templates Reference</li> <li>Troubleshooting</li> <li>Glossary</li> <li>Appendix</li> </ul> <p>... plus:</p> <ul> <li>Addons - Composable components and solutions, validated by Mirantis, ready for use with Mirantis k0rdent Enterprise</li> <li>Services, Support, and Contact - Enterprise support for Mirantis k0rdent Enterprise</li> </ul>"},{"location":"k0rdent_ai/","title":"Mirantis k0rdent Enterprise and AI","text":"<p>Open source k0rdent is the upstream source for a growing range of fully-validated and fully-supported Mirantis multi-cluster Kubernetes solutions for platform engineers and enterprises.</p>"},{"location":"k0rdent_ai/#mirantis-k0rdent-enterprise","title":"Mirantis k0rdent Enterprise","text":"<p>Mirantis k0rdent Enterprise is a Kubernetes-native platform engineering solution that provides open source composability to simplify the creation of use case-specific developer platforms at scale. Mirantis k0rdent Enterprise delivers the open source k0rdent Distributed Container Management Environment (DCME) via a secure software supply chain, with components fully validated by Mirantis and enhanced with additional Enterprise-class features and 24/7 Enterprise Support.</p>"},{"location":"k0rdent_ai/#mirantis-k0rdent-ai","title":"Mirantis k0rdent AI","text":"<p>Mirantis k0rdent AI is an enterprise-grade AI lifecycle management solution for Platform Engineers that accelerates the delivery of AI-powered applications into production at scale -- leveraging the power of Mirantis k0rdent Enterprise. By streamlining the development and deployment of AI applications and their machine learning models, Mirantis k0rdent AI reduces toil for application developers and data scientists, so they can focus on delivering continuous value. Mirantis k0rdent AI is based on open source and open standards, and enables Platform Engineers to easily build composable AI-IDPs tailored to their workload requirements, with a broad selection of infrastructure, services, and AI pipeline components from a partner ecosystem of both open source and proprietary systems and tooling. </p>"},{"location":"addons/","title":"AddOns","text":"<p>Mirantis k0rdent Enterprise is composable by design. The open source community around k0rdent has created a host of composable components that can be leveraged by Mirantis k0rdent Enterprise users -- available through the k0rdent Catalog. Mirantis and partners have also created a growing list of complete functional subsystems for Mirantis k0rdent Enterprise, available as AddOns.</p> <p>Several of these AddOns are documented in this section. These include:</p> <ul> <li> <p>Mirantis k0rdent Virtualization (kubevirt) extends Kubernetes to run virtual machine workloads alongside containers. By introducing native resource types for virtual machines, it allows you to manage VMs using the same operational tools and workflows you already use for Kubernetes resources. Features such as live migration and flexible scheduling ensure that virtualized workloads can be maintained with minimal disruption.</p> </li> <li> <p>Ceph provides a unified storage solution that supports block, file, and object storage within a single system. It simplifies the management of persistent volumes by automating key processes like data replication and recovery, thereby reducing the manual overhead typically associated with enterprise storage.</p> </li> <li> <p>StackLight delivers comprehensive observability, integrating a suite of powerful open-source tools. It includes Prometheus for monitoring and alerting, Elasticsearch for scalable log aggregation and search, and Grafana and Kibana for rich data visualization. In Mirantis k0rdent Enterprise, it also provides operators with deep insights into system health, performance metrics, and centralized logging, enabling proactive issue detection, and faster troubleshooting.</p> </li> </ul> <p>Together, these add-ons increase the utility of Mirantis k0rdent Enterprise, enabling you to tailor your cluster with the additional functionality you need\u2014all while keeping the management process straightforward and consistent.</p>"},{"location":"addons/ceph/","title":"Ceph: A Unified Storage Solution","text":"<p>Ceph is an open\u2010source, distributed storage platform that offers unified block, file, and object storage. When run under Kubernetes, it provides scalable, resilient, and self-healing data management using commodity hardware.</p> <p>In the context of Mirantis k0rdent Enterprise, Ceph\u2019s benefits become even clearer. Mirantis k0rdent Enterprise simplifies deploying and managing Ceph on your own clusters. This means you can take advantage of dynamic persistent volume provisioning, unified storage for varied workloads, and robust, fault-tolerant data distribution\u2014all without the need for complex manual setup.</p> <p>By integrating Ceph with Mirantis k0rdent Enterprise, you get a straightforward way to manage stateful applications in Kubernetes while ensuring efficient, high-availability storage across your environment.</p> <p>In Mirantis k0rdent Enterprise, Ceph is installed on a Kubernetes cluster by creating a <code>ServiceTemplate</code> and enabling the service on a <code>ClusterDeployment</code> that represents it.</p>"},{"location":"addons/ceph/ceph-airgap/","title":"Ceph in an Airgapped Environment","text":"<p>If you are operating in an airgapped environment, you cannot use the typical k0rdent Enterprise Catalog installation, but you can install Ceph manually by following these instructions.</p>"},{"location":"addons/ceph/ceph-airgap/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have the following software installed before beginning Ceph installation:</p> <ul> <li>skopeo 1.6.1 or later </li> <li>Offline registry for uploading/downloading images. This offline registry must use HTTPS, and the TLS certificates should be trusted on all nodes. The registry must be accessible from the airgapped environment.</li> </ul>"},{"location":"addons/ceph/ceph-airgap/#preparation","title":"Preparation","text":"<p>Follow these steps to prepare for Ceph installation.</p> <ol> <li> <p>From a non-airgapped system, download the offline Ceph bundle:</p> <pre><code>curl -L https://binary-mirantis-com.s3.amazonaws.com/ceph/bundles/airgap-bundle-ceph-1.0.11.tar.gz -o airgap-bundle-ceph-1.0.12.tar.gz\n</code></pre> </li> <li> <p>Copy the downloaded file to the airgapped environment</p> </li> <li> <p>Upload the images and charts to the registry.</p> <p>On the node with access to the registry, set up the following ENV variables:</p> <pre><code>export REGISTRY_ADDRESS='&lt;registry_address&gt;'\nexport REGISTRY_USERNAME='&lt;username&gt;'\nexport REGISTRY_PASSWORD='&lt;password&gt;'\nexport BUNDLE_NAME='airgap-bundle-ceph-&lt;version&gt;.tar.gz'\n</code></pre> <p>For example:</p> <pre><code>export REGISTRY_ADDRESS='10.98.15.193:5000'\nexport REGISTRY_USERNAME='user'\nexport REGISTRY_PASSWORD='pass'\nexport BUNDLE_NAME='airgap-bundle-ceph-1.0.12.tar.gz'\n</code></pre> </li> <li> <p>Execute the following script:</p> <pre><code>#!/usr/bin/env bash\nset -ex\n\n\nif [[ -z \"$REGISTRY_ADDRESS\" ]]; then\necho \"Must provide REGISTRY_ADDRESS in environment\" 1&gt;&amp;2\nexit 1\nfi\nif [[ -z \"$REGISTRY_USERNAME\" ]]; then\necho \"Must provide REGISTRY_USERNAME in environment\" 1&gt;&amp;2\nexit 1\nfi\nif [[ -z \"$REGISTRY_PASSWORD\" ]]; then\necho \"Must provide REGISTRY_PASSWORD in environment\" 1&gt;&amp;2\nexit 1\nfi\n\n\nif ! [ -x \"$(command -v skopeo)\" ]; then\necho 'Error: skopeo is not installed.' &gt;&amp;2\nexit 1\nfi\n\n\nBUNDLE_NAME=${BUNDLE_NAME:-\"airgap-bundle-ceph.tar.gz\"}\nREGISTRY_PROJECT_PATH=ceph\nREGISTRY_TLS_VERIFY=${REGISTRY_TLS_VERIFY:-\"true\"}\n\n\n# Login to the registry\nskopeo login \"$REGISTRY_ADDRESS\" -u \"$REGISTRY_USERNAME\" -p \"$REGISTRY_PASSWORD\" --tls-verify=$REGISTRY_TLS_VERIFY\n\n\n# Extract the bundle\nmkdir -p ./bundle\ntar -xzf \"$BUNDLE_NAME\" -C ./bundle\n\n# Iterate over bundle artifacts and upload each one using skopeo\nfor archive in $(find ./bundle -print | grep \".tar\"); do\n    # Form the image name from the archive name\n    img=$(basename \"$archive\" | sed 's~\\.tar~~' | tr '&amp;' '/' | tr '@' ':'| cut -d \"/\" -f 3-);\n    echo \"Uploading $img\";\n    # Copy artifact from local oci archive to the registry\n    skopeo copy --dest-tls-verify=$REGISTRY_TLS_VERIFY -q \"oci-archive:$archive\" \"docker://$REGISTRY_ADDRESS/$REGISTRY_PROJECT_PATH/$img\";\ndone;\n\nrm -r ./bundle || true\n</code></pre> </li> </ol>"},{"location":"addons/ceph/ceph-airgap/#installation","title":"Installation","text":"<ol> <li> <p>Setup the Ceph <code>ServiceTemplate</code> from the registry:</p> <pre><code>helm install ceph-controller-1-0-12 oci://${REGISTRY_ADDRESS}/k0rdent-enterprise-catalog/ceph-controller-service-template --version 1.0.12 -n kcm-system \u2013-set helmRepository.url=oci://${REGISTRY_ADDRESS}/ceph\n</code></pre> </li> <li> <p>Edit the <code>ClusterDeployment</code> to enable Ceph as usual, but but add the docker images repository value:</p> <pre><code>...\nspec:\nserviceSpec:\n    services:\n    ...\n    - name: ceph\n    namespace: ceph-lcm-mirantis\n    template: ceph-controller-1-0-12\n    values: |\n        ceph-operator:\n            global:\n                dockerBaseUrl: ${REGISTRY_ADDRESS}\n            rookExtraConfig:\n                csiKubeletPath: /var/lib/k0s/kubelet\n            controllers:\n                cephMaintenance:\n                    enabled: false\n            installNamespaces: false\n</code></pre> </li> <li> <p>From here, you can pick up from step 3 from the usual Ceph installation.</p> </li> </ol>"},{"location":"addons/ceph/ceph-install/","title":"Ceph Install","text":"<p>This document describes the Ceph storage reference architecture for Kubernetes clusters and details how to deploy and manage Ceph on clusters deployed and managed with Mirantis k0rdent Enterprise. Ceph is a distributed storage system that can be deployed on a Kubernetes cluster using OSS Rook. Deploying Ceph with Mirantis k0rdent Enterprise enables its delivery as a <code>ServiceTemplate</code>. Although Ceph is primarily targeted at supporting Mirantis k0rdent Virtualization (Kubevirt) (i.e., providing storage for virtual machines), it is also well suited for providing persistent block and filesystem storage for any Kubernetes workload. Rook Ceph integrates with Kubernetes as a CSI driver and supports the deployment of StorageClasses based on Ceph RBD (block volumes) and CephFS (filesystem volumes) in both Read-Write-Once and Read-Write-Many modes.</p>"},{"location":"addons/ceph/ceph-install/#prerequisites-and-requirements","title":"Prerequisites and Requirements","text":"<ol> <li> <p>Deployed, healthy Kubernetes cluster: In the context of Mirantis k0rdent Enterprise, this is typically a deployed <code>ClusterDeployment</code>.</p> </li> <li> <p>Networking      Two subnets should be defined and configured for Ceph:</p> <ul> <li>Storage Access Subnet(s):    Provides IP addresses (allocated statically via IPAM) for Ceph nodes. Ceph OSD services bind to these addresses and serve access traffic to and from storage clients. This is considered the public network in Ceph terms.</li> <li>Storage Replication Subnet(s):    Provides IP addresses for Ceph nodes that are used for internal replication traffic. This is considered the cluster network in Ceph terms.</li> </ul> <p>For more details, refer to the Ceph Network Configuration Reference.</p> </li> <li> <p>Hardware      Please refer to the official Ceph minimal recommendations for hardware requirements. Note that based on your running applications, the hardware requirements (CPU, RAM, disks) might need to be increased for better performance:  </p> <ul> <li>Minimum Hardware Recommendations </li> <li>Overall Hardware Recommendations</li> </ul> </li> <li> <p>Nodes Count</p> <ul> <li> <p>Ceph MON Nodes:     These nodes host all control daemons including:</p> <ul> <li>Ceph Monitor: Stores the health and log information for the cluster.</li> <li>Ceph Manager: Provides an endpoint for monitoring, orchestration, and plug-in modules.</li> <li>Ceph Object Gateway (RGW) Daemon: Offers a RESTful gateway (S3-compatible or Swift) between applications and the Ceph cluster.</li> <li>Ceph Metadata Server (MDS): Manages file metadata when using CephFS.</li> </ul> <p>A minimum of 3 monitor nodes is required to maintain cluster quorum in production.</p> </li> <li> <p>Ceph OSD Nodes:     These nodes run data daemons (OSDs) which provide the storage capacity of the cluster. Typically, at least 1 OSD per device is deployed. For NVMe devices, it is recommended to run no more than two OSDs per device. By default, Ceph uses a replication factor of 3. If fewer than 3 Ceph OSD daemons are running, the cluster enters a degraded state with restrictions on write operations until the required number of OSDs is restored. For fault tolerance and recovery operations (such as disk or node replacement), it is advisable to have more than 3 Ceph OSD nodes.</p> </li> </ul> </li> </ol>"},{"location":"addons/ceph/ceph-install/#ceph-deployment-on-k0rdent","title":"Ceph Deployment on k0rdent","text":"<p>Deploying Ceph on k0rdent follows these steps:</p> <ol> <li> <p>Ceph is installed from the Ceph docpage on k0rdent Enterprise Catalog. Once it's installed, you can add it to child clusters.</p> </li> <li> <p>Edit the <code>ClusterDeployment</code> to Enable Ceph.</p> <p>Modify the <code>ClusterDeployment</code> to enable Ceph as a service:</p> <pre><code>...\nspec:\n  serviceSpec:\n    services:\n    ...\n    - name: ceph\n      namespace: ceph-lcm-mirantis\n      template: ceph-1-0-3\n      values: |\n        ceph-operator:\n          rookExtraConfig:\n            csiKubeletPath: /var/lib/k0s/kubelet\n          controllers:\n            cephMaintenance:\n              enabled: false\n          installNamespaces: false\n</code></pre> </li> <li> <p>Post-Deployment Steps</p> <p>After the Ceph Rook infrastructure is ready, perform the following tasks:</p> <ol> <li> <p>Enable Snapshot Controller for Mirantis k0rdent Virtualization Integration</p> <p>According to your environment\u2019s guidelines, update the ClusterDeployment\u2019s <code>spec.serviceSpec.services[ceph].values</code> to enable the snapshot-controller. This step should be done after the initial Ceph chart has been deployed.</p> </li> <li> <p>Configure and Apply MiraCeph</p> <p>Create a <code>MiraCeph</code> resource to configure the Ceph cluster. Below is a simple example of a minimal <code>MiraCeph</code> configuration:</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: MiraCeph\nmetadata:\n  name: rook-ceph\n  namespace: ceph-lcm-mirantis\nspec:\n  rookNamespace: rook-ceph\n  dashboard: false\n  network:\n    publicNet: 10.6.0.0/24\n    clusterNet: 10.6.0.0/24\n  hyperconverge:\n    tolerations:\n      mon:\n        rules:\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n          operator: Exists\n      mgr:\n        rules:\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n          operator: Exists\n      mds:\n        rules:\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n          operator: Exists\n      rgw:\n        rules:\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n          operator: Exists\n    services:\n    - name: ceph\n      namespace: ceph-lcm-mirantis\n      template: ceph-1-0-3\n      values: |\n        global:\n          dockerBaseUrl: docker-dev-kaas-local.docker.mirantis.net\n        rookExtraConfig:\n          csiKubeletPath: /var/lib/k0s/kubelet\n        controllers:\n          cephMaintenance:\n            enabled: false\n        installNamespaces: false\n        snapshotController:\n          enabled: true\n  nodes:\n  - name: pr-k0rdent-env-cp-0\n    roles: [ \"mon\", \"mgr\", \"mds\" ]\n    monitorIP: &lt;IP address of ceph public iface from the node&gt;\n  - name: pr-k0rdent-env-cp-1\n    roles: [ \"mon\", \"mgr\", \"mds\" ]\n    monitorIP: &lt;IP address of ceph public iface from the node&gt;\n  - name: pr-k0rdent-env-cp-2\n    roles: [ \"mon\", \"mgr\", \"mds\" ]\n    monitorIP: &lt;IP address of ceph public iface from the node&gt;\n  - name: pr-k0rdent-env-md-8b7t7-2tnxw\n    roles: []\n    monitorIP: &lt;IP address of ceph public iface from the node&gt;\n    devices:\n    - name: vdb\n      config:\n        deviceClass: hdd\n  - name: pr-k0rdent-env-md-8b7t7-nml9d\n    roles: []\n    devices:\n    monitorIP: &lt;IP address of ceph public iface from the node&gt;\n    - name: vdb\n      config:\n        deviceClass: hdd\n  - name: pr-k0rdent-env-md-8b7t7-pz46v\n    roles: []\n    monitorIP: &lt;IP address of ceph public iface from the node&gt;\n    devices:\n    - name: vdb\n      config:\n        deviceClass: hdd\n  pools:\n  - name: block-pool\n    useAsFullName: true\n    deviceClass: hdd\n    default: true\n    replicated:\n      size: 3\n    role: block-pool\n  objectStorage:\n    rgw:\n      name: rgw-store\n      dataPool:\n        deviceClass: hdd\n        replicated:\n          size: 3\n      metadataPool:\n        deviceClass: hdd\n        replicated:\n          size: 3\n      gateway:\n        allNodes: false\n        instances: 3\n        port: 8080\n        securePort: 8443\n      preservePoolsOnDelete: false\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      dataPools:\n      - name: filesystem-pool\n        deviceClass: hdd\n        replicated:\n          size: 3\n      metadataPool:\n        deviceClass: hdd\n        replicated:\n          size: 3\n      metadataServer:\n        activeCount: 1\n        activeStandby: false\n</code></pre> </li> </ol> </li> </ol> <p>References: For further details on network configuration and hardware requirements, please refer to: - Ceph Network Configuration Reference - Ceph Hardware Recommendations \u2013 Minimum - Ceph Hardware Recommendations \u2013 Overall</p>"},{"location":"addons/ceph/ceph-operating/","title":"Ceph Cluster Operations","text":"<p>Each change in the Ceph cluster should be reflected in the <code>MiraCeph</code> specification. By directly editing the <code>MiraCeph</code> object, you ensure that all desired configuration changes are applied consistently across the Ceph cluster managed by Mirantis k0rdent Enterprise.</p> <p>To update the <code>MiraCeph</code> object, use:</p> <pre><code>kubectl edit miraceph -n ceph-lcm-mirantis\n</code></pre> <p>After saving changes, you can verify the current state of the <code>MiraCeph</code> object. For example, a non-ready <code>MiraCeph</code> object might appear as follows:</p> <pre><code>kubectl get miraceph -n ceph-lcm-mirantis\nNAME          AGE   VALIDATION   PHASE       CLUSTER VERSION   MESSAGE                \ncephcluster   20h   Succeed      Deploying   v18.2.4           Ceph cluster configuration apply is in progress: ceph object storage\n</code></pre> <p>In contrast, an applied <code>MiraCeph</code> object with no pending configuration changes would look like this:</p> <pre><code>kubectl get miraceph -n ceph-lcm-mirantis\nNAME          AGE     VALIDATION   PHASE   CLUSTER VERSION   MESSAGE                  \ncephcluster   2d12h   Succeed      Ready   v18.2.4           Ceph cluster configuration successfully applied\n</code></pre> <p>This clear status reporting allows operators to quickly assess whether the configuration has been fully applied or if further action is needed. The separation of phases (for example, \"Deploying\" vs. \"Ready\") helps in pinpointing any issues during the update process.</p>"},{"location":"addons/ceph/ceph-operating/#checking-cluster-health-and-secrets","title":"Checking Cluster Health and Secrets","text":"<p>Maintaining visibility into the cluster's operational health is critical. Operators can check the overall health of the Ceph cluster by querying the <code>MiraCeph</code>Health object:</p> <pre><code>kubectl get miracephhealth -n ceph-lcm-mirantis\nNAME          AGE     STATE   LAST CHECK             LAST UPDATE \ncephcluster   6d17h   Ready   2024-07-30T11:05:14Z   2024-07-30T11:05:14Z\n</code></pre> <p>Similarly, the status of secrets--used for securely storing critical credentials--is accessible via the <code>MiraCephSecret</code> object:</p> <pre><code>kubectl get miracephsecret -n ceph-lcm-mirantis\nNAME          AGE     STATE   LAST CHECK             LAST UPDATE \ncephcluster   6d17h   Ready   2024-07-30T11:09:03Z   2024-07-24T15:56:12Z\n</code></pre> <p>Regularly monitoring these objects helps operators ensure that both the configuration and the associated security credentials are up-to-date. Quick access to these statuses aids in troubleshooting and provides confidence that the cluster is operating within expected parameters.</p>"},{"location":"addons/ceph/ceph-operating/#ceph-controllers-and-rook-resources","title":"Ceph Controllers and Rook Resources","text":"<p>Ceph controllers play a crucial role in automating the deployment of the Ceph cluster. These controllers are responsible for creating all required Rook resources in the <code>rook-ceph</code> namespace. The <code>rook-ceph-operator</code> is central to this process as it:</p> <ul> <li>Spawns all necessary Ceph daemons.</li> <li>Manages OSD preparation and deployment.</li> <li>Monitors cluster health.</li> <li>Handles version upgrades.</li> </ul> <p>For a deeper dive into how these components work together, refer to the Rook Getting Started Guide.</p> <p>By delegating the management of underlying Rook resources to the Ceph controllers, operators can focus on high-level configuration without worrying about the lower-level orchestration details.</p>"},{"location":"addons/ceph/ceph-operating/#ceph-cluster-lcm-operations","title":"Ceph Cluster LCM Operations","text":"<p>Ceph Lifecycle Management (LCM) operations ensure that the cluster remains healthy and that maintenance tasks are carried out in a controlled manner. These operations are performed by several components:</p> <ul> <li> <p>Ceph Controller:   Applies changes to the <code>MiraCeph</code> spec (excluding OSD removal), ensuring that the overall configuration is consistently enforced across the cluster.</p> </li> <li> <p><code>ceph-disk-daemon</code>:   Collects detailed information from hosts running OSDs\u2014such as disk usage, partition details, and active OSD instances. This information is vital for making informed decisions during maintenance operations.</p> </li> <li> <p>Ceph Request Controller:   Specifically, the <code>osdremove</code> container handles the OSD cleanup process. It:</p> </li> <li> <p>Verifies the <code>CephOsdRemoveRequest</code> provided by the operator.</p> </li> <li>Utilizes the data collected by the <code>ceph-disk-daemon</code>.</li> <li>Executes the removal of OSDs.</li> <li>Updates the <code>CephOsdRemoveRequest</code> with the current cleanup status.</li> </ul> <p>For further details, please refer again to the Rook Getting Started Guide.</p> <p>Breaking down the LCM operations into dedicated roles (configuration enforcement, data collection, and targeted operations like OSD removal) not only enhances operational transparency but also improves the reliability of cluster maintenance procedures.</p>"},{"location":"addons/ceph/ceph-user-scenarios/","title":"Ceph User Scenarios","text":"<p>This section outlines common scenarios for using Ceph as a storage backend for Kubernetes workloads and in Mirantis k0rdent Virtualization (Kubevirt) environments. It demonstrates how Ceph, when integrated with Mirantis k0rdent Enterprise via <code>MiraCeph</code>, simplifies storage provisioning by automatically generating the necessary StorageClasses and PVs.</p>"},{"location":"addons/ceph/ceph-user-scenarios/#ceph-block-pvc-creation","title":"Ceph Block PVC Creation","text":"<p>The Ceph solution supports the creation of Block-based Persistent Volume Claims (PVCs) without requiring manual intervention. When a Ceph Block pool is defined in the <code>spec.pools</code> section of the <code>MiraCeph</code> specification, the Ceph Controller automatically creates a corresponding <code>StorageClass</code>. This automation reduces operational overhead and minimizes configuration drift.</p> <ul> <li>Automated <code>StorageClass</code> Creation: By deriving <code>StorageClass</code> objects directly from the <code>MiraCeph</code> spec, operators avoid the manual steps typically needed to configure storage.  </li> <li>Volume Modes and Access: Ceph RADOS Block Device supports both Block <code>volumeMode</code> (for Read-Write-Once, Read-Only-Many, and Read-Write-Many) and Filesystem <code>volumeMode</code> (for specific access modes). This flexibility allows you to match storage configurations precisely to application needs.  </li> <li>Operational Consistency: The automatic association between a Ceph pool and its <code>StorageClass</code> helps maintain consistency across environments, reducing configuration errors and streamlining troubleshooting.</li> </ul> <p>For example, if there is a pool named <code>block-pool</code> defined as:</p> <pre><code>spec:\n  pools:\n  - name: block-pool\n    useAsFullName: true\n    deviceClass: hdd\n    default: true\n    replicated:\n      size: 3\n    role: block-pool\n</code></pre> <p>Upon creation of this pool, Mirantis k0rdent Enterprise creates a matching <code>StorageClass</code>:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"false\"\n  creationTimestamp: \"2025-02-28T11:12:21Z\"\n  labels:\n    rook-ceph-storage-class: \"true\"\n  name: block-pool\n  resourceVersion: \"20460\"\n  uid: c58f11db-645f-4c45-9dd8-b009f828a058\nparameters:\n  clusterID: rook-ceph\n  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node\n  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph\n  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner\n  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph\n  imageFeatures: layering\n  imageFormat: \"2\"\n  pool: block-pool\nprovisioner: rook-ceph.rbd.csi.ceph.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n</code></pre> <p>Ceph RADOS Block Device supports creating Persistent Volumes (PVs) with:</p> <ul> <li>Block volumeMode: for access modes such as Read-Write-Once (RWO), Read-Only-Many (ROX), and Read-Write-Many (RWX).</li> <li>Filesystem volumeMode: for RWO and ROX access modes.</li> </ul> <p>For the full support matrix, see:  Ceph CSI Support Matrix.</p> <p>To create a Ceph-based Block PV, define a PVC manifest on the Block pool StorageClass:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-block-volume\n  namespace: default\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 20Gi\n  storageClassName: block-pool\n  volumeMode: Block\n</code></pre> <p>This PVC can now be attached as a volume to any Kubernetes workload (for example, <code>Deployments</code>, <code>StatefulSets</code>, or <code>Pods</code>).</p>"},{"location":"addons/ceph/ceph-user-scenarios/#ceph-filesystem-pvc-creation","title":"Ceph Filesystem PVC Creation","text":"<p>Ceph and Mirantis k0rdent Enterprise also supports creating Filesystem Ceph-based PVCs without additional steps. The Ceph Controller automatically creates a corresponding <code>StorageClass</code> for every CephFS data pool defined in the <code>spec.sharedFilesystem.cephFS[].dataPools</code> section of the MiraCeph specification.</p> <ul> <li>Unified Management: The automation extends to CephFS, ensuring that file-based storage is managed in the same seamless way as block storage.  </li> <li>Simplified Provisioning: By creating <code>StorageClass</code> objects on pool creation, CephFS PVCs require no additional manual configuration. Simply define a PVC manifest referencing the appropriate <code>StorageClass</code>, and the underlying file system is provisioned automatically.</li> </ul> <p>For example, if there is a data pool named <code>filesystem-pool</code> defined as:</p> <pre><code>sharedFilesystem:\n  cephFS:\n  - name: cephfs-store\n    dataPools:\n    - name: filesystem-pool\n      deviceClass: hdd\n      replicated:\n        size: 3\n    metadataPool:\n      deviceClass: hdd\n      replicated:\n        size: 3\n    metadataServer:\n      activeCount: 1\n      activeStandby: false\n</code></pre> <p>Then a corresponding <code>StorageClass</code> will be created along with the pool\u2019s creation. The <code>StorageClass</code> parameters are similar to those used for Block PVCs.</p> <p>To create a Ceph-based Filesystem PV, define a PVC manifest using the appropriate StorageClass.</p>"},{"location":"addons/ceph/ceph-user-scenarios/#ceph-usage-in-mirantis-k0rdent-virtualization-vms","title":"Ceph Usage in Mirantis k0rdent Virtualization VMs","text":"<p>Mirantis k0rdent Virtualization leverages Ceph-based <code>StorageClass</code> objects to provide persistent storage for Virtual Machines (VMs). Ceph's ability to support both block and filesystem volumes makes it a versatile backend for diverse workloads.</p> <p>Note: It is recommended to enable the <code>device_ownership_from_security_context</code> feature in the container runtime (CRI) for proper handling of block volumes by Mirantis k0rdent Virtualization. More details are available at: Block CRI Ownership Configuration.</p> <ul> <li>Dual Mode Support: This example illustrates the use of both a filesystem (RWX) and a block (RWO) volume in the same VM, demonstrating Ceph's flexibility in supporting diverse storage requirements.  </li> <li>Unified Storage for VMs and Containers: Using Ceph as the underlying storage system allows for consistent management of storage resources, whether they are being used for traditional container workloads or virtual machine environments.  </li> <li>Simplified VM Provisioning: With pre-configured <code>StorageClass</code> objects, creating and attaching persistent volumes to VMs becomes a seamless process, reducing manual intervention and potential configuration errors.</li> </ul> <p>Ceph-based persistent volumes are defined within a Mirantis k0rdent Virtualization VirtualMachine manifest. Consider the following example:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  labels:\n    kubevirt.io/vm: vm-cirros-datavolume\n  name: vm-cirros-datavolume-ceph\nspec:\n  dataVolumeTemplates:\n  - metadata:\n      name: cirros-dv\n    spec:\n      pvc:\n        accessModes:\n        - ReadWriteMany\n        resources:\n          requests:\n            storage: 2Gi\n        storageClassName: cephfs-store-filesystem-pool\n        volumeMode: Filesystem\n      source:\n        registry:\n          url: docker://docker-dev-kaas-local.docker.mirantis.net/kubevirt/cirros-container-disk-demo:1.4.0-20250217075514\n  - metadata:\n      name: additional-dev\n    spec:\n      storage:\n        accessModes:\n        - ReadWriteOnce\n        resources:\n          requests:\n            storage: 10Gi\n        storageClassName: block-pool\n        volumeMode: Block\n      source:\n        pvc:\n          name: cirros-dv-add-vol\n          namespace: default\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/vm: vm-cirros-datavolume\n    spec:\n      domain:\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: datavolumedisk1\n          - disk:\n              bus: virtio\n            name: datavolumedisk2\n        resources:\n          requests:\n            memory: 128Mi\n      terminationGracePeriodSeconds: 0\n      volumes:\n      - dataVolume:\n          name: cirros-dv\n        name: datavolumedisk1\n      - dataVolume:\n          name: additional-dev\n        name: datavolumedisk2\n</code></pre> <p>In this example, two different Ceph-based PVCs are used as <code>DataVolume</code> objects for the Virtual Machine:</p> <ul> <li>A Filesystem RWX volume based on the CephFS <code>StorageClass</code> (<code>cephfs-store-filesystem-pool</code>) used as the root disk.</li> <li>A Block RWO volume based on an already created PVC (<code>cirros-dv-add-vol</code>) using the Block StorageClass (<code>block-pool</code>).</li> </ul> <p>The underlying Block PVC manifest for <code>cirros-dv-add-vol</code> is as follows:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cirros-dv-add-vol\n  namespace: default\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: block-pool\n  volumeMode: Block\n</code></pre> <p>This manifest creates the additional Block PVC, which can then be attached to the Virtual Machine as an extra <code>DataVolume</code>.</p> <p>By automating creation of <code>StorageClass</code> objects and abstracting much of the complexity involved in provisioning persistent volumes, Ceph simplifies operational tasks for administrators. Whether you're dealing with container workloads or VMs, the consistent and unified approach provided by Mirantis k0rdent Enterprise Ceph, integrated through <code>MiraCeph</code>, streamlines storage management and enhances operational reliability without requiring extensive manual configuration.</p>"},{"location":"addons/ceph/replace-kaascephcluster/","title":"MiraCeph Instead of KaasCephCluster","text":"<p>In some scenarios, operators prefer to use MiraCeph because it provides a more direct and decoupled way to manage Ceph clusters. With <code>MiraCeph</code>, the Ceph cluster configuration is applied directly under the <code>ceph-lcm-mirantis</code> namespace, avoiding the extra layer of indirection introduced by the <code>KaasCephCluster</code> object. This can simplify troubleshooting and provide a clearer view of the storage configuration.</p> <p>A typical <code>KaaSCephCluster</code> specification looks like this: <pre><code>apiVersion: kaas.mirantis.com/v1alpha1\nkind: KaaSCephCluster\nmetadata:\n  name: test-child-req\n  namespace: child-ns\nspec:\n  cephClusterSpec:\n    \u2026 \n    {ceph-cluster-spec}\n    \u2026 \n  kaasCephCluster:\n    name: ceph-cluster-child-cluster\n    namespace: child-ns\n</code></pre></p> <p>To configure MiraCeph, the operator should apply the <code>{ceph-cluster-spec}</code> block directly in the MiraCeph spec field: <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: MiraCeph\nmetadata:\n  name: ceph-cluster\n  namespace: ceph-lcm-mirantis\nspec:\n  \u2026 \n  {ceph-cluster-spec}\n  \u2026 \n</code></pre></p>"},{"location":"addons/ceph/replace-kaascephcluster/#replacing-kaascephoperationrequest","title":"Replacing KaasCephOperationRequest","text":"<p>The same approach applies when the operator creates CephOsdRemoveRequest and CephPerfTestRequest objects. Instead of creating a <code>KaasCephOperationRequest</code>, the operator should create the corresponding object. By using dedicated objects for  each operation, such as CephOsdRemoveRequest and CephPerfTestRequest, the configuration becomes more modular and transparent.</p>"},{"location":"addons/ceph/replace-kaascephcluster/#for-ceph-osd-removal","title":"For Ceph OSD Removal","text":"<p>For example, to remove a Ceph Object Storage Daemon, replace the following <code>KaasCephOperationRequest</code>...</p> <pre><code>apiVersion: kaas.mirantis.com/v1alpha1\nkind: KaaSCephOperationRequest\nmetadata:\n  name: lcm-req\n  namespace: child-ns\nspec:\n  kaasCephCluster:\n    name: ceph-cluster-child-cluster\n    namespace: child-ns\n  osdRemove:\n    \u2026 \n    {osd-request-spec}\n    \u2026 \n</code></pre> <p>... with a <code>CephOsdRemoveRequest</code>:</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveRequest\nmetadata:\n  name: lcm-req\n  namespace: ceph-lcm-mirantis\nspec:\n  \u2026 \n  {osd-request-spec}\n  \u2026 \n</code></pre>"},{"location":"addons/ceph/replace-kaascephcluster/#for-ceph-performance-testing","title":"For Ceph Performance Testing","text":"<p>To engage in performance testing, replace the following <code>KaasCephOperationRequest</code>... <pre><code>apiVersion: kaas.mirantis.com/v1alpha1\nkind: KaaSCephOperationRequest\nmetadata:\n  name: perf-req\n  namespace: child-ns\nspec:\n  kaasCephCluster:\n    name: ceph-cluster-child-cluster\n    namespace: child-ns\n  perfTest:\n    \u2026 \n    {perftest-request-spec}\n    \u2026 \n</code></pre></p> <p>... with a <code>CephPerfTestRequest</code>:</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephPerfTestRequest\nmetadata:\n  name: perftest-req\n  namespace: ceph-lcm-mirantis\nspec:\n  \u2026 \n  {perftest-request-spec}\n  \u2026 \n</code></pre>"},{"location":"addons/ceph/replace-kaascephcluster/#additional-considerations","title":"Additional Considerations","text":"<p>There are several reasons to consider <code>MiraCeph</code> instead of <code>KaasCephCluster</code>, including:</p> <ul> <li>Direct Configuration: MiraCeph enables you to inject the Ceph cluster specification directly, which can simplify management by removing the extra abstraction layer found in KaasCephCluster.</li> <li>Namespace Isolation: Deploying MiraCeph in the <code>ceph-lcm-mirantis</code> namespace centralizes Ceph operations, making it easier to monitor and maintain.</li> <li>Focused Operations: Replacing the general KaasCephOperationRequest with dedicated operation requests (e.g., for OSD removal or performance testing) leads to clearer, purpose-built configurations.</li> <li>Flexibility: For operators who need fine-grained control or who prefer a more transparent view of their Ceph cluster\u2019s configuration, MiraCeph is a straightforward alternative.</li> </ul> <p>For more details, please refer to the official documentation: Mirantis MOSK Ceph Operations</p>"},{"location":"addons/ceph/configuration/","title":"Ceph Cluster Configuration","text":"<p><code>MiraCeph</code> is a Mirantis-developed declarative object used to configure and manage a Ceph cluster within a Kubernetes environment (in this case, on deployed and managed with Mirantis k0rdent Enterprise). The <code>MiraCeph</code> object must be created in the <code>ceph-lcm-mirantis</code> namespace and is processed by the Ceph Controller, ensuring that any desired changes to the cluster are automatically applied.</p> <p><code>MiraCeph</code>'s configuration is split into two main sections: <code>spec</code> and <code>status</code>. The <code>spec</code> section allows you to define every aspect of the cluster, including client access with finely tuned authorization capabilities, data storage paths, external cluster connections, extra options for device management, health check settings for monitors, managers, and OSDs, and resource configuration for hyperconvergence. Additional components like ingress rules, manager modules, network settings, detailed node definitions, object storage options (such as RADOS Gateway and multisite setups), and pool configurations further customize the cluster behavior. Rook-specific configurations (via <code>rookConfig</code> and <code>rookNamespace</code>) and shared filesystem (CephFS) setups also integrate seamlessly.</p> <p>Beyond the primary configuration, the health of the Ceph cluster is monitored through the <code>MiraCephHealth</code> object, which aggregates status checks, timestamps, state information, and detailed cluster conditions. Similarly, the <code>MiraCephSecret</code> object manages and tracks secrets for Ceph clients and RGW users, ensuring secure credential handling.</p> <p>For lifecycle operations, dedicated objects replace more generic requests. The <code>CephOsdRemoveRequest</code> handles the removal of Object Storage Daemons (OSDs), providing granular control over cleanup actions\u2014such as full node cleanups or targeted removal by device or OSD ID. Meanwhile, the <code>CephPerfTestRequest</code> object facilitates periodic performance testing with FIO, complete with configurable parameters, scheduling, and result storage on persistent volumes.</p> <p>Overall, these components work together to provide an integrated, Kubernetes-native way to deploy, manage, and monitor Ceph clusters while automating many of the complexities associated with traditional storage systems.</p>"},{"location":"addons/ceph/configuration/CephOsdRemoveRequest/","title":"CephOsdRemoveRequest","text":"<p>The <code>CephOsdRemoveRequest</code> object is used for Ceph Object Storage Daemon (OSD) lifecycle management (LCM). It allows the operator to manage the removal of OSDs from the cluster.</p>"},{"location":"addons/ceph/configuration/CephOsdRemoveRequest/#spec","title":"Spec","text":"<p>The <code>spec</code> section describes what should be cleaned up:</p> <ul> <li><code>nodes:</code> A mapping of node names with specifications:<ul> <li><code>completeCleanUp:</code> Boolean flag for full node cleanup (including removal from the CRUSH map).</li> <li><code>dropFromClusterOnly:</code> Remove the OSD from the cluster without cleaning devices.</li> <li><code>cleanupStrayPartitionsOnly:</code> Clean only stray partitions.</li> <li><code>cleanupByOsdId:</code> List of OSD IDs to remove.</li> <li><code>cleanupByDevice:</code> List of devices to clean (by name or path).</li> </ul> </li> <li>Additional fields include <code>approve</code>, <code>keepOnFail</code>, <code>resolved</code>, <code>resumeFailed</code>, and <code>resumeFailedStep</code>.</li> </ul> <p>Example: <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveRequest\nmetadata:\n  name: lcm-req\n  namespace: ceph-lcm-mirantis\nspec:\n  nodes:\n    node-a:\n      completeCleanUp: true\n    node-b:\n      cleanupByDevice:\n      - name: sdb\n      - path: \"/dev/disk/by-path/pci-0000:00:0x.0\"\n    node-c:\n      cleanupByOsdId:\n      - 5\n      - 10\n</code></pre></p>"},{"location":"addons/ceph/configuration/CephOsdRemoveRequest/#status","title":"Status","text":"<p>The <code>status</code> section provides: - <code>phase:</code> Current phase of the request (e.g., Pending, Processing, CompletedWithWarnings). - <code>phaseInfo:</code> Additional information about the phase. - <code>removeInfo:</code> Detailed mapping of what is scheduled for removal. - <code>failedStepID:</code> Identifier of any failed step. - <code>messages:</code> List of messages regarding request progress. - <code>conditions:</code> Historical record of phase transitions and status changes.</p> <p>(A full example is provided in the document.)</p>"},{"location":"addons/ceph/configuration/CephPerfTestRequest/","title":"CephPerfTestRequest","text":"<p>The <code>CephPerfTestRequest</code> object is designed to run periodic performance tests using FIO (Flexible I/O Tester) in a Kubernetes job or cronjob. Test results can be stored on a persistent volume (secured with a PVC).</p>"},{"location":"addons/ceph/configuration/CephPerfTestRequest/#spec","title":"Spec","text":"<p>The <code>spec</code> section includes:</p> <ul> <li><code>parameters:</code> List of arguments for the FIO test.</li> <li><code>command:</code> Entrypoint command to run the test (optional; defaults to the image\u2019s entrypoint).</li> <li><code>image:</code> The container image used for testing (default recommended is <code>vineethac/fio_image</code>).</li> <li><code>periodic:</code> Configuration for running the test periodically:<ul> <li><code>schedule</code>: Cron-formatted schedule.</li> <li><code>suspended</code>: Flag to suspend the job.</li> <li><code>runsToKeep</code>: Number of previous runs to retain.</li> </ul> </li> <li><code>saveResultOnPvc:</code> Configuration for saving test results:<ul> <li><code>pvcName</code>: Name of the PVC.</li> <li><code>pvcStorageClass</code>: StorageClass to use.</li> <li><code>pvcSize</code>: Size of the PVC.</li> <li><code>preserveOnDelete</code>: Whether to retain the PVC on deletion.</li> </ul> </li> </ul> <p>Example: <pre><code>spec:\n  periodic:\n    schedule: \"0 */10 * * *\"\n    runsToKeep: 10\n  parameters:\n  - --ioengine=rbd\n  - --pool=mirablock-k8s-block-hdd\n  - --rbdname=tests\n  - --name=hourly_perftest\n  - --rw=randrw:16k\n  - --rwmixread=40\n  - --bs=4k\n  - --size=500M\n  - --iodepth=32\n  - --numjobs=8\n  - --group_reporting\n  - --direct=1\n  - --fsync=32\n  - --buffered=0\n  - --exitall\n  saveResultOnPvc:\n    pvcName: \"perf-test-pvs\"\n    pvcSize: \"1Gi\"\n</code></pre></p>"},{"location":"addons/ceph/configuration/CephPerfTestRequest/#status","title":"Status","text":"<p>The <code>status</code> section provides:</p> <ul> <li><code>phase:</code> Current phase of the test (e.g., Pending, Running, Finished).</li> <li><code>lastStartTime:</code> Time when the test started.</li> <li><code>lastDurationTime:</code> Duration of the test run.</li> <li><code>lastJobStatus:</code> Status of the last test job.</li> <li><code>messages:</code> Any warnings or issues.</li> <li><code>results:</code> Details on where to find the results:<ul> <li><code>perftestReference</code></li> <li><code>referenceNamespace</code></li> <li><code>storedOnPvc</code></li> </ul> </li> <li><code>statusHistory:</code> History of test runs including start time, job status, and duration.</li> </ul> <p>Example: <pre><code>status:\n  lastJobStatus: Running\n  lastStartTime: \"2024-07-30T10:56:00Z\"\n  phase: Running\n  results:\n    perftestReference: cronjob/testperf-cephperftest\n    referenceNamespace: rook-ceph\n    storedOnPvc:\n      pvcName: perf-test-pvs\n      pvcNamespace: rook-ceph\n  statusHistory:\n  - durationTime: 4m34s\n    jobStatus: Completed\n    startTime: \"2024-07-30T00:51:25Z\"\n</code></pre></p>"},{"location":"addons/ceph/configuration/MiraCeph/","title":"MiraCeph","text":"<p>This document describes how to configure a Ceph cluster using the <code>MiraCeph</code> object. <code>MiraCeph</code> must always be created under the <code>ceph-lcm-mirantis</code> namespace\u2014the base controllers namespace\u2014and is processed by the Ceph Controller pod and its ceph-controller container.</p> <p><code>MiraCeph</code> represents a declarative approach to defining your Ceph cluster configuration. It give syou precise control over the storage cluster while integrating with Kubernetes operational patterns.</p> <p><code>MiraCeph</code> consists of two main sections: <code>spec</code> and <code>status</code>.  </p>"},{"location":"addons/ceph/configuration/MiraCeph/#spec","title":"Spec","text":"<p>The <code>spec</code> section defines the desired configuration of the Ceph cluster and its associated resources. This section is intentionally comprehensive, enabling you to tailor every aspect of the cluster's behavior. It includes the following:</p>"},{"location":"addons/ceph/configuration/MiraCeph/#clients-optional","title":"Clients (Optional)","text":"<p>A list of Ceph clients used for cluster connection by consumer services. Each <code>Client</code> includes:</p> <ul> <li><code>name</code>: Client name.</li> <li><code>caps</code>: Short for \"capabilities\", this is a key-value mapping of authorization capabilities. It includes:<ul> <li><code>mon</code>: Specifies monitor-related permissions. A value like allow <code>r</code> grants read-only access, while additional          commands (for example, <code>allow command \"osd blacklist\"</code>) let you enable specific actions.</li> <li><code>osd</code>: Controls what the client can do with object storage daemons. For example, <code>profile rbd pool=images</code>          might grant a predefined set of operations optimized for RBD interactions on a specific pool.          (For details, see Ceph Authorization Capabilities)</li> </ul> </li> </ul> <p>Example: <pre><code>clients:\n  - caps:\n      mon: allow r, allow command \"osd blacklist\"\n      osd: profile rbd pool=images\n    name: glance\n  - caps:\n      mon: allow r, allow command \"osd blacklist\"\n      osd: profile rbd pool=volumes, profile rbd-read-only pool=images, profile rbd pool=backups\n    name: cinder\n  - caps:\n      mon: allow r, allow command \"osd blacklist\"\n      osd: profile rbd pool=vms, profile rbd-read-only pool=images\n    name: nova\n</code></pre></p> <p>By defining clients explicitly, you control access permissions at a granular level, helping to ensure that each service  interacts with the cluster only in the manner intended by its role.</p>"},{"location":"addons/ceph/configuration/MiraCeph/#data-directory","title":"Data Directory","text":"<p>Customizing the data directory path can help align storage paths with local policies or performance tuning considerations.</p> <ul> <li><code>dataDirHostPath</code>: Default hostPath directory where Ceph daemons store data.  This value defaults to <code>/var/lib/rook</code>.</li> </ul>"},{"location":"addons/ceph/configuration/MiraCeph/#external-cluster-configuration-optional","title":"External Cluster Configuration (Optional)","text":"<p>By customizing the external cluster configuration, you can bridge an externally managed Ceph cluster with your Kubernetes environment, offering flexibility in scenarios where storage infrastructure is already in place.</p> <ul> <li><code>external</code>: Enables use of an external Ceph cluster connected to the internal MKE cluster.  </li> <li><code>connectionString</code>: An encrypted structure with all required connection parameters.  </li> </ul>"},{"location":"addons/ceph/configuration/MiraCeph/#extra-options","title":"Extra Options","text":"<p>The <code>extraOpts</code> values help in fine-tuning and safeguarding the cluster. For example, <code>preventClusterDestroy</code> is a simple yet effective guard against unintended operations that could lead to data loss.</p> <ul> <li><code>extraOpts</code>: Extra options for managing the Ceph cluster.   Contains fields such as:</li> <li><code>customDeviceClasses</code>: List of custom device classes (besides the default ssd, hdd, nvme).</li> <li><code>deviceLabels</code>: Labels for devices to aid in node configuration and grouping.</li> <li><code>enableProgressEvents</code>: Flag to enable progress events (disabled by default due to CPU overhead).</li> <li><code>preventClusterDestroy</code>: Prevents accidental cluster removal.</li> </ul> <p>Example: <pre><code>extraOpts:\n  deviceLabels:\n    node-a:\n      disk-1: /dev/disk/by-id/&lt;unique_ID&gt;\n  customDeviceClasses:\n    - nvme-separate-class\n  enableProgressEvents: true\n  preventClusterDestroy: true\n</code></pre></p>"},{"location":"addons/ceph/configuration/MiraCeph/#health-check","title":"Health Check","text":"<p>The <code>healthCheck</code> object provides settings for internal daemon healthchecks and liveness probes for monitors (<code>mon</code>), managers (<code>mgr</code>), and OSDs. It includes values such as:</p> <ul> <li><code>startupProbe</code>: Configuration for the startup probe, with options such as:<ul> <li><code>timeoutSeconds</code></li> <li><code>periodSeconds</code></li> <li><code>successThreshold</code></li> <li><code>failureThreshold</code></li> </ul> </li> <li><code>livenessProbe</code>: Configuration for the liveness probe, with options such as:<ul> <li><code>initialDelaySeconds</code></li> <li><code>timeoutSeconds</code></li> <li><code>periodSeconds</code></li> <li><code>successThreshold</code></li> <li><code>failureThreshold</code></li> </ul> </li> <li><code>daemonHealth</code>: Settings for overall health checks for daemon types, including:<ul> <li><code>mon</code></li> <li><code>osd</code></li> <li><code>status</code></li> </ul> </li> </ul> <p>Example: <pre><code>healthCheck:\n  daemonHealth:\n    mon:\n      disabled: false\n      interval: 45s\n      timeout: 600s\n    osd:\n      disabled: false\n      interval: 60s\n    status:\n      disabled: true\n  livenessProbe:\n    mon:\n      disabled: false\n      probe:\n        timeoutSeconds: 10\n        periodSeconds: 3\n        successThreshold: 3\n    mgr:\n      disabled: false\n      probe:\n        timeoutSeconds: 5\n        failureThreshold: 5\n    osd:\n      probe:\n        initialDelaySeconds: 5\n        timeoutSeconds: 10\n        failureThreshold: 7\n  startupProbe:\n    mon:\n      disabled: true\n    mgr:\n      probe:\n        successThreshold: 3\n</code></pre></p> <p>Properly configured probes not only support operational stability but also provide early warnings of potential issues, enabling proactive remediation.</p>"},{"location":"addons/ceph/configuration/MiraCeph/#hyperconverge","title":"Hyperconverge","text":"<p>Configures resource requests and limits for Ceph daemons and enables scheduling on tainted nodes.</p> <ul> <li><code>tolerations</code>: Tolerations for tainted nodes (see Taints and Tolerations).</li> <li><code>resources</code>: Specifies resource requests or limits (see Manage Resources).</li> </ul> <p>Example: <pre><code>hyperconverge:\n  tolerations:\n    mon:\n      rules:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/controlplane\n        operator: Exists\n  resources:\n    mon:\n      requests:\n        memory: 1Gi\n        cpu: 2\n      limits:\n        memory: 2Gi\n        cpu: 3\n</code></pre></p> <p>This section lets you optimize daemon performance relative to your cluster's overall workload while ensuring that Ceph daemons can coexist with other system components on shared nodes.</p>"},{"location":"addons/ceph/configuration/MiraCeph/#ingress-configuration","title":"Ingress Configuration","text":"<p>Ceph on k0rdent enables you to provide the configuration of custom ingress rules for external access,  such as setting a public endpoint for the RADOS Gateway (RGW).</p> <ul> <li><code>tlsConfig</code>: TLS configuration including:<ul> <li><code>publicDomain</code>: Domain name for public endpoints.</li> <li><code>tlsSecretRefName</code>: Name of the secret holding TLS certs.</li> <li><code>certs</code>: TLS certificate details (<code>cacert</code>, <code>tlsCert</code>, <code>tlsKey</code>).</li> </ul> </li> <li><code>annotations</code>: Extra key-value annotations.</li> <li><code>controllerClassName</code>: Name of the ingress controller.</li> </ul> <p>Example: <pre><code>ingressConfig:\n  annotations:\n    \"nginx.ingress.kubernetes.io/proxy-body-size\": \"0\"\n  tlsConfig:\n    publicDomain: domain.example.com\n    certs: |\n      cacert: |\n        &lt;cacert&gt;\n      tlsKey: |\n        &lt;tlsKey&gt;\n      tlsCert: |\n        &lt;tlsCert&gt;\n</code></pre></p> <p>Or perhaps: <pre><code>ingressConfig:\n  controllerClassName: custom-ingress-controller\n  tlsConfig:\n    publicDomain: domain.example.com\n    tlsSecretRefName: ingress-custom-certs\n</code></pre></p> <p>Configuring ingress in this way allows controlled exposure of services (such as RGW) with security parameters defined as needed. This approach integrates well with standard Kubernetes practices without adding unnecessary layers of abstraction.</p>"},{"location":"addons/ceph/configuration/MiraCeph/#manager-modules-mgr","title":"Manager Modules (mgr)","text":"<p>Enabling specific manager modules helps streamline cluster operations by offloading workload balancing and pool autoscaling, all configured directly within the Ceph management interface.</p> <p>Example: <pre><code>mgr:\n  mgrModules:\n  - name: balancer\n    enabled: true\n  - name: pg_autoscaler\n    enabled: true\n</code></pre></p>"},{"location":"addons/ceph/configuration/MiraCeph/#network","title":"Network","text":"<p>The <code>network</code> object defines ranges for daemon communication.</p> <ul> <li><code>clusterNet</code>: CIDR for OSD replication network.</li> <li><code>publicNet</code>: CIDR for service-to-operator communication.</li> </ul> <p>Example: <pre><code>network:\n  publicNet: \"10.12.0.0/24,10.13.0.0/24\"\n  clusterNet: \"10.10.0.0/24,10.11.0.0/24\"\n</code></pre></p> <p>Separating network traffic based on purpose\u2014replication versus external communication\u2014supports better network hygiene and easier troubleshooting.</p>"},{"location":"addons/ceph/configuration/MiraCeph/#nodes","title":"Nodes","text":"<p>The <code>node</code> object includes the full configuration for Ceph nodes.</p> <p>Each node entry includes:</p> <ul> <li><code>name</code>: Kubernetes node name.</li> <li><code>roles</code>: List of daemons to spawn (e.g., <code>mon</code>, <code>mgr</code>, <code>rgw</code>, <code>mds</code>).</li> <li><code>crush</code>: Explicit key-value CRUSH topology (see CRUSH Maps).</li> <li><code>config</code>: General Ceph node configuration.</li> <li><code>nodeGroup / nodesByLabel</code>: For grouping nodes.</li> <li><code>resources</code>: Kubernetes resource requirements.</li> <li><code>devices</code>: List of devices with:<ul> <li><code>name</code>: Device name.</li> <li><code>fullpath</code>: Device symlink.</li> <li><code>config</code>: Device configuration (must include <code>deviceClass</code> and may include <code>metadataDevice</code> or <code>osdsPerDevice</code>).</li> </ul> </li> </ul> <p>Example: <pre><code>nodes:\n  - name: mon-1\n    roles:\n      - mon\n      - mgr\n      - rgw\n      - mds\n  - name: mon-2\n    roles:\n      - mon\n  - name: mon-3\n    roles:\n      - mon\n  - devices:\n      - config:\n          deviceClass: nvme-separate-class\n        name: disk-1\n</code></pre></p> <p>Detailed node configuration\u2014including CRUSH maps and resource definitions\u2014allows operators to align the cluster layout with existing infrastructure and performance requirements.</p>"},{"location":"addons/ceph/configuration/MiraCeph/#object-storage","title":"Object Storage","text":"<p>Full configuration for RadosGW and multisite features. This section provides a unified method for configuring both object storage and multi-zone capabilities, ensuring that settings remain consistent and traceable.</p> <p>Includes:</p> <ul> <li><code>rgw</code>: RADOS Gateway settings such as:<ul> <li><code>name</code></li> <li><code>dataPool</code></li> <li><code>metadataPool</code></li> <li><code>preservePoolsOnDelete</code></li> <li><code>gateway</code></li> <li><code>externalRgwEndpoints</code></li> <li><code>resources</code></li> </ul> </li> <li> <p><code>objectUsers</code>: A list of object storage users. Each includes:</p> <ul> <li><code>name</code></li> <li><code>displayName</code></li> <li><code>capabilities</code></li> <li><code>quotas</code></li> </ul> </li> <li> <p><code>buckets</code>: List of bucket names.</p> </li> <li><code>skipAutoZoneGroupHostnameUpdate</code>: Flag to disable auto-update of allowed hostnames.</li> <li><code>zone</code>: Specifies the Ceph Multisite zone.</li> <li><code>SSLCert / SSLCertInRef</code>: TLS certificate configuration for RGW.</li> </ul>"},{"location":"addons/ceph/configuration/MiraCeph/#pools","title":"Pools","text":"<p>The <code>pools</code> object lets you define Ceph RBD pools. Pool configuration is where you define data redundancy and performance characteristics. The ability to choose between replication and erasure coding provides flexibility to match different workload requirements.</p> <p>Each pool includes:</p> <ul> <li><code>name</code>: Pool name prefix (final name is <code>&lt;name&gt;-&lt;deviceClass&gt;</code> unless <code>useAsFullName</code> is true).</li> <li><code>useAsFullName</code>: If true, use the pool name as-is.</li> <li><code>role</code>: Pool role.</li> <li><code>default</code>: Indicates if this pool (and its StorageClass) is the default.</li> <li><code>deviceClass</code>: Device class (HDD, SSD, NVMe).</li> <li><code>replicated</code>: Replication settings (for example, <code>size</code> or <code>targetSizeRatio</code>).</li> <li><code>erasureCoded</code>: Erasure coding settings (for example, <code>codingChunks</code>, <code>dataChunks</code>, <code>algorithm</code>).</li> <li><code>failureDomain</code>: The domain for data replication (default: host).</li> <li><code>allowVolumeExpansion</code></li> <li><code>rbdDeviceMapOptions</code></li> <li><code>parameters</code></li> <li><code>reclaimPolicy</code></li> </ul> <p>Example: <pre><code>pools:\n  - name: kubernetes\n    role: kubernetes\n    deviceClass: hdd\n    replicated:\n      - size: 3\n      - targetSizeRatio: 10.0\n    default: true\n  - name: datapool\n    failureDomain: rack\n    role: custom-pool\n    deviceClass: hdd\n    erasureCoded:\n      - codingChunks: 1\n      - dataChunks: 2\n</code></pre></p>"},{"location":"addons/ceph/configuration/MiraCeph/#rook-config","title":"Rook Config","text":"<p>The <code>rookConfig</code> object is a key-value map for Ceph configuration parameters. You can specify  the config section with a delimiter (<code>|</code>). For example:</p> <pre><code>rookConfig:\n  mon|mon_warn_on_insecure_global_id_reclaim: true\n  mon_target_pg_per_osd: 300\n  osd.100|bluestore_allocator: hybrid\n</code></pre> <p>Fine-tuning through the Rook configuration helps in aligning Ceph's behavior with both operational policies and the underlying hardware characteristics.</p>"},{"location":"addons/ceph/configuration/MiraCeph/#rook-namespace","title":"Rook Namespace","text":"<ul> <li><code>rookNamespace</code>: Should be set to <code>rook-ceph</code>.</li> </ul>"},{"location":"addons/ceph/configuration/MiraCeph/#shared-filesystem-cephfs","title":"Shared Filesystem (CephFS)","text":"<p>Enables CephFS with a list of CephFS configurations. Each entry includes:</p> <ul> <li><code>name</code>: CephFS instance name.</li> <li><code>dataPools</code>: List of data pool specifications (first pool is the default and must be replicated).</li> <li><code>metadataPool</code>: CephFS metadata pool (replicated only).</li> <li><code>preserveFilesystemOnDelete</code>: Whether to delete the filesystem when CephFS is removed.</li> <li><code>metadataServer</code>: Configuration for the metadata server (e.g., <code>activeCount</code>, <code>activeStandby</code>).</li> </ul> <p>Example of a simple <code>MiraCeph</code> configuration that includes both RGW and CephFS: <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: MiraCeph\nmetadata:\n  name: ceph-cluster\n  namespace: ceph-lcm-mirantis\nspec:\n  network:\n    publicNet: 10.10.0.0/24\n    clusterNet: 10.11.0.0/24\n  nodes:\n    mon-1:\n      roles:\n      - mon\n      - mgr\n    mon-2:\n      roles:\n      - mon\n      - mgr\n    mon-3:\n      roles:\n      - mon\n      - mgr\n    osd-1:\n      devices:\n      - fullPath: dev/disk/by-id/scsi-1ATA_WDC_WDS100T2B0A-00SM50_200231443409\n        config:\n          deviceClass: ssd\n    osd-2:\n      devices:\n      - fullPath: /dev/disk/by-id/scsi-1ATA_WDC_WDS100T2B0A-00SM50_200231440912\n        config:\n          deviceClass: ssd\n    osd-3:\n      devices:\n      - fullPath: /dev/disk/by-id/scsi-1ATA_WDC_WDS100T2B0A-00SM50_200231434939\n        config:\n          deviceClass: ssd\n  pools:\n  - name: kubernetes\n    role: kubernetes\n    deviceClass: ssd\n    replicated:\n      size: 3\n      targetSizeRatio: 10.0\n    default: true\n  objectStorage:\n    rgw:\n      dataPool:\n        deviceClass: ssd\n        erasureCoded:\n          codingChunks: 1\n          dataChunks: 2\n        failureDomain: host\n      gateway:\n        instances: 3\n        port: 80\n        securePort: 8443\n      metadataPool:\n        deviceClass: ssd\n        failureDomain: host\n        replicated:\n          size: 3\n      name: object-store\n      preservePoolsOnDelete: false\n    sharedFilesystem:\n      cephFS:\n      - name: cephfs-store\n        dataPools:\n        - name: cephfs-pool-1\n          deviceClass: ssd\n          replicated:\n            size: 3\n          failureDomain: host\n        metadataPool:\n          deviceClass: ssd\n          replicated:\n            size: 3\n          failureDomain: host\n        metadataServer:\n          activeCount: 1\n          activeStandby: false\n</code></pre></p>"},{"location":"addons/ceph/configuration/MiraCeph/#status","title":"Status","text":"<p>The <code>status</code> section of <code>MiraCeph</code> reflects the current state of the Ceph cluster configuration as processed by the Ceph controller.</p> <p>Key fields include: - <code>clusterVersion</code>: Current Ceph cluster version. - <code>phase</code>: Current <code>MiraCeph</code> phase. - <code>message</code>: Description of the current phase. - <code>validation</code>: Contains the last validated generation and the result of validation.</p> <p>Example: <pre><code>status:\n  clusterVersion: v18.2.4\n  message: Ceph cluster configuration successfully applied\n  phase: Ready\n  validation:\n    lastValidatedGeneration: 2\n    result: Succeed\n</code></pre></p> <p>Regular status reporting through this section provides operators with a clear, technical view of cluster health and configuration state, aiding in troubleshooting and routine maintenance.</p>"},{"location":"addons/ceph/configuration/MiraCephHealth/","title":"MiraCephHealth","text":"<p>The <code>MiraCephHealth</code> object aggregates all statuses from the Ceph cluster. It is updated automatically by the ceph-controller\u2019s status-controller.</p> <p>Status fields include: 1. <code>lastCheck:</code> Time of the last status check. 2. <code>lastUpdate:</code> Time of the last update. 3. <code>state:</code> Current state of the status check (e.g., Ready means no issues). 4. <code>messages:</code> Descriptions of any detected issues. 5. <code>miraCephGeneration:</code> Generation of the MiraCeph object used for status checks. 6. <code>fullClusterStatus:</code> Overall cluster status including details on block storage, daemon statuses, usage, and conditions.</p> <p>(A detailed example is provided in the full documentation.)</p>"},{"location":"addons/ceph/configuration/MiraCephSecret/","title":"MiraCephSecret","text":"<p>The <code>MiraCephSecret</code> object aggregates secrets created by Rook orchestration and Ceph controller components. It is managed automatically by the ceph-controller pod and secrets-controller container.</p> <p>Status fields include: - <code>lastSecretCheck:</code> Last time secrets were checked. - <code>lastSecretUpdate:</code> Last time secrets were updated. - <code>state:</code> Current state of the secrets check. - <code>secretInfo:</code> Details about secrets, including lists for:     - <code>clientSecrets:</code> Secrets for Ceph clients.     - <code>rgwUserSecrets:</code> Secrets for Ceph RGW users.</p> <p>Example: <pre><code>status:\n  lastSecretCheck: \"2024-07-23T13:40:37Z\"\n  lastSecretUpdate: \"2024-07-16T15:58:15Z\"\n  secretInfo:\n    clientSecrets:\n      - name: client.admin\n        secretName: rook-ceph-admin-keyring\n        secretNamespace: rook-ceph\n      - name: client.cinder\n        secretName: rook-ceph-client-cinder\n        secretNamespace: rook-ceph\n      - name: client.glance\n        secretName: rook-ceph-client-glance\n        secretNamespace: rook-ceph\n      - name: client.nova\n        secretName: rook-ceph-client-nova\n        secretNamespace: rook-ceph\n    rgwUserSecrets:\n      - name: rgw-ceilometer\n        secretName: rook-ceph-object-user-openstack-store-rgw-ceilometer\n        secretNamespace: rook-ceph\n  state: Ready\n</code></pre></p>"},{"location":"addons/mkvirt/","title":"Mirantis k0rdent Virtualization","text":"<p>Mirantis k0rdent Virtualization (KubeVirt) is a complete solution for users looking to run virtual machine\u2013based workloads on top of Kubernetes. The central technology is KubeVirt, a virtual machine management add-on for Kubernetes. Mirantis k0rdent Virtualization extends Kubernetes by adding additional virtualization resource types (especially the <code>VirtualMachine</code> type) through Kubernetes\u2019s Custom Resource Definitions (CRDs) API. This mechanism enables you to manage VirtualMachine resources alongside all other Kubernetes resources.</p>"},{"location":"addons/mkvirt/#concepts","title":"Concepts","text":"<p>At the heart of this approach is the Hyperconverged Cluster Operator (HCO). The HCO streamlines deployment by unifying the configuration and management of multiple operators, including KubeVirt, Containerized Data Importer (CDI), networking components, and <code>kubevirt-manager</code>, via a single entry point. This cohesive setup, delivered through the HCO <code>ServiceTemplate</code> for a Mirantis k0rdent Enterprise <code>ClusterDeployment</code>, helps maintain consistency and reduces the administrative burden.</p>"},{"location":"addons/mkvirt/#architecture","title":"Architecture","text":"<p>The architecture of Mirantis k0rdent Virtualization is designed to leverage Kubernetes-native patterns for effective virtual machine management.</p>"},{"location":"addons/mkvirt/#hyperconverged-cluster-operator","title":"Hyperconverged Cluster Operator","text":"<p>The Hyperconverged Cluster Operator aggregates a suite of components into a single management layer, following the operator pattern for managing multi-operator products. The default HCO <code>ServiceTemplate</code> installs the following components:</p> <ul> <li>Mirantis k0rdent Virtualization: Provides the foundational virtualization capabilities.</li> <li>Containerized Data Importer (CDI): Manages persistent storage by constructing virtual machine disks on PVCs.</li> <li>Cluster Network Addons (CNA): Enhances networking functionality, typically through Multus.</li> <li>Kubevirt-manager: Supplies a Web UI for easier virtual machine management.</li> </ul> <p>(For more details, see Extending Kubernetes API with CRDs)</p> <p>This consolidation reduces operational overhead and contributes to a more maintainable and resilient virtualization environment.</p>"},{"location":"addons/mkvirt/#kubevirt","title":"KubeVirt","text":"<p>KubeVirt is the core engine for virtualization within Mirantis k0rdent Virtualization. It manages the <code>VirtualMachine</code> custom resource via dedicated controllers:</p>"},{"location":"addons/mkvirt/#cluster-level-controllers","title":"Cluster Level Controllers","text":"<ul> <li>virt-controller: Monitors Virtual Machine Instances (VMIs) and orchestrates their lifecycle by managing associated pods.</li> <li>virt-api: An HTTP API server that acts as the entry point for all virtualization-related operations, handling defaulting and validation of VMI CRDs.</li> </ul>"},{"location":"addons/mkvirt/#node-level-controllers","title":"Node Level Controllers","text":"<ul> <li>virt-handler:  When a VMI is assigned to a host, <code>virt-handler</code> initiates the creation of the virtual domain using the <code>libvirtd</code> instance running in the VMI\u2019s pod.</li> <li>virt-launcher (per VMI):  Sets up the necessary cgroups and namespaces for each VMI to run reliably.</li> </ul>"},{"location":"addons/mkvirt/#limitations","title":"Limitations","text":"<p>While the solution is robust, constraints native to <code>kubevirt</code> inherently exist. For example, details on live migration limitations can be found in the KubeVirt Live Migration Limitations documentation.</p>"},{"location":"addons/mkvirt/best-practices/","title":"Virtualization Best Practices","text":"<p>This document outlines best practices to ensure stable performance and effective management of virtual machines running on Mirantis k0rdent Virtualization. Here, \u201cVM\u201d refers to Virtual Machines and also covers VMIs and VMI ReplicaSets. The guidelines provided help ensure that VMs remain live-migratable and recoverable, even under challenging conditions.</p> <p>Note: All recovery procedures required for VMs to be LiveMigratable.</p>"},{"location":"addons/mkvirt/best-practices/#run-strategy","title":"Run Strategy","text":"<p>The <code>runStrategy</code> parameter determines how VMs are started and restarted after failures. For rapid recovery, set <code>runStrategy</code> to either <code>Always</code> or <code>RerunOnFailure</code>.</p> <pre><code>runStrategy: Always\n</code></pre> <p>Using an aggressive run strategy ensures that VMs are automatically restarted in the event of a failure, reducing downtime and maintaining service continuity.</p> <p>For further details, refer to the KubeVirt run strategies guide.</p>"},{"location":"addons/mkvirt/best-practices/#common-practices","title":"Common Practices","text":"<p>To avoid issues with resource consumption (CPU, memory), specify resources for virtual machines via the VM template:</p> <p>To manage resource consumption effectively and prevent performance bottlenecks, it is important to define resource requests and limits within your VM templates. This can be done directly in the YAML specification:</p> <pre><code>resources:\n  requests:\n    memory: 10Gi\n    cpu: 8\n  limits:\n    memory: 12Gi\n    cpu: 10\n</code></pre> <p>Alternatively, you can use instance types:</p> <pre><code>instancetype:\n  kind: VirtualMachineInstancetype\n  name: clarge\n</code></pre> <p>Explicit resource specifications help ensure that VMs are scheduled efficiently across the cluster. Additionally, using instance types simplifies resource allocation and promotes consistency in workload management. It is also possible to schedule a VM to specific nodes or racks by setting <code>nodeSelector</code>:</p> <pre><code>nodeSelector:\n  nodeLabel: labelValue\n</code></pre> <p>Or specify the node when creating a VM via the UI. In such cases, ensure that available resources are checked beforehand\u2014note that some memory and CPU are reserved for other processes running on the nodes.</p>"},{"location":"addons/mkvirt/best-practices/#maintenance-of-node-handling","title":"Maintenance of Node Handling","text":"<p>During node maintenance, it is critical to migrate all running VMs from the affected node. Update the VM specification to set the <code>evictionStrategy</code> to <code>LiveMigrate</code>:</p> <p>Note</p> <p>By default, the <code>EvictionStrategy</code> is defined at the cluster level by HCO, with the default set to <code>LiveMigrate</code> for all VMIs.</p> <pre><code>evictionStrategy: LiveMigrate\n</code></pre> <p>This setting allows you to initiate migration of all running VMs from the node using the <code>kubectl drain</code> command:</p> <pre><code>kubectl drain &lt;nodeName&gt; --ignore-daemonsets=true --force --delete-emptydir-data\n</code></pre> <p>Before running the drain command, verify that VMs can be scheduled on other nodes (i.e., ensure there are no restrictions in the node selector and that sufficient resources are available). Note that a successful VM run after eviction depends on the <code>runStrategy</code> specified in the VM spec. If the VM is part of a VM ReplicaSet, only VMs scheduled on the drained node will be migrated.</p> <p>After completing node maintenance, run the following command to make the node schedulable again:</p> <pre><code>kubectl uncordon &lt;nodeName&gt;\n</code></pre> <p>Migrating VMs prior to node maintenance minimizes service disruption and ensures that workload capacity is maintained throughout the process.</p>"},{"location":"addons/mkvirt/best-practices/#accident-recovery","title":"Accident Recovery","text":"<p>For VMs running on heavily loaded nodes, or in the event of unexpected evictions (for example, due to an OOM event), setting the <code>evictionStrategy</code> to <code>LiveMigrate</code> ensures that VMs are migrated to healthier nodes. This practice, combined with a proactive run strategy, allows for smooth recovery and continued operation.</p> <p>Specifying an eviction strategy helps prevent VMs from being terminated abruptly without a chance to migrate, which could lead to extended downtime.</p>"},{"location":"addons/mkvirt/best-practices/#node-overload","title":"Node Overload","text":"<p>For nodes running VMs under heavy load (especially when VMs are executing resource-intensive workloads), it is recommended to set the eviction strategy for such VMs:</p> <pre><code>evictionStrategy: LiveMigrate\n</code></pre> <p>Refer to these guides for more information: - Node Overcommit - Node Maintenance</p> <p>This configuration allows kubelet to evict a VM from an overloaded node (for example, in the case of an OOM event) using live migration. Without specifying an eviction strategy, a VM may be evicted without migration, potentially leading to VM unavailability until resources become available. Again, the successful recovery of a VM after eviction depends on the <code>runStrategy</code> specified in the VM spec.</p>"},{"location":"addons/mkvirt/best-practices/#node-shutdown","title":"Node Shutdown","text":"<p>If a node shuts down or becomes unreachable/unavailable, it is possible to recover a VM that was running on that node using live migration. First, verify that there are available nodes for the VM to migrate to and that nodeSelector labels permit migration. Then, perform migration of the VM using either <code>virtctl</code> or the Mirantis k0rdent Virtualization (Kubevirt) UI:</p> <pre><code>virtctl migrate &lt;VM Name&gt;\n</code></pre> <p>Live migration in node shutdown scenarios helps ensure that VMs are quickly re-assigned to available nodes, although some pods may remain in a Terminating state until the node recovers.</p>"},{"location":"addons/mkvirt/best-practices/#virtual-machine-crash","title":"Virtual Machine Crash","text":"<p>If a VM shuts down unexpectedly, Mirantis k0rdent Virtualization will automatically restart it. No administrator action is required.</p>"},{"location":"addons/mkvirt/best-practices/#detecting-broken-state","title":"Detecting Broken State","text":"<p>Configuring a <code>LivenessProbe</code> in the VM specification ensures that VMs are continuously monitored and automatically recreated if they enter a failed state. For example:</p> <pre><code>livenessProbe:\n  initialDelaySeconds: 120\n  periodSeconds: 20\n  tcpSocket:\n    port: 1500\n  timeoutSeconds: 10\n</code></pre> <p>This probe verifies that the service is running and that port 1500 is available. If the port is unavailable, the VM will be automatically recreated.</p> <p>Liveness (and optionally, readiness) probes help maintain a healthy VM environment by detecting and remedying issues before they impact service availability. More information is available in the KubeVirt user guide on probes.</p>"},{"location":"addons/mkvirt/configuration/","title":"Configuration","text":"<p>This section describes how to fine-tune Mirantis k0rdent Virtualization by modifying the HyperConverged (HCO) custom resource, which is the central configuration point for Mirantis k0rdent Virtualization (KubeVirt) and its related components. By updating this resource, administrators can adjust certificate management, live migration behavior, resource allocation, and various operational strategies.</p>"},{"location":"addons/mkvirt/configuration/#hco-custom-resource","title":"HCO Custom Resource","text":"<p>Administrators can adjust the configuration of Mirantis k0rdent Virtualization by updating the HyperConverged custom resource. The default configuration includes:</p> <ul> <li> <p><code>CertConfig</code>:  Rotation policy for internal, self-signed certificates, including duration and renewBefore settings. It includes:</p> <ul> <li><code>Ca</code>:<ul> <li><code>Duration</code>: The lifespan of the certificate authority (CA) certificate, such as <code>48h0m0s</code>.</li> <li><code>Renew Before</code>: When the certificate should be renewed prior to expiration, for example, <code>24h0m0s</code>.</li> </ul> </li> <li><code>Server</code>:<ul> <li><code>Duration</code>: The validity period for server certificates, as in <code>24h0m0s</code>.</li> <li><code>Renew Before</code>: How early a server certificate is renewed before it expires, as in <code>12h0m0s</code>.</li> </ul> </li> </ul> </li> <li> <p><code>EvictionStrategy</code>: Specifies how <code>VirtualMachineInstance</code> objects (VMIs) should behave during node drains. The default setting, <code>LiveMigrate</code>, instructs the system to migrate VMIs rather than shutting them down, thereby minimizing service disruption.</p> </li> <li> <p><code>FeatureGates</code>:  A map of feature gate flags that enable experimental or optional functionality. The default settings include:  </p> <ul> <li><code>DownwardMetrics</code>: Allows a limited set of host metrics to be exposed to virtual machine guests.</li> <li><code>EnableCommonBootImageImport</code>: Enables the automatic delivery and updates of common data import cron templates.</li> </ul> </li> <li> <p><code>HigherWorkloadDensity</code>: Provides options to increase virtual machine density on nodes.</p> <ul> <li><code>MemoryOvercommitPercentage</code>: Specifies the ratio of virtual machine memory allocation relative to the physical memory assigned to the hosting pod (<code>virt-launcher</code>). Default: <code>100%</code></li> </ul> </li> <li> <p><code>NodePlacement</code>:   Describes node scheduling preferences and constraints, defaulted to the node labels expected by Mirantis k0rdent Enterprise. This configuration ensures that VM workloads are scheduled on appropriate nodes based on resource availability and policy.</p> </li> <li> <p><code>LiveMigrationConfig</code>:  Sets parameters for live migration operations, ensuring minimal downtime during migration processes:  </p> <ul> <li><code>ParallelMigrationsPerCluster</code>: Maximum number of concurrent live migrations across the cluster. Default: <code>5</code></li> <li><code>ParallelOutboundMigrationsPerNode</code>: Maximum number of migrations leaving a single node simultaneously. Default: <code>2</code></li> <li><code>CompletionTimeoutPerGiB</code>: Timeout for migration completion based on guest resource size (RAM and disk). Default: <code>150</code></li> <li><code>ProgressTimeout</code>: Timeout in seconds if no progress is observed during memory copying. Default: <code>150</code></li> </ul> </li> <li> <p><code>ResourceRequirements</code>:  Specifies resource allocation strategies for workload pods.</p> <ul> <li><code>VmiCPUAllocationRatio</code>: Defines the fraction of a physical CPU that is requested per virtual CPU (vCPU) requested by the VMI. Default: <code>10</code></li> </ul> </li> <li> <p><code>UninstallStrategy</code>: Defines how to proceed on uninstall when workloads (<code>VirtualMachine</code>, <code>DataVolume</code>, and so on) still exist.   Default: <code>BlockUninstallIfWorkloadsExist</code></p> </li> <li> <p><code>WorkloadUpdateStrategy</code>: Specifies how to handle disruptions during automated updates to workloads: </p> <ul> <li><code>WorkloadUpdateMethods</code>: Default: <code>LiveMigrate</code></li> <li><code>BatchEvictionInterval</code>: Time interval to wait before issuing the next batch of shutdowns. Default: <code>1 minute</code></li> <li><code>BatchEvictionSize</code>: The number of VMIs that can be forcefully updated per batch. Default: <code>10</code></li> </ul> </li> </ul>"},{"location":"addons/mkvirt/configuration/#integration-with-ceph","title":"Integration with Ceph","text":"<p>Integration with Ceph allows you to seamlessly configure storage backends for your virtual machine workloads. Specific details such as storage class configuration, block or filesystem options, and persistent volume claims can be customized as needed to match your environment's requirements. This integration is designed to ensure that your virtual machines have access to robust, scalable storage solutions managed by Ceph, while remaining within the Kubernetes ecosystem. Details for integrating with Ceph can be configured as needed.</p>"},{"location":"addons/mkvirt/configuration/#integration-with-stacklight","title":"Integration with StackLight","text":"<p>StackLight integration provides advanced monitoring and logging capabilities for your virtualized workloads. This integration allows you to capture detailed performance metrics and logs, aiding in both troubleshooting and capacity planning. By integrating with StackLight, administrators can gain deeper insights into the behavior of virtual machines and the underlying infrastructure, facilitating proactive management and optimization. Details for integrating with StackLight can be configured as needed.</p>"},{"location":"addons/mkvirt/getting-started/","title":"Getting Started","text":"<p>This guide provides an introduction to setting up and managing Mirantis k0rdent Virtualization (KubeVirt). It covers system requirements, installation steps, and multiple methods for managing virtual machines, ensuring you have the necessary context to deploy and operate virtualization components efficiently.</p>"},{"location":"addons/mkvirt/getting-started/#system-requirements","title":"System Requirements","text":"<p>HCO is deployed on top of a Mirantis k0rdent Enterprise <code>ClusterDeployment</code>. For specific software and hardware requirements, refer to the Mirantis k0rdent Enterprise documentation. For worker nodes, the suggested hardware requirements are:</p> <ul> <li><code>Memory</code>: 64 GB of RAM  </li> <li><code>CPU</code>: 16 vCPUs  </li> <li><code>Storage</code>: 500 GB available</li> </ul> <p>These requirements are designed to provide sufficient resources for running virtual machine workloads reliably while accommodating the overhead of Kubernetes and virtualization management components.</p>"},{"location":"addons/mkvirt/getting-started/#installation","title":"Installation","text":"<p>Before beginning, ensure that HCO has been installed and configured.</p>"},{"location":"addons/mkvirt/getting-started/#managing-virtual-machines","title":"Managing Virtual Machines","text":"<p>Once your environment is set up, you have several options to create, update, delete, and migrate Virtual Machines. The available methods include:</p> <ul> <li>Web UI (<code>kubevirt-manager</code>)</li> <li><code>virtctl</code>: A command-line tool for advanced VM operations</li> <li>YAML Templates: The Kubernetes native approach for managing resources</li> </ul> <p>Each method provides its own advantages depending on your workflow and requirements.</p>"},{"location":"addons/mkvirt/getting-started/#kubevirt-manager","title":"kubevirt-manager","text":"<p>Mirantis provides an enhanced version of <code>kubevirt-manager</code> that is managed by HCO on Mirantis k0rdent Enterprise clusters. This Web UI offers an intuitive interface for managing virtual machines. To access the <code>kubevirt-manager</code> UI, forward the service port to your local machine using the following command:</p> <pre><code>kubectl -n kubevirt port-forward svc/kubevirt-manager 8080:8080\n</code></pre> <p>Default credentials to log in (be sure to change them!) are: <code>Username</code>: <code>admin</code> <code>Password</code>: <code>mirantisdemo</code></p> <p>The <code>kubevirt-manager</code> Web UI simplifies day-to-day operations, making it easier for operators to monitor and manage virtual machines without relying solely on command-line tools or YAML files.</p>"},{"location":"addons/mkvirt/getting-started/#virtctl","title":"Virtctl","text":"<p><code>virtctl</code> is a command-line utility that provides advanced operations for <code>VirtualMachineInstance</code> objects (VMIs). Its features include:</p> <ul> <li>Accessing serial and graphical consoles</li> <li>Starting and stopping VMIs</li> <li>Live migrating VMIs and canceling live migrations</li> <li>Uploading virtual machine disk images</li> </ul> <p>To install virtctl:</p> <ol> <li> <p>Download the tool:</p> <p>Choose the appropriate version for your architecture from the virtctl Artifacts page.</p> </li> <li> <p>Download using <code>wget</code>:</p> <p><pre><code>wget https://binary-mirantis-com.s3.amazonaws.com/kubevirt/bin/artifacts/virtctl-1.3.1-20240911005512-&lt;ARCH&gt;-amd64 -O virtctl\n</code></pre> Replace <code>&lt;ARCH&gt;</code> with either <code>darwin</code> or <code>linux</code>.</p> </li> <li> <p>Move the binary:</p> <p>Place <code>virtctl</code> in a directory included in your <code>PATH</code>.</p> </li> <li> <p>Use the <code>virtctl</code> binary:</p> <p>Some examples of using the <code>virtctl</code> binary include:</p> <pre><code>virtctl start vm-cirros\nvirtctl console vm-cirros\nvirtctl restart vm-cirros\n</code></pre> </li> </ol>"},{"location":"addons/mkvirt/getting-started/#virtual-machine-templates","title":"Virtual Machine Templates","text":"<p>Kubernetes YAML templates offer a native method for creating and managing virtual machines. Below are examples of templates for different distributions, which you can apply to a child cluster using <code>kubectl</code>.</p>"},{"location":"addons/mkvirt/getting-started/#cirros","title":"Cirros","text":"<p>This template creates a lightweight virtual machine using the Cirros image:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: vm-cirros\n  labels:\n    kubevirt.io/vm: vm-cirros\nspec:\n  running: false\n  template:\n    metadata:\n      labels:\n        kubevirt.io/vm: vm-cirros\n    spec:\n      domain:\n        devices:\n          disks:\n            - disk:\n                bus: virtio\n              name: containerdisk\n            - disk:\n                bus: virtio\n              name: cloudinitdisk\n        resources:\n          requests:\n            memory: 128Mi\n      terminationGracePeriodSeconds: 0\n      volumes:\n        - name: containerdisk\n          containerDisk:\n            image: mirantis.azurecr.io/kubevirt/cirros-container-disk-demo:1.3.0-alpha.0-20240516192211\n        - name: cloudinitdisk\n          cloudInitNoCloud:\n            userData: |\n              #!/bin/sh\n              echo 'printed from cloud-init userdata'\n</code></pre>"},{"location":"addons/mkvirt/getting-started/#fedora","title":"Fedora","text":"<p>This template demonstrates a configuration for a Fedora-based virtual machine:</p> <pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: vm-fedora\n  labels:\n    kubevirt.io/vm: vm-fedora\nspec:\n  runStrategy: Always\n  template:\n    metadata:\n      labels:\n        kubevirt.io/vm: vm-fedora\n    spec:\n      domain:\n        resources:\n          requests:\n            memory: 1024M\n        devices:\n          disks:\n            - disk:\n                bus: virtio\n              name: containerdisk\n            - disk:\n                bus: virtio\n              name: cloudinitdisk\n      terminationGracePeriodSeconds: 0\n      volumes:\n        - name: containerdisk\n          containerDisk:\n            image: mirantis.azurecr.io/kubevirt/fedora-with-test-tooling-container-disk:1.3.1-20241030114153\n        - name: cloudinitdisk\n          cloudInitNoCloud:\n            userData: |\n              #cloud-config\n              password: fedora\n              chpasswd: { expire: False }\n</code></pre> <p>Other templates (for Ubuntu or Windows, for example) can be created in a similar manner.</p> <p>Using YAML templates allows for version-controlled and reproducible VM deployments, enabling infrastructure as code practices. This method is particularly useful in automated and CI/CD workflows.</p>"},{"location":"addons/mkvirt/mkvirt-install/","title":"How-To Deploy HCO on a Mirantis k0rdent Enterprise Environment","text":"<p>This guide outlines the step-by-step process for using Mirantis k0rdent Enterprise to deploy the components of Mirantis k0rdent Virtualization on child clusters.</p> <p>The first step in this process is to deploy the Hyperconverged Cluster Operator (HCO) on the target cluster. The HCO simplifies deployment of additional Mirantis k0rdent Virtualization components such as KubeVirt, CDI, networking, and <code>kubevirt-manager</code>, providing a unified method to manage virtual machine workloads on that child cluster.</p>"},{"location":"addons/mkvirt/mkvirt-install/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have the following in place:</p> <ul> <li>A deployed k0rdent child cluster with <code>cert-manager</code> installed.</li> <li>The <code>kubectl</code> utility installed and configured.</li> <li>kubeconfig files for both the KCM (k0rdent Control Manager) and the child clusters.</li> </ul>"},{"location":"addons/mkvirt/mkvirt-install/#cert-manager","title":"cert-manager","text":"<p>In order for the HCO webhook service to function properly, you need to have <code>cert-manager</code> deployed and enabled on the target child cluster.</p> <p>To determine whether <code>cert-manager</code> is deployed, <code>describe</code> the <code>ClusterDeployment</code>, as in:</p> <pre><code>kubectl describe ClusterDeployment &lt;CLUSTER-NAME&gt; -n &lt;CLUSTER-NAMESPACE&gt; \n</code></pre> <p>For example, if you've created a <code>ClusterDeployment</code> named <code>ksi-managed-cluster</code> in the <code>kcm-system</code> namespace, you'd use:</p> <p><pre><code>kubectl describe ClusterDeployment ksi-managed-cluster -n kcm-system\n</code></pre> <pre><code>...\n      - name: cert-manager\n        namespace: cert-manager\n        template: cert-manager-1-17-1\n        values: |\n          cert-manager:\n            crds:\n              enabled: true\n...\n</code></pre></p> <p>If you don't see the <code>cert-manager</code> service, you can go ahead and add it.  Start by adding the new spec in a file called <code>clusterdeployment-patch.yaml</code>:</p> <pre><code>spec:\n  serviceSpec:\n    services:\n      - name: cert-manager\n        namespace: cert-manager\n        template: cert-manager-1-17-1\n        values: |\n          cert-manager:\n            crds:\n              enabled: true\n    # continueOnError: true  # uncomment for troubleshooting\n</code></pre> <p>Then, with the <code>KUBECONFIG</code> pointing at the management cluster, apply the patch to the cluster to tell it to add this service to the <code>ClusterDeployment</code>:</p> <pre><code>kubectl patch clusterdeployment ksi-managed-cluster -n kcm-system --type=merge --patch-file clusterdeployment-patch.yaml\n</code></pre> <p>Wait for the <code>ClusterDeployment</code> to be ready:</p> <pre><code>kubectl get clusterdeployments -A\n</code></pre>"},{"location":"addons/mkvirt/mkvirt-install/#install-hco","title":"Install HCO","text":"<p>Deploying HCO in a Mirantis k0rdent Enterprise child environment relies on the HCO <code>ServiceTemplate</code>, which you can  install from the Mirantis k0rdent Enterprise Catalog:</p> <pre><code>helm install  hco-service-template oci://registry.mirantis.com/k0rdent-enterprise/hco-service-template --version 1.15.1-mira -n kcm-system\n</code></pre> <p>Verify that the HCO <code>ServiceTemplate</code> has been added and is <code>VALID</code>:</p> <p><pre><code>kubectl -n kcm-system get servicetemplate\n</code></pre> <pre><code>NAME                      VALID\ncert-manager-1-17-1       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\nhco-1-15-1-mira           true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre></p> <p>The <code>ServiceTemplate</code> manifest directs the deployment of HCO using the Helm chart:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: hco-1-15-1-mira\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: hco\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: kubevirt-repo\n      version: 1.15.1-mira\n</code></pre> <p>By specifying the chart version and the reconcile strategy, this template ensures that HCO remains up to date with minimal manual intervention.</p>"},{"location":"addons/mkvirt/mkvirt-install/#steps","title":"Steps","text":"<p>To deploy HCO, you will add the relevant templates and changes to the management cluster. These changes will then affect the relevant child cluster. To install and verify HCO, follow these steps:</p> <ol> <li> <p>Add HCO Service Definitions to the <code>Clusterdeployment</code></p> <p>Integrate HCO into your existing Mirantis k0rdent Enterprise child cluster or include it when creating a new <code>Clusterdeployment</code> object by updating the <code>spec.serviceSpec.services</code> array with the following definition:</p> <pre><code>spec:\n  ...\n  serviceSpec:\n    services:\n    - name: hco\n      namespace: kubevirt\n      template: hco-1-15-1-mira\n  ...\n</code></pre> <p>This step links the HCO service definition to the Mirantis k0rdent Enterprise cluster, instructing it to deploy the HCO operator in the <code>kubevirt</code> namespace on the child cluster represented by the <code>ClusterDeployment</code>.</p> </li> <li> <p>Verify HCO Deployment on the Child Cluster</p> <p>To verify the installation, switch your KUBECONFIG context to the Mirantis k0rdent Enterprise child cluster and verify the HCO deployment. For example, run:</p> <pre><code>kubectl -n kubevirt get pods\n</code></pre> <p>Wait until the pod named <code>hyperconverged-cluster-operator-xxxxxxxxxx-xxxxx</code> is in the <code>Ready</code> state. </p> <p>Monitoring pod status on the child cluster is critical to ensure that the operator is running and ready to manage virtualization components.</p> </li> <li> <p>Apply the HCO Custom Resource (CR)</p> <p>Once the HCO operator is ready, apply the HCO <code>CustomResource</code> to the child cluster to activate Mirantis k0rdent Virtualization and its subcomponents (such as network and storage plugins):</p> <pre><code>apiVersion: hco.kubevirt.io/v1beta1\nkind: HyperConverged\nmetadata:\n  name: kubevirt-hyperconverged\n  namespace: kubevirt\nspec:\n  featureGates:\n    downwardMetrics: true\n  infra:\n    nodePlacement:\n      nodeSelector:\n        kubernetes.io/os: linux\n  platform: k0s\n</code></pre> <p>Applying the HCO CR is the final step that triggers the deployment of all virtualization components, ensuring that your cluster is fully equipped to handle VM workloads.</p> </li> </ol>"},{"location":"addons/mkvirt/mkvirt-install/#known-issue","title":"Known Issue","text":""},{"location":"addons/mkvirt/mkvirt-install/#create-datavolume-error","title":"Create DataVolume Error","text":"<p>You may experience a problem in which the DataVolume cannot be created with the specified data source. If so, the cause is a failure in the importer image, due to the following error:</p> <pre><code>blockdev: cannot open /dev/cdi-block-volume: Permission denied\n</code></pre> <p>For more information on this problem, see the CDI project documentation.</p>"},{"location":"addons/mkvirt/mkvirt-windows/","title":"Running Windows Server","text":"<p>This guide explains how to deploy and run Windows 2022 Server on Mirantis k0rdent Virtualization (KubeVirt). It covers the essential steps\u2014from preparing storage to uploading the Windows image and finally creating the virtual machine (VM). The instructions are aimed at operators who need to set up Windows-based workloads in a Kubernetes-native virtualization environment.</p> <ol> <li> <p>Prepare the PVC for Windows</p> <p>Before deploying the VM, you must create a Persistent Volume Claim (PVC) to host the Windows OS disk. This PVC provides the required storage for the operating system and ensures that the Windows installation has persistent storage.  Additional customization might be necessary depending on your environment.</p> </li> <li> <p>Expose CDI Upload Proxy and Upload Windows Image</p> <p>To upload the Windows installation ISO to your cluster, you first need to expose the Containerized Data Importer (CDI) upload proxy and then use it to transfer the Windows image to a PVC. Follow these steps:</p> <ol> <li> <p>Expose CDI Upload Proxy</p> <p>Expose the CDI upload proxy by running the following command. This step makes the upload service accessible on a local port, which is essential for the next step.</p> <pre><code>kubectl port-forward -n kubevirt service/cdi-uploadproxy 18443:443\n</code></pre> <p>Port forwarding creates a temporary local endpoint to securely interact with the CDI upload proxy. This is particularly useful in development and testing environments.</p> </li> <li> <p>Upload Windows Image</p> <p>Upload the Windows image to a PVC by running:</p> <p><pre><code>virtctl image-upload --image-path=&lt;path to ISO image&gt;/SERVER_EVAL_x64FRE_en-us.iso \\\n  --pvc-name=isohd \\\n  --size=20Gi \\\n  --storage-class=mirablock-k8s-block-hdd \\\n  --uploadproxy-url=https://127.0.0.1:18443 \\\n  --insecure \\\n  --wait-secs=240\n</code></pre> This command uploads the Windows image to the PVC named <code>isohd</code>. The <code>--wait-secs</code> parameter allows sufficient time for the operation to complete, which is critical when dealing with large ISO files.</p> </li> </ol> </li> <li> <p>Create the Virtual Machine</p> <p>Once the storage and image are in place, the next step is to define and create the VM. This process involves specifying PVCs and VM configuration in YAML format. Follow these steps:</p> <ol> <li> <p>PVC YAML for Windows Hard Drive</p> <p>First define a PVC to be used as the Windows hard drive. This PVC stores the operating system and any associated data.</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: winhd\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Gi\n  storageClassName: mirablock-k8s-block-hdd\n</code></pre> <p>Ensure that the PVC size is adequate for your Windows workload. Adjust the storage request if needed.</p> </li> <li> <p>Virtual Machine YAML</p> <p>The VM YAML defines the Windows 2022 Server configuration, including resource allocations, boot order, and device configurations.</p> <p><pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: win2022-server\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: win2022-server\n        kubevirt.io/os: windows\n    spec:\n      domain:\n        clock:\n          utc: {}\n          timer:\n            hpet:\n              present: false\n            pit:\n              tickPolicy: delay\n            rtc:\n              tickPolicy: catchup\n            hyperv: {}\n        cpu:\n          cores: 4\n        devices:\n          disks:\n            - bootOrder: 1\n              cdrom:\n                bus: sata\n              name: cdromiso\n            - disk:\n                bus: virtio\n              name: harddrive\n              bootOrder: 2\n            - cdrom:\n                bus: sata\n              name: virtiocontainerdisk\n        machine:\n          type: q35\n        resources:\n          requests:\n            memory: 8G\n        features:\n          acpi: {}\n          apic: {}\n          hyperv:\n            relaxed: {}\n            spinlocks:\n              spinlocks: 8191\n            vapic: {}\n          smm: {}\n        # firmware:\n        #   bootloader:\n        #     efi:\n        #       secureBoot: true\n      volumes:\n        - name: cdromiso\n          persistentVolumeClaim:\n            claimName: isohd\n        - name: harddrive\n          persistentVolumeClaim:\n            claimName: winhd\n        - containerDisk:\n            image: mirantis.azurecr.io/kubevirt/virtio-container-disk:1.4.0-20241128094341\n          name: virtiocontainerdisk\n</code></pre> This configuration sets up a VM with two storage devices: one for the installation media (ISO) and one for the Windows hard drive. The VM uses a Q35 machine type for improved hardware emulation and performance, with Hyper-V features enabled for better compatibility with Windows.</p> </li> </ol> </li> </ol>"},{"location":"addons/mkvirt/mkvirt-windows/#additional-notes","title":"Additional Notes","text":"<ul> <li>Connecting to the VM: Once the VM is running, you can connect to it via VNC to interact with the Windows installation process.</li> <li>Installing the OS: During the OS installation, if no disks are detected, click the <code>Load driver</code> button and provide the necessary drivers for Windows to recognize the storage devices.</li> </ul>"},{"location":"addons/stacklight/","title":"StackLight","text":"<p>Mirantis StackLight LMA is a comprehensive Logging, Monitoring, and Alerting software  suite designed to provide a unified operational view, or \"single pane of glass,\" for  managing cloud environments. By leveraging integrated open-source tools such as Prometheus  for time-series metrics and monitoring, and OpenSearch for log aggregation and analysis,  StackLight delivers critical insights into the health and performance of the cloud  infrastructure and its services, supporting effective day-to-day operations and maintenance  activities. </p> <p>This centralized approach significantly speeds up troubleshooting and enables proactive  identification of potential issues before they impact end-users. Consequently, it enhances  overall system reliability and simplifies the complexities of cloud management.</p> <p>In Mirantis k0rdent Enterprise, StackLight is distributed as Kubernetes Operator--a <code>CustomResourceDefinition</code>  (<code>stacklights.monitoring.mirantis.com</code>) and corresponding controller--and is installed as a Helm chart.  For Mirantis k0rdent Virtualzation (KubeVirt) as part of Mirantis k0rdent Enterprise, StackLight is distributed as a <code>ServiceTemplate</code> object. </p> <p>StackLight consists of the following components:</p> <ul> <li>Prometheus: Provides a time-series database</li> <li>Prometheus Relay: Prometheus API High Availability proxy</li> <li>Grafana: For timeseries visualization</li> <li>AlertManager: Notifications system</li> <li>Alerta: Provides alerts visualization</li> <li>cAdvisor: Exports metrics</li> <li>Node Exporter: OS metrics collector</li> <li>Blackbox exporter: Provides endpoint probes</li> <li>Kube-state-metrics: K8s objects metrics collector</li> <li>OpenSearch: A database for system and audit logs</li> <li>OpenSearch Dashboards: Provides logs visualization</li> <li>Fluentd: A tool for logs collecting and processing</li> <li>Spilo: A High Availability Postgresql solution using Patroni (as a backend for Alerta and Grafana)</li> </ul>"},{"location":"addons/stacklight/api/","title":"API Reference","text":"<p>Note: You can find the complete API reference in the <code>docs/CRD_DESCRIPTION.md</code> file in the chart package, accessible by using:    <pre><code>helm pull https://binary.mirantis.com/stacklight/helm/stacklight-operator-0.1.0-mcp-16.tgz --untar\n</code></pre></p>"},{"location":"addons/stacklight/api/#packages","title":"Packages:","text":"<ul> <li>monitoring.mirantis.com/v1alpha1</li> </ul>"},{"location":"addons/stacklight/api/#resource-types","title":"Resource Types:","text":"<ul> <li>StackLight</li> </ul>"},{"location":"addons/stacklight/api/#stacklight","title":"StackLight","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>StackLight</code> is the Schema for the StackLight API.</p> Name Type Description Required apiVersion string monitoring.mirantis.com/v1alpha1 true kind string StackLight true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            StackLightSpec defines the desired state of StackLight false status object            StackLightStatus defines the observed state of StackLight false"},{"location":"addons/stacklight/api/#stacklightspec","title":"StackLight.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>StackLightSpec</code> defines the desired state of the <code>StackLight</code> object.</p> Name Type Description Required alerta object            Alerta definition. false alertmanager object            Alertmanager definition. false blackboxExporter object            Blackbox Exporter definition. false cadvisor object            cAdvisor definition. false curator object            curator definition. false exposeLoadBalancers boolean            This parameter enables the creation of LoadBalancer services for multiple components, allowing external access to these components. This setting exposes the services on external IPs. The following services are exposed: - alerta - alertmanager - grafana - opensearch-dashboards - prometheus WARNING: This option is unsafe as it does not provide any form of authentication or authorization, making the exposed components accessible to anyone with network access to the service. It is recommended that you use this setting only in controlled environments or behind an external authentication layer. false fluentd object            Fluentd definition. false grafana object            Grafana definition. false highAvailabilityEnabled boolean            This parameter controls whether the StackLight is deployed in High Availability (HA) mode or Non-HA mode. In HA mode, a minimum of 3 nodes is required for StackLight pods. The differences between HA and Non-HA modes are as follows: - Alerta: 2 replicas in HA mode (1 replica in Non-HA mode) - Kube State Metrics: 2 replicas in HA mode (1 replica in Non-HA mode) - Prometheus: 2 replicas in HA mode (1 replica in Non-HA mode) - Alertmanager: 2 replicas in HA mode (1 replica in Non-HA mode) - Spilo: 3 replicas in HA mode (1 replica in Non-HA mode) - Prometheus Relay: 2 replicas in HA mode (1 replica in Non-HA mode) - OpenSearch: 3 replicas in HA mode (1 replica in Non-HA mode) false kubeStateMetrics object            Kube State Metrics definition. false kubernetes object            Parameters specific to the Kubernetes cluster, such as distribution, and other configuration details. false kubevirtMonitoringEnabled boolean            Enables or disables monitoring for KubeVirt virtual machines and related components. Defaults to false. false nodeExporter object            Node Exporter definition. false opensearch object            OpenSearch definition. false opensearchDashboards object            OpenSearch Dashboards definition. false prometheus object            Prometheus definition. false prometheusRelay object            Prometheus Relay definition. false registry string            The base container registry URL used for all images in StackLight. This parameter allows you to specify a default registry that will be prepended to each image's repository. If omitted, a default registry will be used. Defaults to \"mirantis.azurecr.io\". false salesforceNotifier object            Salesforce Notifier definition. false spilo object            Spilo definition. false storageClassName string            What StorageClass will be used for all StackLight PVCs. Components can override this by setting their own storageClassName parameter. If nothing is set, the default StorageClass is used (if it is defined). false"},{"location":"addons/stacklight/api/#stacklightspecalerta","title":"StackLight.spec.alerta","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Alerta definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the Alerta pods are scheduled on. false resources object            Define resource requests and limits for the alerta container. Default values: - requests.cpu: 50m - requests.memory: 150Mi - limits.cpu: - - limits.memory: 500Mi false tolerations []object            Alerta pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecalertaresources","title":"StackLight.spec.alerta.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the alerta container. Default values: - requests.cpu: 50m - requests.memory: 150Mi - limits.cpu: - - limits.memory: 500Mi</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecalertaresourcesclaimsindex","title":"StackLight.spec.alerta.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims.</code></p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecalertatolerationsindex","title":"StackLight.spec.alerta.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecalertmanager","title":"StackLight.spec.alertmanager","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Alertmanager definition.</p> Name Type Description Required extraReceivers string            Provides the ability to add additional notification receivers to the Alertmanager. These custom receivers are merged with the default receivers defined by StackLight. Use this parameter to specify additional receivers that are not included in the default configuration. The receivers must be provided in a valid YAML list format, following Alertmanager's configuration file syntax. Example of a sample receiver: <pre>extraReceivers: |\n  - name: sample-slack\n    slackConfigs:\n    - apiUrl: https://hooks.slack.com/services/...\n      channel: \"#sample\"\n      sendResolved: true</pre>   WARNING: Ensure that the notification receivers are valid and properly formatted to avoid configuration issues. false extraRoutes string            Provides the ability to add additional notification routes to the Alertmanager. These custom routes are merged with the default routes defined by StackLight. Use this parameter to specify additional routes that are not included in the default configuration. The routes must be provided in a valid YAML list format, following Alertmanager's configuration file syntax. Example of a sample route: <pre>extraRoutes: |\n  - receiver: sample-slack\n    matchers:\n    - severity=\"critical\"\n    continue: true</pre>   WARNING: Ensure that the routes are valid and properly formatted to avoid configuration issues. false nodeSelector map[string]string            Define which nodes the Alertmanager pods are scheduled on. false resources object            Define resource requests and limits for the Alertmanager containers. Default values:  - requests.cpu: 50m - requests.memory: 50Mi - limits.cpu: 200m - limits.memory: 200Mi false storageClassName string            What StorageClass will be used for Alertmanager PVC(s). Defaults to the storageClassName parameter value defined at the top level. If nothing is set, the default StorageClass is used (if it is defined). false tolerations []object            Alertmanager pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecalertmanagerresources","title":"StackLight.spec.alertmanager.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the Alertmanager containers. Default values:</p> <ul> <li><code>requests.cpu</code>: 50m</li> <li><code>requests.memory</code>: 50Mi</li> <li><code>limits.cpu</code>: 200m</li> <li><code>limits.memory</code>: 200Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecalertmanagerresourcesclaimsindex","title":"StackLight.spec.alertmanager.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims</code>.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecalertmanagertolerationsindex","title":"StackLight.spec.alertmanager.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecblackboxexporter","title":"StackLight.spec.blackboxExporter","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Blackbox Exporter definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the Blackbox Exporter pods are scheduled on. false resources object            Define resource requests and limits for the Blackbox Exporter containers. Default values:  - requests.cpu: 50m - requests.memory: 50Mi - limits.cpu: 100m - limits.memory: 100Mi false tolerations []object            Blackbox Exporter pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecblackboxexporterresources","title":"StackLight.spec.blackboxExporter.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the Blackbox Exporter containers. Default values:</p> <ul> <li><code>requests.cpu</code>: 50m</li> <li><code>requests.memory</code>: 50Mi</li> <li><code>limits.cpu</code>: 100m</li> <li><code>limits.memory</code>: 100Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecblackboxexporterresourcesclaimsindex","title":"StackLight.spec.blackboxExporter.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims</code>.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecblackboxexportertolerationsindex","title":"StackLight.spec.blackboxExporter.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspeccadvisor","title":"StackLight.spec.cadvisor","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>cAdvisor definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the cAdvisor pods are scheduled on. false resources object            Define resource requests and limits for the cAdvisor containers. Default values:  - requests.cpu: 100m - requests.memory: 100Mi - limits.cpu: 500m - limits.memory: 500Mi false tolerations []object            cAdvisor pod's tolerations. By default the following taints are tolerated:  - node-role.kubernetes.io/control-plane:NoSchedule - node-role.kubernetes.io/master:NoSchedule - com.docker.ucp.manager:NoSchedule false"},{"location":"addons/stacklight/api/#stacklightspeccadvisorresources","title":"StackLight.spec.cadvisor.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the cAdvisor containers. Default values:</p> <ul> <li><code>requests.cpu</code>: 100m</li> <li><code>requests.memory</code>: 100Mi</li> <li><code>limits.cpu</code>: 500m</li> <li><code>limits.memory</code>: 500Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspeccadvisorresourcesclaimsindex","title":"StackLight.spec.cadvisor.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims</code>.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspeccadvisortolerationsindex","title":"StackLight.spec.cadvisor.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspeccurator","title":"StackLight.spec.curator","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>curator definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the curator pods are scheduled on. false resources object            Define resource requests and limits for the curator container. Default values:  - requests.cpu: 20m - requests.memory: 100Mi - limits.cpu: - - limits.memory: 400Mi false tolerations []object            curator pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspeccuratorresources","title":"StackLight.spec.curator.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the curator container. Default values:</p> <ul> <li><code>requests.cpu</code>: 20m</li> <li><code>requests.memory</code>: 100Mi</li> <li><code>limits.cpu</code>: -</li> <li><code>limits.memory</code>: 400Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspeccuratorresourcesclaimsindex","title":"StackLight.spec.curator.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims</code>.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspeccuratortolerationsindex","title":"StackLight.spec.curator.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecfluentd","title":"StackLight.spec.fluentd","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Fluentd definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the Fluentd pods are scheduled on. false resources object            Define resource requests and limits for the fluentd container. Default values:  - requests.cpu: 500m - requests.memory: 200Mi - limits.cpu: 1000m - limits.memory: 800Mi false tolerations []object            Fluentd pod's tolerations. By default the following taints are tolerated:  - node-role.kubernetes.io/control-plane:NoSchedule - node-role.kubernetes.io/master:NoSchedule - com.docker.ucp.manager:NoSchedule false"},{"location":"addons/stacklight/api/#stacklightspecfluentdresources","title":"StackLight.spec.fluentd.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the fluentd container. Default values:</p> <ul> <li><code>requests.cpu</code>: 500m</li> <li><code>requests.memory</code>: 200Mi</li> <li><code>limits.cpu</code>: 1000m</li> <li><code>limits.memory</code>: 800Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecfluentdresourcesclaimsindex","title":"StackLight.spec.fluentd.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims</code>.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecfluentdtolerationsindex","title":"StackLight.spec.fluentd.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecgrafana","title":"StackLight.spec.grafana","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Grafana definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the Grafana pods are scheduled on. false resources object            Define resource requests and limits for the grafana container. Default values:  - requests.cpu: 50m - requests.memory: 200Mi - limits.cpu: - - limits.memory: 400Mi false tolerations []object            Grafana pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecgrafanaresources","title":"StackLight.spec.grafana.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the grafana container. Default values:</p> <ul> <li><code>requests.cpu</code>: 50m</li> <li><code>requests.memory</code>: 200Mi</li> <li><code>limits.cpu</code>: -</li> <li><code>limits.memory</code>: 400Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecgrafanaresourcesclaimsindex","title":"StackLight.spec.grafana.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims</code>.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecgrafanatolerationsindex","title":"StackLight.spec.grafana.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspeckubestatemetrics","title":"StackLight.spec.kubeStateMetrics","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Kube State Metrics definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the Kube State Metrics pods are scheduled on. false resources object            Define resource requests and limits for the Kube State Metrics containers. Default values:  - requests.cpu: 50m - requests.memory: 100Mi - limits.cpu: - - limits.memory: 800Mi false tolerations []object            Kube State Metrics pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspeckubestatemetricsresources","title":"StackLight.spec.kubeStateMetrics.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the Kube State Metrics containers. Default values:</p> <ul> <li><code>requests.cpu</code>: 50m</li> <li><code>requests.memory</code>: 100Mi</li> <li><code>limits.cpu</code>: -</li> <li><code>limits.memory</code>: 800Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspeckubestatemetricsresourcesclaimsindex","title":"StackLight.spec.kubeStateMetrics.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims</code>.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspeckubestatemetricstolerationsindex","title":"StackLight.spec.kubeStateMetrics.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspeckubernetes","title":"StackLight.spec.kubernetes","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Parameters specific to the Kubernetes cluster, such as distribution, and other configuration details.</p> Name Type Description Required distribution enum            This parameter specifies the Kubernetes distribution used in the cluster. This information allows monitoring components to adapt and apply distribution-specific configurations or metrics collection methods. Valid values are:  - \"MKE3\" for Mirantis Kubernetes Engine v3.7+ - \"k0s\" for k0s Defaults to MKE3. Enum: MKE3, k0s false"},{"location":"addons/stacklight/api/#stacklightspecnodeexporter","title":"StackLight.spec.nodeExporter","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Node Exporter definition.</p> Name Type Description Required extraCollectors []string            Define Node Exporter collectors that have to be enabled in addition to the collectors enabled in StackLight. Collectors enabled in StackLight out of the box:  - arp - conntrack - cpu - diskstats - entropy - filefd - filesystem - hwmon - loadavg - meminfo - netclass - netdev - netstat - stat - sockstat - textfile - time - timex - uname - vmstat false nodeSelector map[string]string            Define which nodes the Node Exporter pods are scheduled on. false port integer            Port on which to expose metrics and web interface. Defaults to 19100. Format: int32 false resources object            Define resource requests and limits for the Node Exporter containers. Default values:  - requests.cpu: 50m - requests.memory: 50Mi - limits.cpu: - - limits.memory: 150Mi false tolerations []object            Node Exporter pod's tolerations. By default the following taints are tolerated:  - node-role.kubernetes.io/control-plane:NoSchedule - node-role.kubernetes.io/master:NoSchedule - com.docker.ucp.manager:NoSchedule false"},{"location":"addons/stacklight/api/#stacklightspecnodeexporterresources","title":"StackLight.spec.nodeExporter.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the Node Exporter containers. Default values:</p> <ul> <li><code>requests.cpu</code>: 50m</li> <li><code>requests.memory</code>: 50Mi</li> <li><code>limits.cpu</code>: -</li> <li><code>limits.memory</code>: 150Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecnodeexporterresourcesclaimsindex","title":"StackLight.spec.nodeExporter.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims</code>.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecnodeexportertolerationsindex","title":"StackLight.spec.nodeExporter.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecopensearch","title":"StackLight.spec.opensearch","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OpenSearch definition.</p> Name Type Description Required editMaxMapCount boolean            Enables init container that runs \"sysctl -w vm.max_map_count=262144\" to satisfy OpenSearch requirements. It is better have this parameter disabled and preconfigure vm.max_map_count on the nodes using native kernel parameters modification tools. Min value of vm.max_map_count for OpenSearch is 262144. false nodeSelector map[string]string            Define which nodes the OpenSearch pods are scheduled on. false persistentVolumeClaimSize string            OpenSearch PersistentVolumeClaim(s) size. This field is immutable, its value can't be changed once set. Defaults to 30Gi. false resources object            Define resource requests and limits for the OpenSearch containers. Default values:  - requests.cpu: 500m - requests.memory: 6Gi - limits.cpu: 2000m - limits.memory: 8Gi false storageClassName string            What StorageClass will be used for OpenSearch PVC(s). Defaults to the storageClassName parameter value defined at the top level. If nothing is set, the default StorageClass is used (if it is defined). false tolerations []object            OpenSearch pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecopensearchresources","title":"StackLight.spec.opensearch.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the OpenSearch containers. Default values:</p> <ul> <li><code>requests.cpu</code>: 500m</li> <li><code>requests.memory</code>: 6Gi</li> <li><code>limits.cpu</code>: 2000m</li> <li><code>limits.memory</code>: 8Gi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecopensearchresourcesclaimsindex","title":"StackLight.spec.opensearch.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims.</code></p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecopensearchtolerationsindex","title":"StackLight.spec.opensearch.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecopensearchdashboards","title":"StackLight.spec.opensearchDashboards","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OpenSearch Dashboards definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the OpenSearch Dashboards pods are scheduled on. false resources object            Define resource requests and limits for the OpenSearch Dashboards containers. Default values:  - requests.cpu: 500m - requests.memory: 500Mi - limits.cpu: 1000m - limits.memory: 1500Mi false tolerations []object            OpenSearch Dashboards pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecopensearchdashboardsresources","title":"StackLight.spec.opensearchDashboards.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the OpenSearch Dashboards containers. Default values:</p> <ul> <li><code>requests.cpu</code>: 500m</li> <li><code>requests.memory</code>: 500Mi</li> <li><code>limits.cpu</code>: 1000m</li> <li><code>limits.memory</code>: 1500Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecopensearchdashboardsresourcesclaimsindex","title":"StackLight.spec.opensearchDashboards.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims.</code></p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecopensearchdashboardstolerationsindex","title":"StackLight.spec.opensearchDashboards.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecprometheus","title":"StackLight.spec.prometheus","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Prometheus definition.</p> Name Type Description Required extraAlertingRules string            Provides the ability to add additional alerting rules to the Prometheus. These custom rules are merged with the default alerting rules defined by StackLight. Use this parameter to specify additional alerts that are not included in the default configuration. The rules must be provided in a valid YAML list format, following Prometheus' alerting rule syntax. Example of a sample alerting rule:  <pre>extraAlertingRules: |\n  - alert: SampleAlert\n    expr: sample_metric &gt; 100\n    for: 10m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Sample alert triggered\"\n      description: \"The sample_metric has exceeded the threshold for more than 10 minutes.\"</pre>   WARNING: Ensure that the alerting rules are valid and properly formatted to avoid configuration issues. false extraScrapeConfigs string            Provides the ability to add additional scrape configurations to the Prometheus. These configurations are merged with the default scrape configurations defined by StackLight. Use this parameter to specify custom targets for scraping metrics that are not covered by the default settings. The scrape configurations must be provided in a valid YAML list format, following Prometheus' scrape configuration syntax. Example of a sample scrape configuration:  <pre>extraScrapeConfigs: |\n  - job_name: 'sample-scrape-job'\n    static_configs:\n    - targets: ['localhost:9090']</pre>   WARNING: Ensure that the scrape configurations are valid and properly formatted to avoid configuration issues. false globalExternalLabels map[string]string            Labels to add to any time series or alerts when communicating with external systems (such as federation, remote storage, or Alertmanager). WARNING: Ensure that the label names are unique enough to avoid conflicts or intersections with labels that already exist in metrics and alerts, as overlapping labels can lead to unexpected behavior in metrics aggregation and alert routing. false nodeSelector map[string]string            Define which nodes the Prometheus pods are scheduled on. false persistentVolumeClaimSize string            Prometheus PersistentVolumeClaim(s) size. This field is immutable, its value can't be changed once set. Defaults to 15Gi. false resources object            Define resource requests and limits for the Prometheus containers. Default values:  - requests.cpu: 500m - requests.memory: 2Gi - limits.cpu: 1000m - limits.memory: 8Gi false retentionSize string            The maximum number of bytes of storage blocks to retain. The value is passed to the --storage.tsdb.retention.size flag. Defaults to 14GB. false retentionTime string            How long to retain samples in storage. The value is passed to the --storage.tsdb.retention.time flag. false storageClassName string            What StorageClass will be used for Prometheus PVC(s). Defaults to the storageClassName parameter value defined at the top level. If nothing is set, the default StorageClass is used (if it is defined). false tolerations []object            Prometheus pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecprometheusresources","title":"StackLight.spec.prometheus.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the Prometheus containers. Default values:</p> <ul> <li><code>requests.cpu</code>: 500m</li> <li><code>requests.memory</code>: 2Gi</li> <li><code>limits.cpu</code>: 1000m</li> <li><code>limits.memory</code>: 8Gi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecprometheusresourcesclaimsindex","title":"StackLight.spec.prometheus.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims.</code></p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecprometheustolerationsindex","title":"StackLight.spec.prometheus.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecprometheusrelay","title":"StackLight.spec.prometheusRelay","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Prometheus Relay definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the Prometheus Relay pods are scheduled on. false resources object            Define resource requests and limits for the Prometheus Relay container. Default values:  - requests.cpu: 50m - requests.memory: 100Mi - limits.cpu: 200m - limits.memory: 500Mi false tolerations []object            Prometheus Relay pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecprometheusrelayresources","title":"StackLight.spec.prometheusRelay.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the Prometheus Relay container. Default values:</p> <ul> <li><code>requests.cpu</code>: 50m</li> <li><code>requests.memory</code>: 100Mi</li> <li><code>limits.cpu</code>: 200m</li> <li><code>limits.memory</code>: 500Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecprometheusrelayresourcesclaimsindex","title":"StackLight.spec.prometheusRelay.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims.</code></p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecprometheusrelaytolerationsindex","title":"StackLight.spec.prometheusRelay.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecsalesforcenotifier","title":"StackLight.spec.salesforceNotifier","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Salesforce Notifier definition.</p> Name Type Description Required credentialsSecretName string            Specifies the name of the Kubernetes Secret that contains the Salesforce authentication details, such as the authentication URL, username, password, organization ID, and environment ID. The Secret must be in the same namespace as the custom resource. Ensure the Secret is created and populated with valid credentials before enabling the Salesforce Notifier. The Secret's data must contain the following keys with corresponding values:  - SFDC_AUTH_URL - SFDC_USERNAME - SFDC_PASSWORD - SFDC_ORGANIZATION_ID - SFDC_ENVIRONMENT_ID false enabled boolean            Controls whether the Salesforce Notifier is enabled or disabled. Defaults to false. false nodeSelector map[string]string            Define which nodes the Salesforce Notifier pods are scheduled on. false resources object            Define resource requests and limits for the Salesforce Notifier containers. Default values:  - requests.cpu: 600m - requests.memory: 300Mi - limits.cpu: 1200m - limits.memory: 600Mi false tolerations []object            Salesforce Notifier pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecsalesforcenotifierresources","title":"StackLight.spec.salesforceNotifier.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the Salesforce Notifier containers. Default values:</p> <ul> <li><code>requests.cpu</code>: 600m</li> <li><code>requests.memory</code>: 300Mi</li> <li><code>limits.cpu</code>: 1200m</li> <li><code>limits.memory</code>: 600Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecsalesforcenotifierresourcesclaimsindex","title":"StackLight.spec.salesforceNotifier.resources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims.</code></p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecsalesforcenotifiertolerationsindex","title":"StackLight.spec.salesforceNotifier.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightspecspilo","title":"StackLight.spec.spilo","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Spilo definition.</p> Name Type Description Required nodeSelector map[string]string            Define which nodes the Spilo pods are scheduled on. false patroniExporterResources object            Define resource requests and limits for the patroni-exporter container. Default values:  - requests.cpu: 100m - requests.memory: 40Mi - limits.cpu: 150m - limits.memory: 80Mi false postgresExporterResources object            Define resource requests and limits for the postgres-exporter container. Default values:  - requests.cpu: 150m - requests.memory: 40Mi - limits.cpu: 450m - limits.memory: 80Mi false spiloResources object            Define resource requests and limits for the spilo container. Default values:  - requests.cpu: 300m - requests.memory: 300Mi - limits.cpu: 600m - limits.memory: 2Gi false storageClassName string            What StorageClass will be used for Spilo PVC(s). Defaults to the storageClassName parameter value defined at the top level. If nothing is set, the default StorageClass is used (if it is defined). false tolerations []object            Spilo pod's tolerations. false"},{"location":"addons/stacklight/api/#stacklightspecspilopatroniexporterresources","title":"StackLight.spec.spilo.patroniExporterResources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the patroni-exporter container. Default values:</p> <ul> <li><code>requests.cpu</code>: 100m</li> <li><code>requests.memory</code>: 40Mi</li> <li><code>limits.cpu</code>: 150m</li> <li><code>limits.memory</code>: 80Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecspilopatroniexporterresourcesclaimsindex","title":"StackLight.spec.spilo.patroniExporterResources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims.</code></p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecspilopostgresexporterresources","title":"StackLight.spec.spilo.postgresExporterResources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the postgres-exporter container. Default values:</p> <ul> <li><code>requests.cpu</code>: 150m</li> <li><code>requests.memory</code>: 40Mi</li> <li><code>limits.cpu</code>: 450m</li> <li><code>limits.memory</code>: 80Mi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecspilopostgresexporterresourcesclaimsindex","title":"StackLight.spec.spilo.postgresExporterResources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims.</code></p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecspilospiloresources","title":"StackLight.spec.spilo.spiloResources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Define resource requests and limits for the spilo container. Default values:</p> <ul> <li><code>requests.cpu</code>: 300m</li> <li><code>requests.memory</code>: 300Mi</li> <li><code>limits.cpu</code>: 600m</li> <li><code>limits.memory</code>: 2Gi</li> </ul> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"addons/stacklight/api/#stacklightspecspilospiloresourcesclaimsindex","title":"StackLight.spec.spilo.spiloResources.claims[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p><code>ResourceClaim</code> references one entry in <code>PodSpec.ResourceClaims.</code></p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"addons/stacklight/api/#stacklightspecspilotolerationsindex","title":"StackLight.spec.spilo.tolerations[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The pod this <code>Toleration</code> is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"addons/stacklight/api/#stacklightstatus","title":"StackLight.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>StackLightStatus defines the observed state of StackLight</p> Name Type Description Required alerta object            Alerta status. false alertmanager object            Alertmanager status. false blackboxExporter object            Blackbox Exporter status. false cadvisor object            cAdvisor status. false curator object            curator status. false fluentd object            Fluentd status. false grafana object            Grafana status. false kubeStateMetrics object            Kube State Metrics status. false nodeExporter object            Node Exporter status. false opensearch object            OpenSearch status. false opensearchDashboards object            OpenSearch Dashboards status. false prometheus object            Prometheus status. false prometheusRelay object            Prometheus Relay status. false salesforceNotifier object            Salesforce Notifier status. false spilo object            Spilo status. false"},{"location":"addons/stacklight/api/#stacklightstatusalerta","title":"StackLight.status.alerta","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Alerta status.</p> Name Type Description Required version string            Alerta version. false"},{"location":"addons/stacklight/api/#stacklightstatusalertmanager","title":"StackLight.status.alertmanager","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Alertmanager status.</p> Name Type Description Required version string            Alertmanager version. false"},{"location":"addons/stacklight/api/#stacklightstatusblackboxexporter","title":"StackLight.status.blackboxExporter","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Blackbox Exporter status.</p> Name Type Description Required version string            Blackbox Exporter version. false"},{"location":"addons/stacklight/api/#stacklightstatuscadvisor","title":"StackLight.status.cadvisor","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>cAdvisor status.</p> Name Type Description Required version string            cAdvisor version. false"},{"location":"addons/stacklight/api/#stacklightstatuscurator","title":"StackLight.status.curator","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>curator status.</p> Name Type Description Required version string            curator version. false"},{"location":"addons/stacklight/api/#stacklightstatusfluentd","title":"StackLight.status.fluentd","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Fluentd status.</p> Name Type Description Required version string            Fluentd version. false"},{"location":"addons/stacklight/api/#stacklightstatusgrafana","title":"StackLight.status.grafana","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Grafana status.</p> Name Type Description Required version string            Grafana version. false"},{"location":"addons/stacklight/api/#stacklightstatuskubestatemetrics","title":"StackLight.status.kubeStateMetrics","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Kube State Metrics status.</p> Name Type Description Required version string            Kube State Metrics version. false"},{"location":"addons/stacklight/api/#stacklightstatusnodeexporter","title":"StackLight.status.nodeExporter","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Node Exporter status.</p> Name Type Description Required version string            Node Exporter version. false"},{"location":"addons/stacklight/api/#stacklightstatusopensearch","title":"StackLight.status.opensearch","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OpenSearch status.</p> Name Type Description Required version string            OpenSearch version. false"},{"location":"addons/stacklight/api/#stacklightstatusopensearchdashboards","title":"StackLight.status.opensearchDashboards","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OpenSearch Dashboards status.</p> Name Type Description Required version string            OpenSearch Dashboards version. false"},{"location":"addons/stacklight/api/#stacklightstatusprometheus","title":"StackLight.status.prometheus","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Prometheus status.</p> Name Type Description Required version string            Prometheus version. false"},{"location":"addons/stacklight/api/#stacklightstatusprometheusrelay","title":"StackLight.status.prometheusRelay","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Prometheus Relay status.</p> Name Type Description Required version string            Prometheus Relay version. false"},{"location":"addons/stacklight/api/#stacklightstatussalesforcenotifier","title":"StackLight.status.salesforceNotifier","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Salesforce Notifier status.</p> Name Type Description Required version string            Salesforce Notifier version. false"},{"location":"addons/stacklight/api/#stacklightstatusspilo","title":"StackLight.status.spilo","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Spilo status.</p> Name Type Description Required version string            Spilo version. false"},{"location":"addons/stacklight/install/","title":"Installing StackLight","text":""},{"location":"addons/stacklight/install/#prerequisites","title":"Prerequisites","text":"<p>Before installing Stacklight, make sure to satisfy these prerequisites:</p> <ul> <li>A k0s cluster: The cluster should have at least 1 worker node for non-HA installations, and a minimum of 3 worker nodes for an HA StackLight installation. </li> <li>Metrics scraping: Deploy k0s with the <code>--enable-metrics-scraper</code> feature (as described here.</li> <li>A configured <code>StorageClass</code> in the cluster: StackLight requires persistent volumes for its components. For Mirantis k0rdent Virtualization (KubeVirt), StackLight uses the <code>kubernetes-hdd</code> <code>StorageClass</code> created by Ceph.</li> <li>An external load balancer provider: If you want StackLight's component UIs to be exposed externally, make sure the cluster has this external load balancer provider configured.</li> <li>Kernel count: The node(s) where StackLight pods will be running should have a <code>vm.max_map_count</code> kernel parameter of 262144 or higher. You can also set <code>opensearch.editMaxMapCount=true</code> in your StackLight object spec (see API Documentation). This causes <code>opensearch init container</code> to set <code>vm.max_map_count=262144</code>.</li> <li>Node roles: Only nodes marked as <code>role=controller+worker</code> and <code>role=worker</code> are monitored by StackLight, as StackLight monitoring agents are running as k8s workloads in the cluster. Make sure all nodes you want monitored are marked accordingly.</li> </ul>"},{"location":"addons/stacklight/install/#installation-procedure","title":"Installation Procedure","text":"<p>StackLight has components on both the management and child clusters.</p>"},{"location":"addons/stacklight/install/#on-the-mirantis-k0rdent-enterprise-management-cluster","title":"On the Mirantis k0rdent Enterprise management cluster","text":"<p>Follow these steps to install Stacklight on the management cluster:</p> <ol> <li> <p>Create a <code>HelmRepository</code> object in your KCM cluster. Create a YAML file called <code>stacklight-helm-repo.yaml</code>:      </p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\n  name: stacklight\n  namespace: kcm-system\nspec:\n  interval: 10m0s\n  url: https://binary.mirantis.com/stacklight/helm/\n</code></pre> </li> <li> <p>Apply the YAML to the management cluster:</p> <p><pre><code>kubectl apply -f stacklight-helm-repo.yaml -n kcm-system\n</code></pre> <pre><code>HelmRepository object created.\n</code></pre></p> </li> <li> <p>Verify the object is created and ready: </p> <p><pre><code>kubectl get helmrepo stacklight -n kcm-system\n</code></pre> <pre><code>NAME         URL                                            AGE   READY   STATUS\nstacklight   https://binary.mirantis.com/stacklight/helm/   12d   True    stored artifact: revision 'sha256:1f5fa3f4adbfeffa6ffb929ee4488341f473b3795d9a16da0e5170450631cce4'  \n</code></pre></p> </li> <li> <p>Create a <code>ServiceTemplate</code> object by starting with a YAML file called <code>stacklight-operator-template.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: stacklight-operator-0-1-0\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: stacklight-operator\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: stacklight\n      version: 0.1.0-mcp-16\n</code></pre> </li> <li> <p>Add the YAML to the cluster:</p> <p><pre><code>kubectl apply -f stacklight-operator-template.yaml -n kcm-system\n</code></pre> <pre><code>ServiceTemplate object created.\n</code></pre></p> </li> <li> <p>Verify the object is created and valid:</p> <p><pre><code>kubectl get servicetemplate stacklight-operator-0-1-0 -n kcm-system\n</code></pre> <pre><code>NAME                        VALID\nstacklight-operator-0-1-0   true\n</code></pre></p> </li> <li> <p>Finally, add the <code>stacklight-operator</code> <code>Service</code> to the <code>.spec.serviceSpec.services</code> list of the targeted <code>ClusterDeployment</code> object. You can do this on the initial <code>ClusterDeployment</code> creation or by modifying the existing <code>ClusterDeployment</code> object, as in:  </p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\n...\nspec:\n  serviceSpec:\n    services:\n    ...\n    - name: stacklight-operator\n      namespace: stacklight-system\n      template: stacklight-operator-0-1-0\n    ...\n</code></pre> </li> </ol>"},{"location":"addons/stacklight/install/#on-the-targeted-kcm-mirantis-k0rdent-virtualization-managed-child-cluster","title":"On the targeted KCM Mirantis k0rdent Virtualization managed child cluster","text":"<p>Once you've installed StackLight into the management cluster, you need to install it on the child cluster you want to monitor. Follow these steps, making sure KUBECONFIG points to the proper cluster:</p> <ol> <li> <p>Create a separate StackLight namespace:     </p> <pre><code>kubectl create ns stacklight\n</code></pre> </li> <li> <p>Create a <code>StackLight</code> object and configure it according to your needs. Check the available configuration options (see the API Documentation). For example, create a YAML file called <code>minimal-stacklight.yaml</code> with a minimal required configuration:  </p> <pre><code>apiVersion: monitoring.mirantis.com/v1alpha1\nkind: StackLight\nmetadata:\n  name: stacklight\n  namespace: stacklight\nspec:\n  exposeLoadBalancers: true\n  highAvailabilityEnabled: true\n  kubernetes:\n    distribution: k0s\n  kubevirtMonitoringEnabled: true\n  opensearch:\n    editMaxMapCount: true\n    persistentVolumeClaimSize: 50Gi\n  prometheus:\n    persistentVolumeClaimSize: 50Gi\n    retentionSize: 45GB\n    retentionTime: 30d\n  storageClassName: kubernetes-hdd\n</code></pre> <p>Note</p> <p> You must set <code>kubevirtMonitoringEnabled=true</code> to enable installation of Mirantis k0rdent Virtualization monitoring-related components.</p> </li> <li> <p>Finally, add the <code>StackLight</code> YAML to the cluster:</p> <p><pre><code>kubectl apply -f minimal-stacklight.yaml -n stacklight\n</code></pre> <pre><code>StackLight object created.\n</code></pre></p> </li> </ol>"},{"location":"addons/stacklight/operations/","title":"Operations","text":"<p>Some operations require extra prerequisites and have limitations. </p>"},{"location":"addons/stacklight/operations/#salesforce-notifier","title":"Salesforce Notifier","text":"<p>The Salesforce notification system alerts users to important updates, assigned tasks, approval requests, mentions, and other relevant activities within the platform. These alerts are typically delivered via the bell icon in the desktop interface or as push notifications on the Salesforce mobile app, helping users stay informed and take timely action.</p> <p>Integrating StackLight with Salesforce's notification system typically involves configuring StackLight alerts (from Prometheus/Alertmanager) to trigger webhooks, which can then use the Salesforce API (potentially via middleware) to create custom notifications, Chatter posts, or Cases, ensuring critical cloud infrastructure issues are visible to relevant teams within their Salesforce environment.</p> <p>To do that, create a <code>Secret</code> (<code>salesforce-credentials</code> in this example) in the selected namespace (<code>stacklight</code> in this example) before enabling the Salesforce Notifier, then use the <code>Secret</code> name as the value for the <code>salesforceNotifier.credentialsSecretName</code> parameter:</p> <pre><code>kubectl -n stacklight create secret generic salesforce-credentials \\\n    --from-literal=SFDC_AUTH_URL=\"SFDC_AUTH_URL\" \\\n    --from-literal=SFDC_USERNAME=\"SFDC_USERNAME\" \\\n    --from-literal=SFDC_PASSWORD=\"SFDC_PASSWORD\" \\\n    --from-literal=SFDC_ORGANIZATION_ID=\"SFDC_ORGANIZATION_ID\" \\\n    --from-literal=SFDC_ENVIRONMENT_ID=\"SFDC_ENVIRONMENT_ID\"\n</code></pre> <p>Don't forget to substitute in your actual values!</p> <p>Note</p> <ul> <li>Enabling the Salesforce Notifier causes duplicate metrics to be generated. This is not a problem, but be aware that it will cause Prometheus'  <code>PrometheusTargetScrapesDuplicate</code> alert to fire constantly, so you'll want to configure the alert accordingly. </li> </ul>"},{"location":"addons/stacklight/operations/#stacklight-deinstallation","title":"StackLight Deinstallation","text":"<p>Removing the <code>StackLight</code> <code>CustomResource</code> doesn't remove everything installed in the cluster.  Remaining objects, which  exist in both the namespace and cluster-wide, need to be removed manually, particularly if you are going to re-install StackLight. These objects include:</p>"},{"location":"addons/stacklight/operations/#cluster-wide","title":"Cluster-wide","text":"<ul> <li><code>clusterrole.rbac.authorization.k8s.io/stacklight-fluentd</code></li> <li><code>clusterrole.rbac.authorization.k8s.io/stacklight-kube-state-metrics</code></li> <li><code>clusterrole.rbac.authorization.k8s.io/stacklight-prometheus</code></li> <li><code>clusterrolebinding.rbac.authorization.k8s.io/stacklight-fluentd</code></li> <li><code>clusterrolebinding.rbac.authorization.k8s.io/stacklight-kube-state-metrics</code></li> <li><code>clusterrolebinding.rbac.authorization.k8s.io/stacklight-prometheus</code></li> </ul>"},{"location":"addons/stacklight/operations/#namespace-local","title":"Namespace-local","text":"<ul> <li><code>persistentvolumeclaim/data-alertmanager-0</code></li> <li><code>persistentvolumeclaim/data-alertmanager-1</code></li> <li><code>persistentvolumeclaim/data-volume-opensearch-0</code></li> <li><code>persistentvolumeclaim/data-volume-opensearch-1</code></li> <li><code>persistentvolumeclaim/data-volume-opensearch-2</code></li> <li><code>persistentvolumeclaim/data-volume-prometheus-0</code></li> <li><code>persistentvolumeclaim/data-volume-prometheus-1</code></li> <li><code>persistentvolumeclaim/data-volume-spilo-0</code></li> <li><code>persistentvolumeclaim/data-volume-spilo-1</code></li> <li><code>persistentvolumeclaim/data-volume-spilo-2</code></li> <li><code>service/spilo-config</code></li> <li><code>endpoints/spilo-config</code></li> <li><code>endpoints/spilo-sync</code></li> </ul>"},{"location":"addons/ui/","title":"Mirantis k0rdent Enterprise UI","text":"<p>Mirantis k0rdent Enterprise UI is a web-based interface that simplifies the day-to-day management of Kubernetes infrastructure. Designed to run directly within your k0rdent management cluster, it gives you a powerful yet intuitive visual experience for deploying, observing, and maintaining clusters and services across multiple environments. Rather than manually creating <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, you can create clusters and serivces directly from the UI.</p> <p></p>"},{"location":"addons/ui/#why-use-mirantis-k0rdent-enterprise-ui","title":"Why Use Mirantis k0rdent Enterprise UI","text":"<p>Managing Kubernetes clusters and services at scale can be inherently complex, especially when balancing declarative infrastructure, GitOps workflows, and multi-cloud or edge deployments. The Mirantis k0rdent Enterprise UI is built to reduce that complexity and expose the key capabilities of the Mirantis k0rdent Enterprise control plane, providing:</p> <ul> <li>Operational Visibility: Get a real-time overview of your clusters and deployments, including health, status, and geographic distribution.</li> <li>Declarative Accessibility: Manage <code>ServiceTemplate</code>, <code>ClusterDeployment</code>, and Addon objects without needing to write raw YAML.</li> <li>Secure Access: Supports basic authentication out of the box, with the ability to add OIDC, making it safe for internal platform teams.</li> </ul>"},{"location":"addons/ui/#core-features","title":"Core Features","text":"<p>Mirantis k0rdent Enterprise UI includes:</p> <ul> <li>Dashboard: At-a-glance status of your entire Mirantis k0rdent Enterprise-managed footprint</li> <li>Cluster Map: Real-time map of deployed clusters and their health status</li> <li>Addon Catalog (experimental): Browse and apply platform extensions</li> <li>Service Management: Deploy and monitor services across clusters</li> <li>Template Management: Create and control reusable cluster/service templates</li> <li>Credential Viewer: Inspect and manage existing infrastructure credentials</li> <li>Embedded Terminal: Run <code>kubectl</code> commands in context</li> <li>Authentication: Role-based access with basic auth support</li> </ul>"},{"location":"addons/ui/#get-started","title":"Get Started","text":"<ul> <li>Configure k0rdent UI</li> <li>Use k0rdent UI</li> </ul>"},{"location":"addons/ui/k0rdent-ui-config/","title":"Configuring the UI","text":"<p>The Mirantis k0rdent Enterprise UI can assist in performing actions and getting visibility into what's happening, but there are certain resources that need to be created ahead of time.</p>"},{"location":"addons/ui/k0rdent-ui-config/#enable-disable-the-ui","title":"Enable / Disable the UI","text":"<p>You don't need to install the Mirantis k0rdent Enterprise UI because it's already included with k0rdent-enterprise, but you can enable or disable it by editing the <code>Management</code> object:</p> <p><pre><code>kubectl edit management kcm -n kcm-system\n</code></pre> <pre><code>...\nspec:\n  core:\n    capi: {}\n    kcm:\n      config:\n        k0rdent-ui:\n          auth:\n            basic:\n              password: mypassword\n          enabled: false\n  providers:\n  - name: cluster-api-provider-k0sproject-k0smotron\n...\n</code></pre></p> <p>This opens a text editor with the YAML for <code>kcm</code>.  Set the value of <code>spec.core.kcm.config.k0rdent-ui.enabled</code> to <code>true</code> or <code>false</code> and save the document.</p> <p>It will take a few minutes for the <code>Management</code> object to become ready:</p> <p><pre><code>kubectl get management\n</code></pre> <pre><code>NAME   READY   RELEASE                        AGE\nkcm    False   k0rdent-enterprise-1-1-0-rc9   46m\n</code></pre> <pre><code>kubectl get management\n</code></pre> <pre><code>NAME   READY   RELEASE                        AGE\nkcm    True    k0rdent-enterprise-1-1-0-rc9   46m\n</code></pre></p> <p>Once the <code>Management</code> object is ready, the changes take effect.</p>"},{"location":"addons/ui/k0rdent-ui-config/#signing-into-the-ui","title":"Signing into the UI","text":"<p>The Mirantis k0rdent Enterprise UI can handle authentication via Basic Authentication or OIDC.</p>"},{"location":"addons/ui/k0rdent-ui-config/#basic-authentication","title":"Basic Authentication","text":"<p>For security reasons, you MUST change the UI password immediately after installation (if you didn't change it as part of the installation process itself).  To do that, edit the <code>Management</code> object. This is the same process as enabling/disabling the UI.</p> <pre><code>...\nspec:\n  core:\n    capi: {}\n    kcm:\n      config:\n        k0rdent-ui:\n          auth:\n            basic:\n              password: myNEWpassword\n  providers:\n...\n</code></pre> <p>Edit the value of <code>spec.core.kcm.config.k0rdent-ui.auth.basic.password</code> to your new password, and save the document.  The new password will take effect as soon as the <code>Management</code> object is ready.</p> <p><pre><code>kubectl get management\n</code></pre> <pre><code>NAME   READY   RELEASE                        AGE\nkcm    True    k0rdent-enterprise-1-1-0-rc9   46m\n</code></pre></p>"},{"location":"addons/ui/k0rdent-ui-config/#oidc","title":"OIDC","text":"<p>As of version 1.1.0, Mirantis k0rdent Enterprise supports OIDC using Google.  To set it up, follow these steps:</p> <ol> <li> <p>Start by going to the Google Developer site and getting a client ID and secret. </p> </li> <li> <p>Edit the <code>Management</code> object:</p> <p><pre><code>kubectl edit management kcm -n kcm-system\n</code></pre> <pre><code>...\nspec:\ncore:\n  capi: {}\n  kcm:\n    config:\n      k0rdent-ui:\n        auth:\n          google:\n            secretKeyRef:\n              name: oidc-secret\n              clientIDKey: client-id\n              clientSecretKey: client-secret\n          basic:\n            password: myNEWpassword\n        nextAuth:\n          secretKeyRef:\n          name: nextauth-secret\n          key: nextauth-secret\nproviders:\n...\n</code></pre></p> <p>The <code>clientIdKey</code> and <code>clientSecretKey</code> are what you got from Google, but the <code>nextAuth</code> secret is an arbitrary value to keep the user from being logged out when the pod is restarted.</p> </li> <li> <p>Add role bindings for the users to whom you want to grant access to the Mirantis k0rdent Enterprise UI:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n    name: kcm-oidc-admin\nroleRef:\n    kind: ClusterRole\n    name: kcm-k0rdent-enterprise-global-admin-role\n    apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: User\n    name: &lt;user-email&gt;\n    apiGroup: rbac.authorization.k8s.io\n</code></pre> </li> </ol>"},{"location":"addons/ui/k0rdent-ui-config/#credentials","title":"Credentials","text":"<p>Mirantis k0rdent Enterprise requires permissions to perform actions. You provide these permissions by creating <code>Credential</code> objects.</p>"},{"location":"addons/ui/k0rdent-ui-config/#infrastructure-credentials","title":"Infrastructure credentials","text":"<p>Because Mirantis k0rdent Enterprise can work with multiple infrastructures, the <code>Credential</code> created will depend on the infrastructure on which the target cluster runs. You can get more information here. Create a <code>Credential</code> for every infrastructure on which you intend to work.</p>"},{"location":"addons/ui/k0rdent-ui-config/#cluster-credentials","title":"Cluster credentials","text":"<p>While Mirantis k0rdent Enterprise can be used to create and manage new clusters, it can also \"adopt\" existing clusters. To adopt a cluster, it needs login access to the cluster via a valid <code>KUBECONFIG</code>, just as you would use with <code>kubectl</code>. To make this available to Mirantis k0rdent Enterprise, you can create a <code>Credential</code> that includes the <code>KUBECONFIG</code>. See the documentation for adopting clusters for more information.</p>"},{"location":"addons/ui/k0rdent-ui-config/#make-the-ui-available","title":"Make the UI available","text":"<p>To make the UI available to users, create an <code>Ingress</code> that points to the <code>k0rdent-ui-*</code> <code>Pod</code>.</p> <p>Alternatively, you can also create local access to the UI by following these steps:</p> <ol> <li> <p>Install <code>kubectl</code> and set the <code>KUBECONFIG</code> to point to the Mirantis k0rdent Enterprise management cluster.</p> </li> <li> <p>Forward the UI <code>Pod</code> to the local machine:</p> <p><pre><code>kubectl port-forward svc/kcm-k0rdent-ui 3000:3000 -n kcm-system\n</code></pre> <pre><code>Forwarding from 127.0.0.1:3000 -&gt; 3000\nForwarding from [::1]:3000 -&gt; 3000\n</code></pre></p> </li> <li> <p>Access the UI at http://localhost:3000.</p> </li> </ol>"},{"location":"addons/ui/k0rdent-ui-config/#catalog-repository","title":"Catalog Repository","text":"<p>In order to use the Addons catalog, you'll need to add the <code>k0rdent-catalog</code> Helm repository.  To do that, create a file, such as <code>helmrepo.yaml</code> and add the following:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: k0rdent-catalog\n  namespace: kcm-system  \nspec:\n  type: oci\n  url: oci://ghcr.io/k0rdent/catalog/charts\n</code></pre> <p>Then add the object to your Mirantis k0rdent Enterprise management cluster:</p> <p><pre><code>kubectl apply -f helmrepo.yaml\n</code></pre> <pre><code>helmrepository.source.toolkit.fluxcd.io/k0rdent-catalog created\n</code></pre></p>"},{"location":"addons/ui/k0rdent-ui-usage/","title":"Using the Mirantis k0rdent Enterprise UI","text":"<p>The Mirantis k0rdent Enterprise offers a streamlined, visual interface for managing Kubernetes clusters and services at scale. It\u2019s designed for platform engineers and operators who need a consistent way to manage declarative infrastructure, track changes, operate multi-cluster workloads, and monitor system health with high visibility. This guide takes you through all of the steps and concepts you need to use the UI effectively, from creating clusters to deploying workloads.</p> <p>Key capabilities include:</p> <ul> <li>Dashboard: Real-time overview of cluster state, service sync status, and health indicators.</li> <li>Cluster Management: Template-based creation, editing, and deletion of clusters; support for adopting existing clusters.</li> <li>Multicluster Services: Deploy and manage services across multiple clusters from a single service template with tagging, sync control, and drift detection.</li> <li>Credentials: View infrastructure and access credentials required for operations (must be created outside the UI).</li> <li>Addons: Deploy cluster-level tools using service templates from a pre-integrated catalog.</li> <li>Templates: Reusable, versioned blueprints for both clusters and services; includes YAML editing, version control, and audit trails.</li> </ul> <p>The guide walks through these workflows in detail with UI screenshots, showing you how to use the platform effectively for declarative, multi-cluster Kubernetes operations.</p> <p>Let's start with the dashboard.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#ui-dashboard","title":"UI Dashboard","text":"<p>At any given time, Mirantis k0rdent Enterprise manages a significant number of objects, so it's handy to have a single window in which you can observe and manage most of them.</p> <p></p> <p>The dashboard serves as the entry point to the platform. It provides an at-a-glance view of the current system state: number of clusters, active service templates, synchronization status, and health of underlying infrastructure and existing clusters. </p> <p>You can use this screen to identify problem areas and navigate quickly to deeper management views.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#working-with-clusters","title":"Working with Clusters","text":"<p>Of course the main point of Mirantis k0rdent Enterprise is to create and manage Kubernetes clusters. The Mirantis k0rdent Enterprise UI enables you to manage the full lifecycle of clusters, including creation of new clusters, adoption of existing clusters, updating clusters, and decommissioning them, all through templates and declarative configuration.</p> <p></p> <p>When you click the <code>Clusters</code> button in the left-hand pane, you see a view that lists all clusters currently managed by Mirantis k0rdent Enterprise, along with basic information such as the infrastructure provider, template, tags, and status.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#create-a-new-clusterdeployment","title":"Create a New ClusterDeployment","text":"<p>To have Mirantis k0rdent Enterprise create or deploy a new cluster, click the \"+ Create Cluster\" button.</p> <p></p> <p>Start by naming the cluster; the cluster name should include only lowercase letters, numbers, and either <code>-</code> or <code>.</code> and must start and end with a letter or a number.  You can place the cluster in the <code>kcm-system</code> namespace, but it is good practice to limit users to accessing other namespaces.</p> <p>Next. choose a <code>ClusterTemplate</code> on which to base the cluster. You will see all of the available templates listed; choose a template based on the provider for which you have credentials.</p> <p></p> <p>Choose the <code>Credential</code> for the relevant infrastructure. Note that you can't create credentials in the UI; you need to create them ahead of time and select them from this form.</p> <p>Specify the cluster size and, if necessary, the <code>Region</code> or other provider-specific parameters, as well as the machine size.  If you are accessing the cluster from the public internet, don't forget to specify <code>Assign Public IP</code>.</p> <p></p> <p>Set tags and their values. Later, when creating workload services, you can specify the tags and values a cluster must have in order to run that particular workload.</p> <p>Click <code>Create Cluster</code>.</p> <p></p> <p>You can follow the status of the deployment from the <code>Clusters</code> page.</p> <p> </p> <p>You can also click on the cluster name to get more details about the cluster.  This includes both configuration information and log events as the cluster is created.</p> <p></p> <p>To edit the configuration of an existing cluster, click the edit link.</p> <p></p> <p>You can then use the form to edit basic information about the configuration.</p> <p></p> <p>You can also edit the YAML directly to make more extensive changes.</p> <p></p> <p>From the cluster information page, you can also click <code>Delete Cluster</code> to start the termination process.  When you delete a cluster, Mirantis k0rdent Enterprise deletes the machines associated with the cluster before deleting the reference in Kubernetes.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#adopting-a-cluster","title":"Adopting a cluster","text":"<p>While Mirantis k0rdent Enterprise focuses on creating clusters, you will often want to use it to manage clusters that already exist. For example, Mirantis k0rdent Enterprise makes it simple to add services to a cluster.</p> <p>To enable Mirantis k0rdent Enterprise to manage an existing cluster, you adopt that cluster. To begin that process, first make sure you have the appropriate <code>Credential</code> created to manage the cluster, then click the <code>Adopt Cluster</code> button.</p> <p></p> <p>Enter the name of the cluster; this is an arbitrary name used just for your reference.</p> <p>Just as with creating a new cluster, you can use the <code>kcm-system</code> namespace, but you should consider creating it elsewhere.</p> <p>Regardless of the infrastructure on which the adopted cluster lives, use the \"adopted\" <code>ClusterTemplate</code>; management is handled via the <code>KUBECONFIG</code> specified in the adoption <code>Credential</code>.  The provider credentials are handled separately, in the <code>ProviderCredential</code> field.  Again, you must create this <code>Credential</code> ahead of time.</p> <p>Even though you're adopting an existing cluster, you can add tags to help Mirantis k0rdent Enterprise provision workloads to the appropriate cluster.</p> <p>Finally, click <code>Adopt Cluster</code> to start the process.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#multicluster-services","title":"Multicluster Services","text":"<p>Mirantis k0rdent Enterprise makes it easy to deploy services across one or more clusters from a single source template, ensuring consistency and centralized tracking.  </p> <p>Keep in mind that in the context of Mirantis k0rdent Enterprise, a <code>Service</code> is a one or more applications deployed with Helm according to a Service Template, which is separate from a Kubernetes <code>Service</code> object.</p> <p>Click <code>Multicluster Services</code> to get a list of applications that have been installed on child clusters. </p> <p></p> <p>You can see what template was used and also the number of clusters on which it's running.  To create a new service, click <code>+ Deploy Service</code>.</p> <p> </p> <p>Specify the name for the service; it will be used to create the Kubernetes resources such as pods. Specify the priority; should it pre-empt other services if necessary, or \"step aside\" if resources are short?  In addition, when a serivce is propagated to multiple clusters, should that synchronization happen continuousy or just once?  You can also force a sync if things have changed.</p> <p>If you have specific requirements for a cluster (for example, a cluster that has an <code>Ingress</code> installed, or a cluster in the development environment) you can set the value of tags that must be present on a cluster before the application is deployed there.</p> <p>A note about these tags: they are completely arbitrary. There is no special intelligence involved in them. A cluster that has the <code>ingress=true</code> tag has that value because it's been deliberately set, not because an <code>Ingress</code> has been installed.</p> <p>You can then add one or more applications based on a Service Template.  Click <code>+ Add Another Service</code> to add an additional service to the application.</p> <p>Click <code>Create Service</code> to deploy the application.</p> <p>You can see the new service on the Services list. </p> <p></p> <p>Click the name of the service to get details.</p> <p></p> <p>The details page shows the parameters for the service, as well as the latest events and information on the clusters on which it can run.</p> <p>To remove a service, click the <code>Delete Service</code> button.</p> <p></p> <p>Once you confirm the deletion, Mirantis k0rdent Enterprise will remove the service, leaving the Service Template intact.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#credentials","title":"Credentials","text":"<p>Typically there are two types of credentials: provider credentials (such as those needed to create and manage servers, networking, and so on), and cluster credentials used to manage clusters themselves.</p> <p></p> <p>Although credentials must be created directly in the Mirantis k0rdent Enterprise management cluster, the UI gives you the ability to see what credentials are available.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#addons","title":"Addons","text":"<p>Mirantis k0rdent Enterprise comes pre-integrated with an entire catalog of useful services such as ArgoCD, Calico Networking, and Weights &amp; Biases.  To see what's available, click the <code>Addons</code> link in the left-hand pane.</p> <p></p> <p>To add one of these services to your Mirantis k0rdent Enterprise environment, click the <code>Add Template</code> link at the end of the line.</p> <p></p> <p>From here you can set the parameters for the Service Template that gets added to Mirantis k0rdent Enterprise.  You'll use this tempate to create services.  </p> <p>Set the name to something meaningful to you, but in this case you likely don't want to change any of the template values such as the chart name or helm repository, as they point to actual resources Mirantis k0rdent Enterprise uses to create the service.</p> <p>If you do need to make detailed changes, you can also click the <code>YAML Editor</code> tab.</p> <p></p> <p>From here you can edit the YAML directly, editing advanced parameters such as the <code>reconcileStrategy</code>, Kubernetes version requirements, or even the providers on which the service can be instantiated.</p> <p>Click <code>Create</code>.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#deploy-a-service-from-a-template","title":"Deploy a Service from a Template","text":"<p>Once you've created a Service Template, either directly or from the Addon Catalog, you can use it to deploy a service.  Click the <code>Service Templates</code> list in the left-hand pane.</p> <p></p> <p>Click the Service Template you'd like to implement to see the details.</p> <p></p> <p>From here you can see the details of the Service Template, or you can click <code>View YAML</code> for more details.</p> <p></p> <p>The YAML view lets you see existing details of the way in which the Service Template is implemented, but it also lets you download the YAML for the template itself. This can be particularly important if you are building your application based on Infrastructure as Code.</p> <p>In addition, you can download the existing Service Template, make changes, and add it back to the system as a new template from which to create services.</p> <p>To deploy a service from the template, go to the main Service Template information page and click <code>Deploy Service</code>.</p> <p></p> <p>From here the process is the same as it was for Multicluster Services.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#cluster-templates","title":"Cluster Templates","text":"<p>Templates are reusable, versioned cluster blueprints. Central to Mirantis k0rdent Enterprise declarative approach, they combine references to a Helm chart with other information about how to handle the cluster and other provider-specific parameters.</p> <p>In most cases, you won't need to create your own templates; Mirantis k0rdent Enterprise comes with Cluster Template objects for deploying to most providers.  However, there can be situations in which you do want to create your own templates. For example, you may have internal infrastructure on which you want to deploy clusters; creating your own Cluster Template is one way to standardize that process for your users.</p> <p>Start by clicking <code>Cluster Templates</code> in the left-hand pane.</p> <p></p> <p>Click the <code>+ Create Template</code> button to bring up the Cluster Template editor.</p> <p></p> <p>Name your template and the namespace in which you want that template to live. Note that the template does not have to be in the same namespace to which you deploy the actual cluster.</p> <p>Specify the cloud or infrastructure provider on which these clusters will run. If the cluster will run on-premise, choose the <code>infrastructure-internal</code> provider.</p> <p>You can also use the <code>YAML Editor</code> to directly make changes to the template definition.</p> <p></p> <p>Click <code>Create</code> to create the template.</p> <p>Once your template has been created, you can click <code>Create Cluster</code> on the template's information page to deploy a cluster based on that template.</p>"},{"location":"addons/ui/k0rdent-ui-usage/#conclusion","title":"Conclusion","text":"<p>Mirantis k0rdent Enterprise provides a comprehensive UI for managing Kubernetes infrastructure declaratively and at scale. Through an intuitive dashboard and clearly defined workflows, platform engineers can create, adopt, and manage clusters; deploy multi-cluster services; and maintain infrastructure consistency using reusable templates and pre-integrated add-ons. By standardizing the management of credentials, templates, and services, the UI enables efficient day-to-day operations across diverse environments.</p>"},{"location":"admin/","title":"Administering Mirantis k0rdent Enterprise","text":"<p>Before you start working with Mirantis k0rdent Enterprise, it helps to understand a few basics.</p>"},{"location":"admin/#how-mirantis-k0rdent-enterprise-works","title":"How Mirantis k0rdent Enterprise works","text":"<p>Mirantis k0rdent Enterprise has several important subsystems, notably:</p> <ul> <li>KCM - k0rdent Cluster Manager: KCM wraps and manages Kubernetes Cluster API (ClusterAPI) and lets you treat clusters as Kubernetes objects. Within a Mirantis k0rdent Enterprise management cluster, you'll have a <code>ClusterDeployment</code> object that represents a deployed cluster, with <code>Machine</code> objects, and so on. When you create a <code>ClusterDeployment</code>, Mirantis k0rdent Enterprise deploys the cluster, when you delete it, Mirantis k0rdent Enterprise deletes it, and so on.</li> <li>KSM - k0rdent State Manager: KSM wraps and manages several interoperating open source projects such as Helm and Sveltos, which lets you treat services and applications as Kubernetes objects.</li> </ul> <p>Together, KCM and KSM interoperate to create a complete, template-driven system for defining and managing Internal Development Platforms (IDPs) made up of suites of services, plus a cluster and its components as realized on a particular cloud or infrastructure substrate.</p> <ul> <li> <p>ClusterAPI providers: ClusterAPI uses <code>providers</code> to manage different clouds and infrastructures, including bare metal. Mirantis k0rdent Enterprise ships with providers for AWS, Azure, OpenStack and vSphere, and you can add additional providers in order to control other clouds or infrastructures that ClusterAPI supports.</p> </li> <li> <p>Templates: When you create a cluster, that cluster is based on a template, which specifies all of the various information about the cluster, such as where to find images, and so on. These templates get installed into Mirantis k0rdent Enterprise, but they don't do anything until you reference them in a <code>ClusterDeployment</code> that represents an actual cluster.</p> <pre><code>graph LR;\n    subgraph Infrastructure\n        WN1[\"ClusterDeployment Worker Node\"]\n        CP[\"ClusterDeployment Managed Server Control Plane\"]\n        WN2[\"ClusterDeployment Worker Node\"]\n    end\n\n    subgraph \"Mirantis k0rdent Enterprise Mgmnt Cluster\"\n        CT[\"Cluster Template\"]\n        P[\"Provider\"]\n    end\n\n    CP --&gt;|Controls| WN1\n    CP --&gt;|Controls| WN2\n    CT --&gt;|Defines| CP &amp; WN1 &amp; WN2\n    P --&gt;|Provisions| Infrastructure\n</code></pre> <p>Mirantis k0rdent Enterprise can also manage these clusters, upgrading them, scaling them, or installing software and services.</p> </li> <li> <p>Services: To add (or manage) services, you also use templates. These <code>ServiceTemplate</code> objects are like <code>ClusterTemplate</code> objects, in that you install them into the cluster, but until they're actually referenced, they don't do anything. When you reference a <code>ServiceTemplate</code> as part of a <code>ClusterDeployment</code>, Mirantis k0rdent Enterprise knows to install that service into that cluster.</p> <pre><code>graph TB;\n\n    subgraph \"Mirantis k0rdent Enterprise Mgmnt Cluster\"\n        ST[\"Service Template\"]\n    end\n\n    subgraph Infrastructure\n        CP[\"ClusterDeployment Managed Server Control Plane\"]\n\n        subgraph WN1[\"Worker Node\"]\n        POD1[App]\n        end\n\n        subgraph WN2[\"Worker Node\"]\n        POD2[App]\n        end\n    end\n    CP --&gt;|Controls| WN1\n    CP --&gt;|Controls| WN2\n    POD1 --&gt;|Defined By| ST\n    POD2 --&gt;|Defined By| ST\n</code></pre> <p>These services can be actual services, such as Nginx or Kyverno, or they can be user applications.</p> </li> </ul>"},{"location":"admin/#how-credentials-work","title":"How Credentials work","text":"<p>Of course you can't do any of this without permissions. As a human, you can log into, say, AWS, and tell it to create a new instance on which you are going to install Kubernetes, but how does Mirantis k0rdent Enterprise get that permission? It gets it through the use of <code>Credential</code> objects.</p> <p>When you create a <code>ClusterDeployment</code> or deploy an application, you include a reference to a <code>Credential</code> object that has been installed in the Mirantis k0rdent Enterprise management cluster. Depending on whether the target infrastructure is AWS, Azure, or something else, that <code>Credential</code> might reference an access key and secret, or it might reference a service provider, but all of that gets abstracted out by the time you get to the <code>Credential</code>, which is what you'll actually reference.</p> <pre><code>graph LR;\n    Secret[\"Secret\n    (includes Provider-specific passwords\n    secret keys, etc.)\"] --&gt; ClusterIdentity[\"ClusterIdentity\n    (Provider-specific)\"];\n    ClusterIdentity --&gt; Credential[\"Credential\"];</code></pre> <p>By abstracting everything out to create a standard <code>Credential</code> object, users never have to have access to actual credentials (lowercase \"c\"). This enables the administrator to keep those credentials private, and to rotate them as necessary without disturbing users or their applications. The administrator simply updates the <code>Credential</code> object and everything continues to work.</p> <p>You can find more information on creating these <code>Credential</code> objects in the Credentials chapter.</p>"},{"location":"admin/#mirantis-k0rdent-enterprise-and-gitops","title":"Mirantis k0rdent Enterprise and GitOps","text":"<p>At its heart, Mirantis k0rdent Enterprise is a Kubernetes-native way to declaratively specify what should be happening in the infrastructure and have that maintained. In other words, if you want to, say, scale up a cluster, you would give that cluster a new definition that includes the additional nodes, and then Mirantis k0rdent Enterprise, seeing that reality no longer matches the definition, it will reconcile the difference.</p> <p>In some ways this way of working is similar to GitOps, in which you commit definitions and tools such as Flux or ArgoCD ensure that reality matches the definition. We can say that Mirantis k0rdent Enterprise is GitOps-compatible, in the sense that you can (and should) consider storing Mirantis k0rdent Enterprise templates and YAML object definitions in Git repos, and can (and may want to) use GitOps tools like ArgoCD to modify and manage them upstream of Mirantis k0rdent Enterprise itself.</p> <p>The main difference is that Mirantis k0rdent Enterprise's way of representing clusters and services is fully compliant with Kubernetes-native tools like ClusterAPI, Sveltos and Helm. So you could, if you needed to, port much of what you do with Mirantis k0rdent Enterprise templates and objects directly to other solution environments that leverage these standard tools.</p>"},{"location":"admin/#the-mirantis-k0rdent-enterprise-initialization-process","title":"The Mirantis k0rdent Enterprise initialization process","text":""},{"location":"admin/#the-process","title":"The process","text":"<p>The Mirantis k0rdent Enterprise initialization process involves tools such as Helm and FluxCD.</p> <ol> <li><code>helm install kcm</code> brings up the bootstrap components (yellow in the above diagram).</li> <li><code>kcm-controller-manager</code> sets up webhooks to validate its <code>CustomResource</code> objects, then cert-manager handles the webhooks\u2019 certificates.</li> <li><code>kcm-controller-manager</code> generates a <code>Release</code> object corresponding to the KCM helm chart version.</li> <li><code>kcm-controller-manager</code> (or rather the release-controller inside it) generates template objects (<code>ProviderTemplate</code>/<code>ClusterTemplate</code>/<code>ServiceTemplate</code>) corresponding to a <code>Release</code> to be further processed.</li> <li><code>kcm-controller-manager</code> generates a <code>HelmRelease</code> object for every standard template. Note that this includes the KCM helm chart itself.</li> <li>Flux (source-controller and helm-controller pods) reconciles the HelmRelease objects. In other words, it installs all the helm charts referred to in the templates. After this point, the deployment is completely controlled by Flux.</li> <li><code>kcm-controller-manager</code> creates a <code>Management</code> object that refers to the above <code>Release</code> and the <code>ProviderTemplate</code> objects. The <code>Management</code> object represents the Mirantis k0rdent Enterprise management cluster as a whole. The management cluster Day-2 operations (such as upgrade) are  executed by manipulating the <code>Release</code> and <code>Management</code> objects.</li> <li><code>kcm-controller-manager</code> generates an empty <code>AccessManagement</code> object. <code>AccessManagement</code> defines access rules for <code>ClusterTemplate</code>/<code>ServiceTemplate</code> propagation across user namespaces. Further, the <code>AccessManagement</code> might be edited and used along with admin-created <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects.</li> </ol> <p>This Administration Guide provides information on:</p> <ul> <li>Installing and preparing Mirantis k0rdent Enterprise for use</li> <li>Working with clusters</li> <li>Working with services</li> <li>Hosted control planes</li> <li>Mirantis k0rdent Enterprise Observability &amp; FinOps</li> <li>Upgrading Mirantis k0rdent Enterprise</li> <li>Access Management (Credentials and RBAC)</li> <li>Backup and Restore</li> </ul>"},{"location":"admin/access/","title":"Access Control","text":"<p>Mirantis k0rdent Enterprise gives you fine-grained control over access to resources, whether  it's by creating credentials for cloud resources or creating roles within the cluster to control access to specific services or other Kubernetes resources.</p> <ul> <li>Mirantis k0rdent Enterprise Credentials Management</li> <li>Mirantis k0rdent Enterprise Role Based Access Control (RBAC)</li> </ul>"},{"location":"admin/access/credentials/","title":"The Credential System","text":"<p>In order for Mirantis k0rdent Enterprise to be able to take action on a particular provider, it must have the proper credentials. You can create those credentials as Kubernetes objects.</p> <ul> <li>The Credentials Process</li> <li>Credential Propagation</li> </ul>"},{"location":"admin/access/credentials/credentials-process/","title":"The process","text":"<p>In order to pass credentials to Mirantis k0rdent Enterprise so it can take action, the following has to happen:</p> <ol> <li> <p>The lead platform engineer, or whoever has access to the actual provider credentials, creates a <code>Secret</code> that includes that information. For example, for an AWS cluster, it might look like this:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n    name: aws-cluster-identity-secret\n    namespace: kcm-system\ntype: Opaque\nstringData:\n    AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n    SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Once this secret is created, it can be referenced without the user having access to the content, and thus the actual credentials.</p> </li> <li> <p>A provider-specific <code>ClusterIdentity</code> gets created. The <code>ClusterIdentity</code> references the <code>Secret</code> from step one. For example, for an AWS cluster, this object might look like this:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Notice that it references the <code>aws-cluster-identity-secret</code> we created earlier. It also specifies the namespaces in which this <code>ClusterIdentity</code> can be used. (In this case there are no restrictions.)</p> </li> <li> <p>Now you can create a <code>Credential</code> object that references the <code>ClusterIdentity</code>, thus making the credentials available and specifying the namespaces where it can be used. Continuing our AWS example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: aws-cluster-credential\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> Notice that it references the previous <code>ClusterIdentity</code> (in this case an <code>AWSClusterStaticIdentity</code>). Also notice that you can use the <code>.spec.description</code> field to add additional text about the <code>Credential</code> so users can choose if multiple <code>Credential</code> objects are available.</p> </li> <li> <p>Finally, when you create a <code>ClusterDeployment</code>, you reference the <code>Credential</code> object in order to enable Mirantis k0rdent Enterprise to pass that information to the infrastructure provider:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n    name: my-aws-clusterdeployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-12\n  credential: aws-cluster-credential\n    config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n    instanceType: t3.small\n    worker:\n    instanceType: t3.small\n</code></pre> <p>As you can see, the user doesn't have to pass anything but the name of the <code>Credential</code> in order to deploy the cluster. So all an administrator has to do is add these <code>Credential</code>objects to the system and make them available. Note also that the <code>Credential</code> has to be available in the <code>ClusterDeployment</code>s namespace. (See Cloud provider credentials propagation for more information on how that works. )</p> </li> <li> <p>Optionally, certain credentials MAY be propagated to the <code>ClusterDeployment</code> after it is created.</p> <p>The following diagram illustrates the process:</p> <pre><code>flowchart TD\n  Step1[\"&lt;b&gt;Step 1&lt;/b&gt; (Lead Engineer):&lt;br/&gt;Create ClusterIdentity and Secret objects where ClusterIdentity references Secret\"]\n  Step1 --&gt; Step2[\"&lt;b&gt;Step 2&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create Credential object referencing ClusterIdentity\"]\n  Step2 --&gt; Step3[\"&lt;b&gt;Step 3&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create ClusterDeployment referencing Credential object\"]\n  Step3 --&gt; Step4[\"&lt;b&gt;Step 4&lt;/b&gt; (Any Engineer):&lt;br/&gt;Apply ClusterDeployment, wait for provisioning &amp; reconciliation, then propagate credentials to nodes if necessary\"]</code></pre> <p>By design steps 1 and 2 should be executed by the lead engineer who has access to the credentials. Thus credentials could be used by engineers without a need to have access to actual credentials or underlying resources, like <code>ClusterIdentity</code>.</p> </li> </ol>"},{"location":"admin/access/credentials/credentials-propagation/","title":"Cloud provider credentials propagation","text":"<p>Some components in the cluster deployment require cloud provider credentials to be passed for proper functioning. For example, Cloud Controller Manager (CCM) requires provider credentials to create load balancers and provide other functionality.</p> <p>This poses a challenge for credentials delivery. Currently <code>cloud-init</code> is used to pass all necessary credentials, but this approach has several problems:</p> <ul> <li>Credentials are stored unencrypted in the instance metadata.</li> <li>Rotation of the credentials is impossible without complete instance   redeployment.</li> <li>Possible leaks, since credentials are copied to several <code>Secret</code> objects   related to bootstrap data.</li> </ul> <p>To solve these problems in Mirantis k0rdent Enterprise we're using the Sveltos controller, which can render the CCM template with all necessary data from the CAPI provider resources (like <code>ClusterIdentity</code>) and can create secrets directly on the cluster deployment.</p> <p>Note</p> <p> CCM template examples can be found in <code>*-credentials.yaml</code> here. Look for the <code>ConfigMap</code> object that has the <code>projectsveltos.io/template: \"true\"</code> annotation and <code>*-resource-template</code> as the object name.</p> <p>This eliminates the need to pass anything credentials-related to <code>cloud-init</code> and makes it possible to rotate credentials automatically without the need for instance redeployment.</p> <p>Also, this automation makes it possible to separate roles and responsibilities so that only the lead engineer has access to credentials, and other engineers can use them without seeing values and even any access to underlying infrastructure platform.</p> <p>The process is fully automated and credentials will be propagated automatically within the <code>ClusterDeployment</code> reconciliation process; user only needs to provide the correct <code>Credential</code> object.</p>"},{"location":"admin/access/credentials/credentials-propagation/#provider-specific-notes","title":"Provider-specific notes","text":"<p>Since this feature depends on the provider, it's important to review any provider-specific notes and clarifications.</p> <p>Note</p> <p>More detailed research notes can be found here.</p>"},{"location":"admin/access/credentials/credentials-propagation/#aws","title":"AWS","text":"<p>Since AWS uses roles, which are assigned to instances, no additional credentials will be created.</p> <p>The AWS provider supports 3 types of <code>ClusterIdentity</code> and, which one to use depends on your specific use case. More information regarding CAPA <code>ClusterIdentity</code> resources can be found in the CRD Reference.</p>"},{"location":"admin/access/credentials/credentials-propagation/#azure","title":"Azure","text":"<p>Currently the Cluster API Azure (CAPZ) provider creates <code>azure.json</code> <code>Secret</code> objects in the same namespace as the <code>Cluster</code> object. By design they should be referenced in the <code>cloud-init</code> YAML later during bootstrap process.</p> <p>In Mirantis k0rdent Enterprise these <code>Secret</code> objects aren't used and will not be added to the <code>cloud-init</code>, but engineers can access them without restrictions, which is a security issue.</p>"},{"location":"admin/access/credentials/credentials-propagation/#openstack","title":"OpenStack","text":"<p>For OpenStack, CAPO relies on a <code>clouds.yaml</code> file. In Mirantis k0rdent Enterprise, you provide this file in a Kubernetes <code>Secret</code> that references OpenStack credentials (ideally application credentials for enhanced security). During reconciliation, KCM automatically generates the cloud-config required by OpenStack\u2019s cloud-controller-manager.</p> <p>For more details, refer to the KCM OpenStack Credential Propagation doc.</p>"},{"location":"admin/access/credentials/credentials-propagation/#adopted-clusters","title":"Adopted clusters","text":"<p>Credentials for adopted clusters consist of a secret containing a kubeconfig file to access the existing kubernetes cluster.  The kubeconfig file for the cluster should be contained in the value key of the secret object. The following is an example of  a secret that contains the kubeconfig for an adopted cluster. To create this secret, first create or obtain a kubeconfig file  for the cluster that is being adopted and then run the following command to base64 encode it:</p> <pre><code>cat kubeconfig | base64 -w 0\n</code></pre> <p>Once you have obtained a base64 encoded kubeconfig file create a secret:</p> <pre><code>apiVersion: v1\ndata:\n  value: &lt;base64 encoded kubeconfig file&gt;\nkind: Secret\nmetadata:\n  name: adopted-cluster-kubeconf\n  namespace: &lt;namespace&gt;\ntype: Opaque\n</code></pre>"},{"location":"admin/access/credentials/credentials-propagation/#the-credential-distribution-system","title":"The Credential Distribution System","text":"<p>Mirantis k0rdent Enterprise provides a mechanism to distribute <code>Credential</code> objects across namespaces using the <code>AccessManagement</code> object. This object defines a set of <code>accessRules</code> that determine how credentials are distributed.</p> <p>Each access rule specifies:</p> <ol> <li>The target namespaces where credentials should be delivered.</li> <li>A list of <code>Credential</code> names to distribute to those namespaces.</li> </ol> <p>The KCM controller copies the specified <code>Credential</code> objects from the <code>system</code> namespace to the target namespaces based on the <code>accessRules</code> in the <code>AccessManagement</code> spec.</p> <p>Info</p> <p> Access rules can also include <code>Cluster</code> and <code>Service</code> Template Chains (<code>ClusterTemplateChain</code> objects and <code>ServiceTemplateChain</code> objects) to distribute templates to target namespaces. For more details, read: Template Life Cycle Management.</p>"},{"location":"admin/access/credentials/credentials-propagation/#how-to-configure-credential-distribution","title":"How to Configure Credential Distribution","text":"<p>To configure the distribution of <code>Credential</code> objects:</p> <ol> <li>Edit the <code>AccessManagement</code> object.</li> <li>Populate the <code>.spec.accessRules</code> field with the list of <code>Credential</code> names and the target namespaces.</li> </ol> <p>Here\u2019s an example configuration:</p> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - dev\n        - test\n    credentials:\n      - aws-demo\n      - azure-demo\n</code></pre> <p>In this example, the <code>aws-demo</code> and <code>azure-demo</code> <code>Credential</code> objects will be distributed to the <code>dev</code> and <code>test</code> namespaces.</p>"},{"location":"admin/access/rbac/","title":"Role Based Access Control","text":"<p>Mirantis k0rdent Enterprise provides the opportunity to use Role Based Access Control in order to try to use the principle of least privilege and only give users access to the objects and resources they absolutely have to have.</p> <ul> <li>What Roles Do</li> <li>Role Definitions</li> </ul>"},{"location":"admin/access/rbac/roles-summary/","title":"Roles summary","text":"<p>Note</p> <p> The names of the <code>ClusterRole</code> objects may have different prefixes depending on the name of the Mirantis k0rdent Enterprise Helm chart. The <code>ClusterRole</code> object definitions below use the <code>kcm</code> prefix, which is the default name of the Mirantis k0rdent Enterprise Helm chart.</p>"},{"location":"admin/access/rbac/roles-summary/#global-admin","title":"Global Admin","text":"<p>The <code>Global Admin</code> role provides full administrative access across all the Mirantis k0rdent Enterprise system.</p> <p>Name: <code>kcm-global-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to the Mirantis k0rdent Enterprise API</li> <li>Full access to Flux Helm repositories and Helm charts</li> <li>Full access to Cluster API identities</li> <li>Full access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Admin</code> role is authorized to perform the following actions:</p> <ol> <li>Manage the Mirantis k0rdent Enterprise configuration</li> <li>Manage namespaces in the management cluster</li> <li>Manage <code>ProviderTemplate</code> objects: add new templates or remove unneeded ones</li> <li>Manage <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in any namespace, including adding and removing templates</li> <li>Manage Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in any namespace</li> <li>Manage access rules for <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, including distributing templates across namespaces using    <code>TemplateChain</code> objects</li> <li>Manage upgrade sequences for <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects</li> <li>Manage and deploy Services across multiple clusters in any namespace by modifying <code>MultiClusterService</code> resources</li> <li>Manage <code>ClusterDeployment</code> objects in any namespace</li> <li>Manage <code>Credential</code> and <code>Secret</code> objects in any namespace</li> <li>Upgrade Mirantis k0rdent Enterprise</li> <li>Uninstall Mirantis k0rdent Enterprise</li> </ol>"},{"location":"admin/access/rbac/roles-summary/#global-viewer","title":"Global Viewer","text":"<p>The <code>Global Viewer</code> role grants read-only access across the Mirantis k0rdent Enterprise system. It does not permit any modifications, including the creation of clusters.</p> <p>Name: <code>kcm-global-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-viewer: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to Mirantis k0rdent Enterprise API</li> <li>Read access to Flux Helm repositories and Helm charts</li> <li>Read access to Cluster API identities</li> <li>Read access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Viewer</code> role is authorized to perform the following actions:</p> <ol> <li>View the Mirantis k0rdent Enterprise configuration</li> <li>List namespaces available in the management cluster</li> <li>List and get the detailed information about available <code>ProviderTemplate</code> objects</li> <li>List available <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in any namespace</li> <li>List and view detailed information about Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in any namespace</li> <li>View access rules for <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, including <code>TemplateChain</code> objects in any namespace</li> <li>View full details about the created <code>MultiClusterService</code> objects</li> <li>List and view detailed information about <code>ClusterDeployment</code> objects in any namespace</li> <li>List and view detailed information about created <code>Credential</code> and <code>Secret</code> objects in any namespace</li> </ol>"},{"location":"admin/access/rbac/roles-summary/#namespace-admin","title":"Namespace Admin","text":"<p>The <code>Namespace Admin</code> role provides full administrative access within a namespace.</p> <p>Name: <code>kcm-namespace-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployment</code>, <code>Credential</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in the namespace</li> <li>Full access to <code>TemplateChain</code> objects in the namespace</li> <li>Full access to Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Admin</code> role is authorized to perform the following actions within the namespace:</p> <ol> <li>Create and manage all <code>ClusterDeployment</code> objects in the namespace</li> <li>Create and manage <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects in the namespace</li> <li>Manage the distribution and upgrade sequences of Templates within the namespace</li> <li>Create and manage Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> <li>Manage <code>Credential</code> objects created by any user in the namespace</li> </ol>"},{"location":"admin/access/rbac/roles-summary/#namespace-editor","title":"Namespace Editor","text":"<p>The <code>Namespace Editor</code> role allows users to create and modify <code>ClusterDeployment</code> objects within namespace using predefined <code>Credential</code> and <code>Template</code> objects.</p> <p>Name: <code>kcm-namespace-editor-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployment</code> objects in the allowed namespace</li> <li>Read access to <code>Credential</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code>, and <code>TemplateChain</code> objects in the namespace</li> <li>Read access to Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Editor</code> role has the following permissions in the namespace:</p> <ol> <li>Can create and manage <code>ClusterDeployment</code> objects in the namespace using existing <code>Credential</code> and <code>Template</code> objects</li> <li>Can list and view detailed information about the <code>Credential</code> objects available in the namespace</li> <li>Can list and view detailed information about the available <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects and the <code>Template</code>  upgrade sequences</li> <li>Can list and view detailed information about the Flux <code>HelmRepository</code> and <code>HelmChart</code> objects</li> </ol>"},{"location":"admin/access/rbac/roles-summary/#namespace-viewer","title":"Namespace Viewer","text":"<p>The <code>Namespace Viewer</code> role grants read-only access to resources within a namespace.</p> <p>Name: <code>kcm-namespace-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRole</code> objects with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to <code>ClusterDeployment</code> objects in the namespace</li> <li>Read access to <code>Credential</code>, <code>ClusterTemplate</code>, <code>ServiceTemplate</code>, and <code>TemplateChain</code> objects in the namespace</li> <li>Read access to Flux <code>HelmRepository</code> and <code>HelmChart</code> objects in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Viewer</code> role has the following permissions in the namespace:</p> <ol> <li>Can list and view detailed information about all the <code>ClusterDeployment</code> objects in the allowed namespace</li> <li>Can list and view detailed information about <code>Credential</code> objects available in the specific namespace</li> <li>Can list and view detailed information about available <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, and <code>Template</code>    upgrade sequences</li> <li>Can list and view detailed information about Flux <code>HelmRepository</code> and <code>HelmChart</code> objects</li> </ol>"},{"location":"admin/access/rbac/roles-summary/#limit-credential-access","title":"Limit credential access","text":"<p>To ensure developers can only access secrets in the <code>kcm-system</code> namespace, create the following <code>RoleBinding</code>:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: namespace-viewer\n  namespace: kcm-system\nsubjects:\n  - kind: User\n    name: user\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kcm-namespace-viewer-role\nEOF\n</code></pre>"},{"location":"admin/access/rbac/what-roles-do/","title":"What roles do","text":"<p>Mirantis k0rdent Enterprise leverages the Kubernetes RBAC system and provides a set of standard <code>ClusterRole</code> objects with associated permissions. These standard <code>ClusterRole</code> objects are created as part of the Mirantis k0rdent Enterprise helm chart. Mirantis k0rdent Enterprise roles are based on labels and aggregated permissions, meaning they automatically collect rules from other <code>ClusterRole</code> objects with specific labels.</p> <p>The following table outlines the roles available in Mirantis k0rdent Enterprise, along with their respective read/write or read-only permissions:</p> Roles Global Admin Global Viewer Namespace Admin Namespace Editor Namespace Viewer Scope Global Global Namespace Namespace Namespace Mirantis k0rdent Enterprise management r/w r/o - - - Namespaces management r/w r/o - - - Provider Templates r/w r/o - - - Global Template Management r/w r/o - - - Multi Cluster Service Management r/w r/o - - - Template Chain Management r/w r/o r/w r/o r/o Cluster and Service Templates r/w r/o r/w r/o r/o Credentials r/w r/o r/w r/o r/o Flux Helm objects r/w r/o r/w r/o r/o Cluster Deployments r/w r/o r/w r/w r/o"},{"location":"admin/backup/","title":"Backing Up and Restoring a Mirantis k0rdent Enterprise Management Cluster","text":"<p>Any production system needs to provide Disaster Recovery features, and the heart of these capabilities is the ability to perform backup and restore operations. In this chapter we'll look at backing up and restoring Mirantis k0rdent Enterprise management cluster so that in case of an emergency, you can restore your system to its previous condition or recreate it on another cluster.</p> <p>While it's possible to back up a Kubernetes cluster manually, it's better to build on the work of others. In this case we're going to leverage the <code>velero</code> project for backup management on the backend and see how it integrates with Mirantis k0rdent Enterprise to ensure data persistence and recovery.</p>"},{"location":"admin/backup/#motivation","title":"Motivation","text":"<p>The primary goal of this feature is to provide a reliable and efficient way to back up and restore a Mirantis k0rdent Enterprise deployment in the event of a disaster that impacts the management cluster. By using <code>velero</code> as the backup provider, we can create consistent backups across different cloud storage options while maintaining the integrity of critical resources.</p> <p>The main goal of the feature is to provide:</p> <ul> <li>Management Backup: The ability to backup all configuration objects created and managed by Mirantis k0rdent Enterprise, including   into an offsite location.</li> <li>Restore: The ability to create configuration objects from a specific Management Backup in order to create a management   cluster in the same state that existed at the time of backup without (re)provisioning of cloud resources.</li> <li>Disaster Recovery: The ability to restore Mirantis k0rdent Enterprise on another management cluster, plus ensuring that clusters are not   recreated or lost.</li> <li>Rollback: The possibility to manually restore after a specific event, such as a failed Mirantis k0rdent Enterprise upgrade</li> </ul>"},{"location":"admin/backup/#velero-as-provider-for-management-backups","title":"Velero as Provider for Management Backups","text":"<p><code>Velero</code> is an open-source tool that simplifies backing up and restoring clusters as well as individual resources. It seamlessly integrates into the Mirantis k0rdent Enterprise management environment to provide robust disaster recovery capabilities.</p> <p>The <code>velero</code> instance is part of the Helm chart that installs Mirantis k0rdent Enterprise, which means that it can be customized if necessary.</p> <p>Mirantis k0rdent Enterprise manages the schedule and is responsible for collecting data to be included in a backup.</p> <ul> <li>Scheduled Management Backups</li> <li>Management Backup on Demand</li> <li>What's Included in a Backup</li> <li>Restoring From Backup</li> <li>Upgrades and Rollbacks</li> <li>Caveats</li> <li>Customization</li> </ul>"},{"location":"admin/backup/caveats/","title":"Caveats / Limitations","text":"<p>The credentials stored in backups can and will get stale, so a proper rotation should be considered beforehand.</p> <p>Only plugins that support Object Store can be used to store backups into an object storage.</p> <p>All <code>velero</code> caveats and limitations are transitively implied in Mirantis k0rdent Enterprise. In particular, that means no backup encryption is provided until it is implemented by a <code>velero</code> plugin that supports encryption and cloud storage backups.</p>"},{"location":"admin/backup/caveats/#velero-backups-restores-deletion","title":"Velero Backups / Restores deletion","text":""},{"location":"admin/backup/caveats/#delete-restores","title":"Delete Restores","text":"<p>To delete a <code>velero</code> <code>Restore</code> from the management cluster and from cloud storage, delete <code>restores.velero.io</code> object(s), such as with the following command:</p> <pre><code>kubectl delete restores.velero.io -n kcm-system &lt;restore-name&gt;\n</code></pre> <p>Warning</p> <p> Deletion of a <code>Restore</code> object deletes it from both the management cluster and from cloud storage.</p>"},{"location":"admin/backup/caveats/#delete-backups","title":"Delete Backups","text":"<p>To remove a <code>velero</code> <code>Backup</code> from the management cluster, delete <code>backups.velero.io</code> object(s), such as with the following command:</p> <pre><code>kubectl delete backups.velero.io -n kcm-system &lt;velero-backup-name&gt;\n</code></pre> <p>Hint</p> <p> The command above only removes objects from the cluster; the data continues to persist on the cloud storage.</p> <p>The deleted object will be recreated in the cluster if its <code>BackupStorageLocation</code> <code>.spec.backupSyncPeriod</code> is set and does not equal <code>0</code>.</p> <p>To delete a <code>velero</code> <code>Backup</code> from the management cluster and from cloud storage, create the following <code>DeleteBackupRequest</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: DeleteBackupRequest\nmetadata:\n  name: delete-backup-completely\n  namespace: kcm-system\nspec:\n  backupName: &lt;velero-backup&gt;\n</code></pre> <p>Warning</p> <p> Deletion of a <code>Backup</code> object via the <code>DeleteBackupRequest</code> deletes it from both the management cluster and from the cloud storage.</p> <p>Optionally, delete the created <code>DeleteBackupRequest</code> object from the cluster after <code>Backup</code> has been deleted.</p> <p>For reference, follow the official documentation.</p>"},{"location":"admin/backup/customization/","title":"Customization","text":"<p>This section covers different topics of customization regarding backing up and restoring Mirantis k0rdent Enterprise.</p>"},{"location":"admin/backup/customization/#velero-installation","title":"Velero installation","text":"<p>The Velero helm chart is supplied with the Mirantis k0rdent Enterprise helm chart and is enabled by default. There are 2 ways of customizing the chart values:</p> <ol> <li> <p>Install using <code>helm</code> and add corresponding parameters to the <code>helm install</code> command.</p> <p>Note</p> <p> Only a plugin that supports Object Store is required during restoration; the other parameters are optional.</p> <p>For example, this command installs Mirantis k0rdent Enterprise via <code>helm install</code> with a configured plugin, <code>BackupStorageLocation</code> and propagated credentials:</p> <pre><code>helm install kcm oci://registry.mirantis.com/k0rdent-enterprise/charts/k0rdent-enterprise \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set-file velero.credentials.secretContents.cloud=&lt;full-path-to-file&gt; \\\n --set velero.credentials.useSecret=true \\\n --set velero.backupsEnabled=true \\\n --set velero.configuration.backupStorageLocation[0].name=&lt;backup-storage-location-name&gt; \\\n --set velero.configuration.backupStorageLocation[0].provider=&lt;provider-name&gt; \\\n --set velero.configuration.backupStorageLocation[0].bucket=&lt;bucket-name&gt; \\\n --set velero.configuration.backupStorageLocation[0].config.region=&lt;region&gt; \\\n --set velero.initContainers[0].name=velero-plugin-for-&lt;provider-name&gt; \\\n --set velero.initContainers[0].image=velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt; \\\n --set velero.initContainers[0].volumeMounts[0].mountPath=/target \\\n --set velero.initContainers[0].volumeMounts[0].name=plugins\n</code></pre> </li> <li> <p>Create or modify the existing <code>Management</code> object in the <code>.spec.config.kcm</code>.</p> <p>Note</p> <p> Only a plugin that supports Object Store is required during restoration; the other parameters are optional.</p> <p>For example, this is a <code>Management</code> object with a configured plugin and enabled metrics:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  # ...\n  core:\n    kcm:\n      config:\n        velero:\n          initContainers:\n          - name: velero-plugin-for-&lt;provider-name&gt;\n            image: velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt;\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - mountPath: /target\n              name: plugins\n          metrics:\n            enabled: true\n  # ...\n</code></pre> </li> </ol> <p>To fully disable <code>velero</code>, set the <code>velero.enabled</code> parameter to <code>false</code>.</p>"},{"location":"admin/backup/customization/#schedule-expression-format","title":"Schedule Expression Format","text":"<p>The <code>ManagementBackup</code> <code>.spec.schedule</code> field accepts a correct Cron expression, along with the nonstandard predefined scheduling definitions and an extra definition <code>@every</code> with a number and a valid time unit (valid time units are <code>ns</code>, <code>us</code> (or <code>\u00b5s</code>), <code>ms</code>, <code>s</code>, <code>m</code>, <code>h</code>).</p> <p>The following list contains acceptable <code>.spec.schedule</code> example values:</p> <ul> <li><code>0 */1 * * *</code> (standard Cron expression)</li> <li><code>@hourly</code> (nonstandard predefined definition)</li> <li><code>@every 1h</code> (extra definition)</li> </ul>"},{"location":"admin/backup/customization/#putting-extra-objects-in-a-management-backup","title":"Putting Extra Objects in a Management Backup","text":"<p>If you need to back up objects other than those backed up by default, you can add the label <code>k0rdent.mirantis.com/component=\"kcm\"</code> to these objects.</p> <p>All objects containing the label will be automatically added to the management backup.</p>"},{"location":"admin/backup/ondemand-backups/","title":"Management Backup on Demand","text":"<p>To create a single backup of the existing Mirantis k0rdent Enterprise management cluster information, you can create a <code>ManagementBackup</code> object using a YAML document and the <code>kubectl</code> CLI. The object then creates only one instance of a backup. For example you can backup to the location created previously. Create a YAML file called <code>one-time-backup.yaml</code>:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ManagementBackup\nmetadata:\n  name: example-backup\nspec:\n  storageLocation: aws-s3\n</code></pre> Create a one time backup of your Mirantis k0rdent Enterprise cluster by applying the YAML to the cluster: <pre><code>kubectl apply -f one-time-backup.yaml\n</code></pre> Confirm the backup was successful by navigating to the appropriate storage console UI or from the command line: <pre><code>kubectl get managementbackup\n</code></pre> The <code>managementbackup</code> should show as <code>Completed</code>: <pre><code>NAME              LASTBACKUPSTATUS   NEXTBACKUP   AGE\nexample-backup    Completed                       8m\n</code></pre></p>"},{"location":"admin/backup/prepare-backups/","title":"Preparing for Backups","text":""},{"location":"admin/backup/prepare-backups/#preparation","title":"Preparation","text":"<p>Note</p> <p>The following instructions are tailored for AWS. Please adapt them to your chosen platform and storage.</p> <p>Before you create a manual one-off or scheduled backup, review the steps below and update your configuration accordingly:</p> <ol> <li> <p>Verify whether the <code>velero</code> plugins have been installed as suggested in Velero installation. If the <code>velero</code> plugins with the desired storage option are already configured, please skip the next step.</p> </li> <li> <p>If no <code>velero</code> plugins have yet been installed in your k0rdent cluster, start by getting the kcm management yaml file:</p> <p><pre><code>kubectl get management kcm -n kcm-system -o yaml &gt; management.yaml\n</code></pre> then edit the <code>management.yaml</code> file so that the velero plugin details are filled in under <code>spec.core.kcm</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  # ... \n  core:\n    kcm:\n      config:\n        velero:\n          initContainers:\n          - name: velero-plugin-for-&lt;PROVIDER-NAME&gt;\n            image: velero/velero-plugin-for-&lt;PROVIDER-NAME&gt;:&lt;PROVIDER-PLUGIN-TAG&gt;\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - mountPath: /target\n              name: plugins\n  # ...\n</code></pre> <p>Please review Velero's Docker Hub image plugin repositories to help identify the required <code>&lt;PROVIDER-NAME&gt;</code>. Once the required image has been identified, select from the available tags to determine the correct <code>&lt;PROVIDER-PLUGIN-TAG&gt;</code>. In the case of AWS, the provider-name would be <code>velero-plugin-for-aws</code>, we can select from the available tags.</p> </li> <li> <p>Prepare a storage location, such as an Amazon S3 bucket, to store Mirantis k0rdent Enterprise backups.</p> </li> <li> <p>Prepare a yaml file containing a <code>BackupStorageLocation</code>    object referencing a <code>Secret</code> with credentials to access the cloud storage    (if the multiple credentials feature is supported by the plugin). For example, you can create the <code>BackupStorageLocation</code> and the related <code>Secret</code> yaml for the Amazon S3 configuration by following these steps.</p> <p>First create a file called <code>credentials.txt</code> with your credentials, as in:</p> <pre><code>[default]\naws_access_key_id = EXAMPLE_ACCESS_KEY_ID\naws_secret_access_key = EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>The IAM user being used in this configuration will require certain permissions for the   appropriate Velero S3 bucket access. Review the necessary permissions here. Reference the JSON policy file named <code>velero-policy.json</code> and take care to replace <code>${BUCKET}</code> with the correct bucket name).</p> <p>Note</p> <p> If you're using EKS, the \"user\" is actually a role. If you get an error such as  <pre><code>AccessDenied: User: arn:aws:sts::026090528175:assumed-role/eksctl-JohnDoeEKSK0rdentMgmtClus-NodeInstanceRole-j0olMRJHrM0A/i-0f7dad2d91447f173 is not authorized to perform: s3:ListBucket on resource: \"arn:aws:s3:::nick-chase-backup-bucket\" because no identity-based policy allows the s3:ListBucket action\n</code></pre> You an extract the role from the message (in this example, it's the <code>assumed-role</code>) and create the policy. For example: <pre><code>aws iam put-role-policy\n--role-name eksctl-JohnDoeEKSK0rdentMgmtClus-NodeInstanceRole-j0olMRJHrM0A\n--policy-name velero\n--policy-document file://velero-policy.json\n</code></pre></p> <p>Generate the necessary base64-encoded credentials using:   <pre><code>base64 -w0 credentials.txt; echo\n</code></pre></p> <p>Use this base64 value in the <code>data.cloud</code> field in the <code>creds-and-backup-storage-location.yaml</code> you'll create next. Also make sure to substitute the appropriate <code>REGION-NAME</code> and <code>BUCKET-NAME</code>:   <pre><code>---\napiVersion: v1\ndata:\n  # base64-encoded credentials for Amazon S3 in the following format:\n  # [default]\n  # aws_access_key_id = EXAMPLE_ACCESS_KEY_ID\n  # aws_secret_access_key = EXAMPLE_SECRET_ACCESS_KEY\n  cloud: &lt;BASE64_VALUE&gt;\nkind: Secret\nmetadata:\n  name: cloud-credentials\n  namespace: kcm-system\ntype: Opaque\n---\napiVersion: velero.io/v1\nkind: BackupStorageLocation\nmetadata:\n  name: aws-s3\n  namespace: kcm-system\nspec:\n  config:\n    region: &lt;REGION-NAME&gt;\n  default: true # optional, if not set, then storage location name must always be set in ManagementBackup\n  objectStorage:\n    bucket: &lt;BUCKET-NAME&gt;\n  provider: aws\n  backupSyncPeriod: 1m\n  credential:\n    name: cloud-credentials\n    key: cloud\n</code></pre></p> </li> <li> <p>Create the necessary Kubernetes resources in your k0rdent cluster by applying the YAML to the management cluster:     <pre><code>kubectl apply -f creds-and-backup-storage-location.yaml\nkubectl apply -f management.yaml\n</code></pre></p> </li> <li> <p>Confirm that the previous steps were applied correctly:     <pre><code>kubectl get management kcm -n kcm-system -o yaml\n</code></pre>     The management configuration yaml should have the new velero plugin details, as shown in step 2. </p> <p>Now make sure the <code>backupstoragelocation</code> shows as <code>Available</code>: <pre><code>kubectl get backupstoragelocation -n kcm-system\n</code></pre> <pre><code>NAME     PHASE       LAST VALIDATED   AGE   DEFAULT\naws-s3   Available   27s              2d    true\n</code></pre> You can get more information on how to build these objects at the official Velero documentation.</p> </li> </ol>"},{"location":"admin/backup/restore/","title":"Restoring From Backup","text":"<p>Note</p> <p> Please refer to the official migration documentation to familiarize yourself with potential limitations of the Velero backup system.</p> <p>In the event of disaster, you can restore from a backup by doing the following:</p> <ol> <li> <p>Create a clean Mirantis k0rdent Enterprise installation, including <code>velero</code> and its plugins.    Specifically, you want to avoid creating a <code>Management</code> object and similar objects because they    will be part of your restored cluster. You can remove these objects after installation, but you    can also install Mirantis k0rdent Enterprise without them in the first place:</p> <pre><code>helm install kcm oci://registry.mirantis.com/k0rdent-enterprise/charts/k0rdent-enterprise \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set controller.createManagement=false \\\n --set controller.createAccessManagement=false \\\n --set controller.createRelease=false \\\n --set controller.createTemplates=false \\\n --set velero.initContainers[0].name=velero-plugin-for-&lt;provider-name&gt; \\\n --set velero.initContainers[0].image=velero/velero-plugin-for-&lt;provider-name&gt;:&lt;provider-plugin-tag&gt; \\\n --set velero.initContainers[0].volumeMounts[0].mountPath=/target \\\n --set velero.initContainers[0].volumeMounts[0].name=plugins\n</code></pre> </li> <li> <p>Create the <code>BackupStorageLocation</code>/<code>Secret</code> objects that were created during the preparation stage    of creating a backup (preferably the same depending on a plugin).</p> </li> <li> <p>Restore the <code>kcm</code> system creating the <code>Restore</code> object.    Note that it is important to set the <code>.spec.existingResourcePolicy</code> field value to <code>update</code>:</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  includedNamespaces:\n  - '*'\n</code></pre> </li> <li> <p>Wait until the <code>Restore</code> status is <code>Completed</code> and all <code>kcm</code> components are up and running.</p> </li> </ol>"},{"location":"admin/backup/restore/#if-the-restore-fails","title":"If the Restore fails","text":"<p>At the time of this writing, there is a mis-match between what Mirantis k0rdent Enterprise expects     and the objects <code>velero</code> provides, which may result in a <code>PartiallyFailed</code> result. A fix for this problem     is coming soon, but in the meantime, you will need to rename the <code>kcm-velero</code> <code>Deployment</code> to <code>velero</code>.  </p> <p>Follow these steps:</p> <ol> <li> <p>Export the YAML for the object, then delete it:</p> <p><pre><code>kubectl -n kcm-system get deployment kcm-velero -o yaml &gt; velero-deployment.yaml\nkubectl delete -n kcm-system deployment kcm-velero\n</code></pre> <pre><code>deployment.apps \"kcm-velero\" deleted\n</code></pre></p> </li> <li> <p>Edit the <code>velero-deployment.yaml</code> file to change <code>metadata.name</code> from <code>kcm-velero</code> to <code>velero</code>:</p> <pre><code>...\n    component: velero\n    helm.sh/chart: velero-9.1.2\n  name: velero\n  namespace: kcm-system\n  resourceVersion: \"1653\"\n...\n</code></pre> </li> <li> <p>Recreate the <code>Deployment</code> with the new name:</p> <p><pre><code>kubectl apply -n kcm-system -f velero-deployment.yaml\n</code></pre> <pre><code>deployment.apps/velero created\n</code></pre></p> </li> <li> <p>Delete the failed <code>Restore</code>:</p> <pre><code>kubectl delete restore -n &lt;your-namespace&gt; &lt;restore-name&gt;\n</code></pre> </li> <li> <p>Recreate the <code>Restore</code> object as before. It should now complete successfully.</p> </li> </ol>"},{"location":"admin/backup/restore/#caveats","title":"Caveats","text":"<p>For some CAPI providers it is necessary to make changes to the <code>Restore</code> object due to the large number of different resources and logic in each provider. The resources described below are not excluded from a <code>ManagementBackup</code> by default to avoid logical dependencies on one or another provider, and to create a provider-agnostic system.</p> <p>Note</p> <p> The described caveats apply only to the <code>Restore</code> object creation step and do not affect the other steps.</p>"},{"location":"admin/backup/restore/#azure-capz","title":"Azure (CAPZ)","text":"<p>The following resources should be excluded from the <code>Restore</code> object:</p> <ul> <li><code>natgateways.network.azure.com</code></li> <li><code>resourcegroups.resources.azure.com</code></li> <li><code>virtualnetworks.network.azure.com</code></li> <li><code>virtualnetworkssubnets.network.azure.com</code></li> </ul> <p>Due to the webhook conversion, objects of these resources cannot be restored, and they will be created in the management cluster by the <code>CAPZ</code> provider automatically with the same <code>spec</code> as in the backup.</p> <p>The resulting <code>Restore</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  excludedResources:\n  - natgateways.network.azure.com\n  - resourcegroups.resources.azure.com\n  - virtualnetworks.network.azure.com\n  - virtualnetworkssubnets.network.azure.com\n  includedNamespaces:\n  - '*'\n</code></pre>"},{"location":"admin/backup/restore/#vsphere-capv","title":"vSphere (CAPV)","text":"<p>The following resources should be excluded from the <code>Restore</code> object:</p> <ul> <li><code>mutatingwebhookconfiguration.admissionregistration.k8s.io</code></li> <li><code>validatingwebhookconfiguration.admissionregistration.k8s.io</code></li> </ul> <p>Due to the Velero Restoration Order, some of the <code>CAPV</code> core objects cannot be restored, and they will not be recreated automatically. Because all of the objects have already passed both mutations and validations, there is not much sense in validating them again. The webhook configurations will be restored during installation of the <code>CAPV</code> provider.</p> <p>The resulting <code>Restore</code> object:</p> <pre><code>apiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  excludedResources:\n  - mutatingwebhookconfiguration.admissionregistration.k8s.io\n  - validatingwebhookconfiguration.admissionregistration.k8s.io\n  includedNamespaces:\n  - '*'\n</code></pre>"},{"location":"admin/backup/scheduled-backups/","title":"Scheduled Management Backups","text":"<p>Backups should be run on a schedule consistent with the policy requirements of the environment. For example a production environment might be set for \"daily\" backups, while a testing environment is set for \"weekly\".</p>"},{"location":"admin/backup/scheduled-backups/#create-a-management-backup","title":"Create a Management Backup","text":"<p>Periodic backups are handled by a <code>ManagementBackup</code> object, which uses a Cron expression for its <code>.spec.schedule</code> field. If the <code>.spec.schedule</code> field is not set, a backup on demand will be created instead.</p> <p>Optionally, set the name of the <code>.spec.backup.storageLocation</code> of the <code>BackupStorageLocation</code> object. The default location is the <code>BackupStorageLocation</code> object with <code>.spec.default</code> set to <code>true</code>.</p> <p>For example, you can create a <code>ManagementBackup</code> object that backs up to the storage object created in the preparation step every 6 hours (ref: Kubernetes CronJob schedule syntax, \"Vixie cron\" step values). Create a YAML file called <code>scheduled-backup.yaml</code>:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ManagementBackup\nmetadata:\n  name: kcm\nspec:\n  schedule: \"0 */6 * * *\"\n  storageLocation: aws-s3\nEOF\n</code></pre> Start the scheduled backup process by applying the YAML to the cluster: <pre><code>kubectl apply -f scheduled-backup.yaml\n</code></pre> Confirm the backup creation was successful by navigating to the appropriate storage console UI or from the command line: <pre><code>kubectl get managementbackup\n</code></pre> The <code>managementbackup</code> should show as <code>Completed</code>: <pre><code>NAME              LASTBACKUPSTATUS   NEXTBACKUP   AGE\nexample-backup    Completed                       8m  \n</code></pre></p>"},{"location":"admin/backup/upgrades-rollbacks/","title":"Upgrades and rollbacks","text":"<p>The Disaster Recovery Feature provides a way to create backups on each <code>kcm</code> upgrade automatically.</p>"},{"location":"admin/backup/upgrades-rollbacks/#automatic-management-backups","title":"Automatic Management Backups","text":"<p>Each <code>ManagementBackup</code> with a non-empty <code>.spec.schedule</code> field can enable the automatic creation of backups before upgrading to a new version.</p> <p>To enable, set the <code>.spec.performOnManagementUpgrade</code> to <code>true</code>.</p> <p>For example, a <code>ManagementBackup</code> object with enabled auto-backup before the <code>kcm</code> version upgrade looks like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ManagementBackup\nmetadata:\n  name: example-backup\nspec:\n  schedule: \"0 */6 * * *\"\n  performOnManagementUpgrade: true\n</code></pre> <p>After the enablement, before each upgrade of <code>kcm</code> to a new version, a new backup will be created.</p> <p>Automatically created backups have the following name template to make it easier to find them: the name of the <code>ManagementBackup</code> object with enabled <code>performOnManagementUpgrade</code> concatenates with the name of the release before the upgrade, for example, <code>example-backup-kcm-1-1-0</code>.</p> <p>Automatically created backups have the label <code>k0rdent.mirantis.com/release-backup</code> with the name of the release before the upgrade as its value to simplify querying if required.</p>"},{"location":"admin/backup/upgrades-rollbacks/#rollbacks","title":"Rollbacks","text":"<p>If during the <code>kcm</code> upgrade a failure happens, a rollback operation should be performed to restore the <code>kcm</code> to its before-the-upgrade state:</p> <ol> <li> <p>Follow the first 2 steps from the restoration section, creating a clean <code>kcm</code>    installation and <code>BackupStorageLocation</code>/<code>Secret</code>.</p> <p>Warning</p> <p> Please consider the restoration caveats section before proceeding.</p> </li> <li> <p>Create the <code>ConfigMap</code> object with patches to revert the <code>Management</code> <code>.spec.release</code>, substitute the <code>&lt;version-before-upgrade&gt;</code> with    the version of <code>kcm</code> before the upgrade, and create the <code>Restore</code> object,    propagating the <code>ConfigMap</code> to it:</p> <pre><code>---\napiVersion: v1\ndata:\n  patch-mgmt-spec-release: |\n    version: v1\n    resourceModifierRules:\n    - conditions:\n        groupResource: managements.k0rdent.mirantis.com\n      patches:\n      - operation: replace\n        path: \"/spec/release\"\n        value: \"&lt;version-before-upgrade&gt;\"\nkind: ConfigMap\nmetadata:\n  name: patch-mgmt-spec-release\n  namespace: kcm-system\n---\napiVersion: velero.io/v1\nkind: Restore\nmetadata:\n  name: &lt;restore-name&gt;\n  namespace: kcm-system\nspec:\n  backupName: &lt;backup-name&gt;\n  existingResourcePolicy: update\n  includedNamespaces:\n  - '*'\n  resourceModifier: # propagate patches\n    kind: ConfigMap\n    name: patch-mgmt-spec-release\n</code></pre> </li> <li> <p>Wait until the <code>Restore</code> status is <code>Completed</code> and all <code>kcm</code> components are up and running.</p> </li> <li>Optionally delete the created <code>ConfigMap</code>.</li> </ol>"},{"location":"admin/backup/whats-included/","title":"What's Included in the Management Backup","text":"<p>The backup includes all of Mirantis k0rdent Enterprise <code>kcm</code> component resources, parts of the <code>cert-manager</code> components required for other components creation, and all the required resources of <code>CAPI</code> and the <code>ClusterDeployment</code>s currently in use in the management cluster.</p> <p>By default, objects satisfying these labels will be included in the backup:</p> <pre><code>cluster.x-k8s.io/cluster-name=\"&lt;cluster-deployment-name&gt;\"\nhelm.toolkit.fluxcd.io/name=\"&lt;cluster-deployment-name&gt;\"\n\ncluster.x-k8s.io/provider=\"bootstrap-&lt;provider&gt;\"\ncluster.x-k8s.io/provider=\"control-plane-&lt;provider&gt;\"\ncluster.x-k8s.io/provider=\"infrastructure-&lt;provider&gt;\"\n\ncluster.x-k8s.io/provider=\"cluster-api\"\n\ncontroller.cert-manager.io/fao=\"true\"\n\nk0rdent.mirantis.com/component=\"kcm\"\n</code></pre> <p>An example sorted set of labels, objects satisfying these labels will be included in the backup:</p> <pre><code>cluster.x-k8s.io/cluster-name=\"some-cluster-deployment-name\"\ncluster.x-k8s.io/provider=\"bootstrap-k0sproject-k0smotron\"\ncluster.x-k8s.io/provider=\"cluster-api\"\ncluster.x-k8s.io/provider=\"control-plane-k0sproject-k0smotron\"\ncluster.x-k8s.io/provider=\"infrastructure-aws\"\ncontroller.cert-manager.io/fao=\"true\"\nhelm.toolkit.fluxcd.io/name=\"some-cluster-deployment-name\"\nk0rdent.mirantis.com/component=\"kcm\"\n</code></pre>"},{"location":"admin/clusters/","title":"Working With Clusters","text":"<p>Mirantis k0rdent Enterprise enables you to create and manage Kubernetes child clusters on a variety of  infrastructure choices.  You can also \"adopt\" Kubernetes clusters created outside of Mirantis k0rdent Enterprise so that Mirantis k0rdent Enterprise can manage them.  (You can also  \"self-adopt\" the management cluster so you can use Mirantis k0rdent Enterprise to manage itself.)</p> <ul> <li>Deploying standalone clusters</li> <li>Updating standalone clusters</li> <li>Adopting clusters</li> <li>IP Address Management (IPAM)</li> </ul>"},{"location":"admin/clusters/admin-adopting-clusters/","title":"Adopting an Existing Cluster","text":"<p>Creating a new cluster isn't the only way to use Mirantis k0rdent Enterprise. Adopting an existing Kubernetes cluster enables you to  bring it under Mirantis k0rdent Enterprise's management. This process is useful when you already have a running cluster but want  to centralize management and leverage Mirantis k0rdent Enterprise's capabilities, such as unified monitoring, configuration, and automation, but you don't want to redeploy your cluster.</p>"},{"location":"admin/clusters/admin-adopting-clusters/#adopting-a-cluster","title":"Adopting a Cluster","text":"<p>To adopt a cluster, Mirantis k0rdent Enterprise establishes communication between the management cluster (where kcm is installed)  and the target cluster. This requires proper credentials, network connectivity, and a standardized configuration. </p> <p>Follow these steps to adopt an existing cluster:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A kubeconfig file for the cluster you want to adopt (this file provides access credentials and configuration details    for the cluster).</li> <li>A management cluster with Mirantis k0rdent Enterprise installed and running. See the installation instructions    if you need to set it up.</li> <li>Network connectivity between the management cluster and the cluster to be adopted (for example, ensure firewall    rules and VPNs allow communication).</li> </ul> </li> <li> <p>Create a Credential</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure  provider. Follow the instructions in the Credential System, as well as the specific instructions  for your target infrastructure.</p> <p>Tip</p> <p>Double-check that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Configure the management cluster kubeconfig</p> <p>Set the <code>KUBECONFIG</code> environment variable to the path of your management cluster's kubeconfig file so you can  execute commands against the management cluster.</p> <p>For example:</p> <pre><code>export KUBECONFIG=/path/to/management-cluster-kubeconfig\n</code></pre> </li> <li> <p>Create the <code>ClusterDeployment</code> YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is used to define how Mirantis k0rdent Enterprise should manage the adopted cluster. Create a  YAML file for the <code>ClusterDeployment</code> object, as shown below:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;CLUSTER_NAME&gt;\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  template: adopted-cluster-&lt;VERSION&gt;\n  credential: &lt;CREDENTIAL_NAME&gt;\n  dryRun: &lt;BOOLEAN&gt;\n  config:\n    &lt;CONFIGURATION&gt;\n</code></pre> <p>Replace placeholders such as <code>&lt;CLUSTER_NAME&gt;</code>, <code>&lt;NAMESPACE&gt;</code>, <code>&lt;VERSION&gt;</code>, <code>&lt;CREDENTIAL_NAME&gt;</code>, and <code>&lt;CONFIGURATION&gt;</code> with actual values. The <code>dryRun</code> flag is useful for testing the configuration without making changes to the cluster. For more details, see the Dry Run section.</p> <p>You can also get a list of the available templates with:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-0-1           true\naws-eks-1-0-2                   true\naws-hosted-cp-1-0-11             true\naws-standalone-cp-1-0-12         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-13           true\nazure-standalone-cp-1-0-13       true\ndocker-hosted-cp-1-0-2          true\ngcp-gke-1-0-3                   true\ngcp-hosted-cp-1-0-12             true\ngcp-standalone-cp-1-0-12         true\nopenstack-standalone-cp-1-0-11   true\nremote-cluster-1-0-11            true\nvsphere-hosted-cp-1-0-10         true\nvsphere-standalone-cp-1-0-11     true\n</code></pre></p> <p>Putting it all together, your YAML would look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster\n  namespace: kcm-system\nspec:\n  template: adopted-cluster-1-1-0\n  credential: my-cluster-credential\n  dryRun: false\n  config: {}\n</code></pre> </li> <li> <p>Apply the <code>ClusterDeployment</code> configuration</p> <p>Once your configuration file is ready, apply it to the management cluster using <code>kubectl</code>:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits the <code>ClusterDeployment</code> object to Mirantis k0rdent Enterprise, initiating the adoption process.</p> </li> <li> <p>Check the Status of the <code>ClusterDeployment</code> Object</p> <p>To ensure the adoption process is progressing as expected, check the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output includes the current state and any conditions (for example, errors or progress updates). Review  this information to confirm that the adoption is successful.</p> </li> </ol>"},{"location":"admin/clusters/admin-adopting-clusters/#whats-happening-behind-the-scenes","title":"What's Happening Behind the Scenes?","text":"<p>When you adopt a cluster, Mirantis k0rdent Enterprise performs several actions:</p> <ol> <li>It validates the credentials and configuration provided in the <code>ClusterDeployment</code> object.</li> <li>It ensures network connectivity between the management cluster and the adopted cluster.</li> <li>It registers the adopted cluster within the Mirantis k0rdent Enterprise system, enabling it to be monitored and managed like      any k0rdent-deployed cluster.</li> </ol> <p>This process doesn't change the adopted cluster's existing workloads or configurations. Instead, it enhances your  ability to manage the cluster through Mirantis k0rdent Enterprise.</p>"},{"location":"admin/clusters/admin-adopting-clusters/#self-adopting-the-management-cluster","title":"Self-Adopting the Management Cluster","text":"<p>Mirantis k0rdent Enterprise makes it easy to manage Kubernetes clusters, but it only manages child clusters represented by a <code>ClusterDeployment</code>. So in order for Mirantis k0rdent Enterprise to manage itself, you must adopt the management cluster. Fortunately, because you're using the target cluster's <code>kubeconfig</code>, this is pretty straightforward.</p> <p>For example, adopting a k0s-based management cluster might look like this:</p> <ol> <li> <p>Get the IP address of the control plane:</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code>NAME             STATUS   ROLES           AGE   VERSION       INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME\nip-172-31-8-77   Ready    control-plane   9d    v1.33.2+k0s   172.31.8.77   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-1029-aws   containerd://1.7.27\n</code></pre></p> <p>Because the cluster will be accessing itself, the <code>INTERNAL-IP</code> (in this example, <code>172.21.8.77</code>), is sufficient.</p> </li> <li> <p>Edit the <code>kubeconfig</code>:</p> <p>Make sure that the <code>kubeconfig</code> file references the IP address, rather than <code>localhost</code>.  You can do this by editing the file directly:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CR...tLQo=\n    server: https://172.21.8.77:6443\n  name: local\ncontexts:\n...\n</code></pre> </li> <li> <p>Get the base64-encoded <code>kubeconfig</code>:</p> <p><pre><code>base64 /path/to/kubeconfig\n</code></pre> <pre><code>YXBpVmVyc2lvbjogdjEKY2x1c3RlcnM6Ci0gY2x1c3RlcjoKICAgIGNlcnRpZmljYXRlLWF1dGhv\ncml0eS1kYXRhOiBMUzB0TFMxQ1JVZEpUaUJEUlZKVVNVWkpRMEZVUlMwdExTMHRDazFKU1VSQlJF\n...\ndFZjVlphZVVWWlUyMDVlRFF4UVVoMlpYSnhWVGxvQ2kwdExTMHRSVTVFSUZKVFFTQlFVa2xXUVZS\nRklFdEZXUzB0TFMwdENnPT0K\n</code></pre></p> </li> <li> <p>Create the <code>Credential</code> to access the cluster:</p> <p>Create a file with the <code>Secret</code> and <code>Credential</code> objects, such as <code>adopt-creds.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: self-adopt-cluster-kubeconfig\n  namespace: kcm-system\ntype: Opaque\ndata:\n  value: &lt;BASE64_KUBECONFIG&gt;\n---\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: self-adopt-cluster-credential\n  namespace: kcm-system\nspec:\n  description: \"Credential For Self Adoption of Management Cluster\"\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: self-adopt-cluster-kubeconfig\n    namespace: kcm-system\n</code></pre> <p>Make sure to remove the line feeds from the encoded <code>kubeconfig</code>.</p> </li> <li> <p>Add the credential objects:</p> <p><pre><code>kubectl apply -f adopt-creds.yaml\n</code></pre> <pre><code>secret/self-adopt-cluster-kubeconfig created\ncredential.k0rdent.mirantis.com/self-adopt-cluster-credential created\n</code></pre></p> </li> <li> <p>Define the <code>ClusterDeployment</code>:</p> <p>First determine the <code>ClusterTemplate</code>:</p> <p><pre><code>kubectl get ClusterTemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-0-2-0           true\nadopted-cluster-1-0-0           true\nadopted-cluster-1-0-1           true\naws-eks-0-2-0                   true\n...\n</code></pre></p> <p>Create the definition file, such as <code>self-adopt-cluster.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: self-adopted-mgmt\n  namespace: kcm-system\nspec:\n  template: adopted-cluster-1-0-1\n  credential: self-adopt-cluster-credential\n  dryRun: False\n  config: {}\n</code></pre> </li> <li> <p>Add the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl apply -f self-adopt-cluster.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/self-adopted-mgmt created\n</code></pre></p> </li> <li> <p>Verify the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployment -A\n</code></pre> <pre><code>NAMESPACE    NAME                READY   SERVICES   TEMPLATE                MESSAGES          AGE\nkcm-system   self-adopted-mgmt   True    0/0        adopted-cluster-1-0-1   Object is ready   14s\n</code></pre></p> </li> </ol> <p>Now you can manage the Mirantis k0rdent Enterprise management cluster just as you'd manage  any other child cluster.</p>"},{"location":"admin/clusters/admin-adopting-clusters/#additional-tips","title":"Additional Tips","text":"<ul> <li>If you encounter issues, double-check that kubeconfig file you used for the adopted cluster is valid    and matches the cluster you're trying to adopt.</li> <li>Use the <code>dryRun</code> option during the first attempt to validate the configuration without making actual changes.</li> </ul>"},{"location":"admin/clusters/cluster-ipam/","title":"IP Address Management (IPAM)","text":"<p><code>Mirantis k0rdent Enterprise</code> provides a flexible IP Address Management (IPAM) system that enables deterministic allocation of IP addresses throughout the cluster lifecycle.</p> <p>Warning</p> <p>Keep in mind the following about IPAM support:</p> <ul> <li>At the moment only node network is supported.</li> <li>IPAM is currently unsupported on ARM64 architectures.</li> <li>IPAM has only been tested on VMware VSphere. Support for other providers will be added in the future.</li> </ul> <p>With IPAM enabled, IP addresses can be assigned to both worker and control plane nodes.</p> <p>Administrators can define address ranges using either CIDR blocks or explicit IP lists, enabling:</p> <ul> <li>Predictable, conflict-free assignments</li> <li>Seamless integration with existing network topologies</li> <li>Fine-grained control in multi-tenant or segmented environments</li> </ul>"},{"location":"admin/clusters/cluster-ipam/#deploying-a-cluster-with-ipam","title":"Deploying a Cluster with IPAM","text":"<p>Follow these instructions to configure IPAM for your cluster deployment.</p>"},{"location":"admin/clusters/cluster-ipam/#prerequisites","title":"Prerequisites","text":"<p>Ensure the following before configuring IPAM:</p> <ul> <li>A valid, unused IP space is available (CIDR or static IP list).</li> <li>The reserved space must accommodate:<ul> <li>One IP per control plane node</li> <li>One IP per worker node</li> </ul> </li> </ul>"},{"location":"admin/clusters/cluster-ipam/#define-ipam-configuration","title":"Define IPAM configuration","text":"<p>There are two options for configuring IPAM in Mirantis k0rdent Enterprise</p>"},{"location":"admin/clusters/cluster-ipam/#option-1-use-mutual-references-in-clusterdeployment-and-clusteripamclaim","title":"Option 1: Use mutual references in <code>ClusterDeployment</code> and <code>ClusterIPAMClaim</code>","text":"<p>To use mutual references, follow these steps:</p> <ol> <li> <p>Define a <code>ClusterIPAMClaim</code></p> <p>The <code>ClusterIPAMClaim</code> resource reserves the required IP address space for the cluster. The node network segment can be defined using either a <code>cidr</code> or a static list of <code>ipAddresses</code>.</p> <p>Note</p> <p> The value for <code>provider</code> must be <code>in-cluster</code> or <code>ipam-infoblox</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterIPAMClaim\nmetadata:\n  name: &lt;claim-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  provider: &lt;provider-name&gt;\n  nodeNetwork:\n    cidr: &lt;cidr&gt;\n    # ipAddresses:\n    # - &lt;ip-1&gt;\n    # - &lt;ip-2&gt;\n  cluster: &lt;cluster-name&gt;\n</code></pre> <ul> <li>The <code>cluster</code> field in <code>ClusterIPAMClaim</code> is immutable once set.</li> <li>The <code>cluster</code> field links the claim to a specific <code>ClusterDeployment</code>, ensuring IPs are reserved before provisioning begins.</li> </ul> </li> <li> <p>Apply the <code>ClusterIPAMClaim</code></p> <p>To create the claim:</p> <pre><code>kubectl apply -f &lt;cluster-ipam-claim-file&gt;.yaml\n</code></pre> <p>To verify the claim:</p> <p><pre><code>kubectl get clusteripamclaim &lt;claim-name&gt; -n &lt;namespace&gt;\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterIPAMClaim\nmetadata:\n  name: &lt;claim-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  cluster: &lt;cluster-name&gt;\n  clusterIPAMRef: &lt;claim-name&gt;\n  nodeNetwork:\n    cidr: &lt;cidr&gt;\n    # ipAddresses:\n    # - &lt;ip-1&gt;\n    # - &lt;ip-2&gt;\n  provider: &lt;provider-name&gt;\nstatus:\n  bound: true\n</code></pre></p> <ul> <li><code>.spec.clusterIPAMRef</code>: If this field is set, it indicates that the child <code>ClusterIPAM</code> object was successfully created.</li> <li><code>.status.bound</code>: If <code>true</code>, it means the child <code>ClusterIPAM</code> was successfully reconciled and the defined addresses were allocated.</li> </ul> </li> <li> <p>Define a <code>ClusterDeployment</code></p> <p>Finally, define the <code>ClusterDeployment</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;provider-credential-name&gt;\n  dryRun: &lt;\"true\" | \"false\"&gt;  # Optional; defaults to \"false\"\n  config:\n    &lt;cluster-configuration&gt;\n  ipamClaim:\n    ref: &lt;claim-name&gt;\n</code></pre> </li> </ol>"},{"location":"admin/clusters/cluster-ipam/#option-2-use-inline-ipam-configuration-in-clusterdeployment","title":"Option 2: Use inline IPAM configuration in <code>ClusterDeployment</code>","text":"<p>The IPAM configuration can also be defined inline within the <code>ClusterDeployment</code> resource as follows:</p> <ol> <li> <p>Define a <code>ClusterDeployment</code></p> <p>First, define the <code>ClusterDeployment</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;provider-credential-name&gt;\n  dryRun: &lt;\"true\" | \"false\"&gt;  # Optional; defaults to \"false\"\n  config:\n    &lt;cluster-configuration&gt;\n  ipamClaim:\n    spec:\n      provider: &lt;provider-name&gt;\n      nodeNetwork:\n        cidr: &lt;cidr&gt;\n        # ipAddresses:\n        # - &lt;ip-1&gt;\n        # - &lt;ip-2&gt;\n</code></pre> </li> <li> <p>Apply the <code>ClusterDeployment</code>:</p> <pre><code>kubectl apply -f &lt;cluster-deployment-file&gt;.yaml\n</code></pre> </li> <li> <p>Verify IPAM</p> <p>The specified IPAM settings will be used to allocate IP addresses during provisioning. Keep in mind that cluster provisioning will not proceed until IPAM resources are ready and addresses are allocated.</p> <p>To inspect the resulting <code>ClusterIPAM</code> resource:</p> <p><pre><code>kubectl get -n &lt;namespace&gt; ClusterIPAM &lt;claim-name&gt;\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterIPAM\nmetadata:\n  name: &lt;cluster-ipam-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  provider: &lt;provider-name&gt;\n  clusterIPAMClaimRefs: &lt;cluster-ipam-claim-name&gt;\nstatus:\n  phase: Bound\n  providerData:\n    &lt;provider data&gt;\n</code></pre></p> </li> </ol>"},{"location":"admin/clusters/deploy-cluster/","title":"Deploying a Cluster","text":"<p>Mirantis k0rdent Enterprise is designed to simplify the process of deploying and managing Kubernetes clusters across various cloud platforms. It does this through the use of <code>ClusterDeployment</code> objects, which include all of the information Mirantis k0rdent Enterprise needs to know in order to create the cluster you're looking for. This <code>ClusterDeployment</code> system relies on predefined templates and credentials. </p> <p>A cluster deployment typically involves:</p> <ol> <li>Setting up credentials for the infrastructure provider (for example, AWS, vSphere).</li> <li>Choosing a template that defines the desired cluster configuration (for example, number of nodes, instance types).</li> <li>Submitting the configuration for deployment and monitoring the process.</li> </ol> <p>Follow these steps to deploy a standalone Kubernetes cluster tailored to your specific needs:</p> <ol> <li> <p>Create the <code>Credential</code> object</p> <p>Credentials are essential for Mirantis k0rdent Enterprise to communicate with the infrastructure provider (for example, AWS, Azure, vSphere). These credentials enable Mirantis k0rdent Enterprise to provision resources such as virtual machines, networking components, and storage.</p> <p><code>Credential</code> objects are generally created ahead of time and made available to users, so before you look into creating a new one be sure what you're looking for doesn't already exist. You can see all of the existing <code>Credential</code> objects by  querying the management cluster:</p> <pre><code>kubectl get credentials --all-namespaces\n</code></pre> <p>If the <code>Credential</code> you need doesn't yet exist, go ahead and create it.</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure provider. Follow the instructions in the chapter about credential management, as well as the specific instructions for your target infrastructure.</p> <p>Tip</p> <p>Double-check to make sure that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Select a Template</p> <p>Templates in Mirantis k0rdent Enterprise are predefined configurations that describe how to set up the cluster. Templates include details such as:</p> <ul> <li>The number and type of control plane and worker nodes</li> <li>Networking settings</li> <li>Regional deployment preferences</li> </ul> <p>Templates act as a blueprint for creating a cluster. To see the list of available templates, use the following command:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-0-1           true\naws-eks-1-0-2                   true\naws-hosted-cp-1-0-11             true\naws-standalone-cp-1-0-12         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-13           true\nazure-standalone-cp-1-0-13       true\ndocker-hosted-cp-1-0-2          true\ngcp-gke-1-0-3                   true\ngcp-hosted-cp-1-0-12             true\ngcp-standalone-cp-1-0-12         true\nopenstack-standalone-cp-1-0-11   true\nremote-cluster-1-0-11            true\nvsphere-hosted-cp-1-0-10         true\nvsphere-standalone-cp-1-0-11     true\n</code></pre></p> <p>You can then get information on the actual template by describing it, as in:</p> <pre><code>kubectl describe clustertemplate aws-standalone-cp-1-0-12 -n kcm-system\n</code></pre> </li> <li> <p>Create a ClusterDeployment YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is the main configuration file that defines your cluster's specifications. It includes:</p> <ul> <li>The template to use</li> <li>The credentials for the infrastructure provider</li> <li>Optional customizations such as instance types, regions, and networking</li> </ul> <p>Create a <code>ClusterDeployment</code> configuration in a YAML file, following this structure:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>You will of course want to replace the placeholders with actual values. (For more information about <code>dryRun</code> see Understanding the Dry Run.) For example, this is a simple AWS infrastructure provider <code>ClusterDeployment</code>:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-12\n  credential: aws-credential\n  dryRun: false\n  config:\n    clusterLabels: {}\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n      rootVolumeSize: 32          \n    worker:\n      instanceType: t3.small\n      rootVolumeSize: 32          \n</code></pre> Note that the <code>.spec.credential</code> value should match the <code>.metadata.name</code> value of a created <code>Credential</code> object.</p> </li> <li> <p>Apply the Configuration</p> <p>Once the <code>ClusterDeployment</code> configuration is ready, apply it to the Mirantis k0rdent Enterprise management cluster:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits your deployment request to Mirantis k0rdent Enterprise. If you've set <code>dryRun</code> to <code>true</code> you can observe what would happen. Otherwise, Mirantis k0rdent Enterprise will go ahead and begin provisioning the necessary infrastructure.</p> </li> <li> <p>Verify Deployment Status</p> <p>After submitting the configuration, verify that the <code>ClusterDeployment</code> object has been created successfully:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output shows the current status and any errors.</p> </li> <li> <p>Monitor Provisioning</p> <p>Mirantis k0rdent Enterprise will now start provisioning resources (for example, VMs and networks) and setting up the cluster. To monitor this process, run:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>Tip</p> <p>For a detailed view of the provisioning process, use the <code>clusterctl describe</code> command (note that this requires the <code>clusterctl</code> CLI):</p> <pre><code>clusterctl describe cluster &lt;cluster-name&gt; -n &lt;namespace&gt; --show-conditions all\n</code></pre> </li> <li> <p>Retrieve the Kubernetes Configuration</p> <p>When provisioning is complete, retrieve the kubeconfig file for the new cluster. This file enables you to interact with the cluster using <code>kubectl</code>:</p> <p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> You can then use this file to access the cluster, as in:</p> <pre><code>export KUBECONFIG=kubeconfig\nkubectl get pods -A\n</code></pre> <p>Store the kubeconfig file securely, as it contains authentication details for accessing the cluster.</p> </li> </ol>"},{"location":"admin/clusters/update-cluster/","title":"Updating a Single Standalone Cluster","text":"<p>Mirantis k0rdent Enterprise <code>ClusterTemplate</code> objects are immutable, so the only way to change a <code>ClusterDeployment</code> is to change the template that forms its basis. </p> <p>To update the <code>ClusterDeployment</code>, modify the <code>.spec.template</code> field to use the name of the new <code>ClusterTemplate</code>.  This enables you to apply changes to the cluster configuration. These changes will then be applied to the actual  cluster. For example, if the cluster currently uses <code>t2.large</code> instances, that will be specified in its current template.  To change the cluster to use <code>t2.xlarge</code> instances, you would simply apply a template that references that new size;  Mirantis k0rdent Enterprise will then realize the cluster is out of sync and will attempt to remedy the situation by updating the cluster.</p> <p>Follow these steps to update the <code>ClusterDeployment</code>:</p> <ol> <li> <p>Patch the <code>ClusterDeployment</code> with the new template</p> <p>Run the following command, replacing the placeholders with the appropriate values:</p> <pre><code>kubectl patch clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; --patch '{\"spec\":{\"template\":\"&lt;new-template-name&gt;\"}}' --type=merge\n</code></pre> </li> <li> <p>Check the status of the <code>ClusterDeployment</code></p> <p>After applying the patch, verify the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt;\n</code></pre> </li> <li> <p>Inspect the detailed status</p> <p>For more details, use the <code>-o=yaml</code> option to check the <code>.status.conditions</code> field:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; -o=yaml\n</code></pre> </li> </ol> <p>Note that not all updates are possible; <code>ClusterTemplateChain</code> objects limit what templates can be applied.  Consider, for example, this <code>ClusterTemplateChain</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws-standalone-cp-1-0-12\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-2\n      availableUpgrades:\n        - name: aws-standalone-cp-1-0-12\n    - name: aws-standalone-cp-1-0-12\n</code></pre> <p>As you can see from the <code>.spec</code>, the <code>aws-standalone-cp-1-0-12</code> template can be applied to a cluster that also uses the <code>aws-standalone-cp-1-0-12</code> template, or it can be used as an upgrade from a cluster that uses <code>aws-standalone-cp-0.0.2</code>. You wouldn't be able to use this template to update a cluster that uses any other <code>ClusterTemplate</code>.</p> <p>Similarly, the <code>AccessManagement</code> object must have properly configured <code>spec.accessRules</code> with a list of allowed  <code>ClusterTemplateChain</code> object names and their namespaces. For more information, see Template Life Cycle Management.</p> <p>Note</p> <p>Support for displaying all available Cluster Templates for updates in the <code>ClusterDeployment</code> status is planned.</p>"},{"location":"admin/hosted-control-plane/","title":"Deploying a Hosted Control Plane","text":"<p>Under normal circumstances, Mirantis k0rdent Enterprise deploys a child cluster as a complete, self-contained unit. That is, the controllers and workers are all part of the <code>ClusterDeployment</code>. There are serious advantages to deploying a cluster this way, not the least of which is the fact that the cluster is essentially independent of the Management cluster, in that if the Management cluster becomes inaccessible for any reason the child cluster can continue on as though nothing has happened. This is known as a \"standalone\" deployment.</p> <p>On the other hand, for many Kubernetes clusters, the controllers are either too busy -- that is, they are constantly being scaled up and down -- or they're not busy enough -- that is, they're taking up server resources that aren't being fully used.</p> <p>For either of these cases, a better choice may be a \"hosted control plane\".</p> <p>A hosted control plane is a Kubernetes setup in which the control plane components (such as the API server,  etcd, and controllers) run as pods inside the management cluster instead of separate controller nodes. This architecture centralizes control plane management and improves scalability by sharing resources in the management cluster. Need more controllers? Spin up another pod. Need fewer controllers? Remove some pods.</p> <p>It's important to remember that if a child cluster's control plan goes down, workloads will continue on indefinitely, but you won't be able to manage the cluster. So be sure to back up your management cluster after deploying important child clusters!</p> <p>Hosted control planes are managed by k0smotron, which makes it possible for Kubernetes controller nodes and worker nodes to reside not only in different clusters, but even in different clouds.</p> <p>Instructions for setting up a hosted control plane vary slighting depending on the provider.</p> <ul> <li>AWS</li> <li>Azure</li> <li>OpenStack</li> <li>VMware</li> <li>GCP</li> </ul>"},{"location":"admin/hosted-control-plane/hcp-aws/","title":"AWS Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on AWS: </p> <ol> <li> <p>Prerequisites</p> <p>Before proceeding, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28 or later) deployed on AWS with Mirantis k0rdent Enterprise installed.</li> <li>A default storage class configured on the management cluster to support Persistent Volumes.</li> <li>The VPC ID where the worker nodes will be deployed.</li> <li>The Subnet ID and Availability Zone (AZ) for the worker nodes.</li> <li>The AMI ID for the worker nodes (Amazon Machine Image ID for the desired OS and Kubernetes version).</li> </ul> <p>Important</p> <p>All control plane components for your hosted cluster will reside in the management cluster, and the management cluster  must have sufficient resources to handle these additional workloads.</p> </li> <li> <p>Networking</p> <p>To deploy a hosted control plane, the necessary AWS networking resources must already exist or be created. If you're  using the same VPC and subnets as your management cluster, you can reuse these resources.</p> <p>If your management cluster was deployed using the Cluster API Provider AWS (CAPA), you can gather the required  networking details using the following commands:</p> <p>Retrieve the VPC ID: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre></p> <p>Retrieve Subnet ID: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre></p> <p>Retrieve Availability Zone: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre></p> <p>Retrieve Security Group: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre></p> <p>Retrieve AMI ID: <pre><code>kubectl get awsmachinetemplate &lt;cluster-name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre></p> <p>Tip</p> <p>If you want to use different VPCs or regions for your management and hosted clusters, you\u2019ll need to configure additional networking, such as VPC peering, to allow communication between them.</p> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>Once you've collected all the necessary data, you can create the <code>ClusterDeployment</code> manifest. This file tells Mirantis k0rdent Enterprise how to  deploy and manage the hosted control plane. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted-cp\nspec:\n  template: aws-hosted-cp-1-0-11\n  credential: aws-credential\n  config:\n    managementClusterName: aws\n    clusterLabels: {}\n    vpcID: vpc-0a000000000000000\n    region: us-west-1\n    publicIP: true\n    subnets:\n      - id: subnet-0aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: true\n        natGatewayID: xxxxxx\n        routeTableId: xxxxxx\n      - id: subnet-1aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: false\n        routeTableId: xxxxxx\n    instanceType: t3.medium\n    rootVolumeSize: 32\n    securityGroupIDs:\n      - sg-0e000000000000000\n</code></pre> <p>Note</p> <p>The example above uses the <code>us-west-1</code> region, but you should use the region of your VPC.</p> </li> <li> <p>Generate the <code>ClusterDeployment</code> Manifest</p> <p>To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dynamically  inserts the appropriate values: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp-1-0-11\n  credential: aws-credential\n  config:\n    managementClusterName: \"{{.metadata.name}}\"\n    clusterLabels: {}\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n    {{- range $subnet := .spec.network.subnets }}\n      - id: \"{{ $subnet.resourceID }}\"\n        availabilityZone: \"{{ $subnet.availabilityZone }}\"\n        isPublic: {{ $subnet.isPublic }}\n        {{- if $subnet.isPublic }}\n        natGatewayId: \"{{ $subnet.natGatewayId }}\"\n        {{- end }}\n        routeTableId: \"{{ $subnet.routeTableId }}\"\n    {{- end }}\n    instanceType: t3.medium\n    rootVolumeSize: 32        \n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre></p> <p>Save this template as <code>clusterdeployment.yaml.tpl</code>, then generate your manifest using the following command:</p> <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre> </li> <li> <p>Apply the <code>ClusterTemplate</code></p> <p>Nothing actually happens until you apply the <code>ClusterDeployment</code> manifest to create a new cluster deployment:</p> <pre><code>kubectl apply -f clusterdeployment.yaml -n kcm-system\n</code></pre> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-aws/#deployment-tips","title":"Deployment Tips","text":"<p>Here are some additional tips to help with deployment:</p> <ol> <li> <p>Controller and Template Availability:</p> <p>Make sure the KCM controller image and templates are available in a public or accessible repository.</p> </li> <li> <p>Install Charts and Templates:</p> <p>If you're using a custom repository, run the following commands with the appropriate <code>kubeconfig</code>:</p> <pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/k0rdent/kcm/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/k0rdent/kcm/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> </li> <li> <p>Mark the Infrastructure as Ready:</p> <p>To scale up the <code>MachineDeployment</code>, manually mark the infrastructure as ready: <pre><code>kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}' -n kcm-system\n</code></pre> For more details on why this is necessary, click here.</p> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-azure/","title":"Azure Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on Azure:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on Azure with Mirantis k0rdent Enterprise installed.</li> <li>A default storage class configured    on the management cluster to support Persistent Volumes.</li> </ul> <p>Note</p> <p>All control plane components for managed clusters will run in the management cluster. Make sure the management cluster    has sufficient CPU, memory, and storage to handle the additional workload.</p> </li> <li> <p>Gather Pre-existing Resources</p> <p>In a hosted control plane setup, some Azure resources must exist before deployment and must be explicitly  provided in the <code>ClusterDeployment</code> configuration. These resources can also be reused by the management cluster.</p> <p>If you deployed your Azure Kubernetes cluster using the Cluster API Provider for Azure (CAPZ), you can retrieve  the required information using the following commands:</p> <p>Location: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.location}}'\n</code></pre></p> <p>Subscription ID: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre></p> <p>Resource Group: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre></p> <p>VNet Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre></p> <p>Subnet Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre></p> <p>Route Table Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre></p> <p>Security Group Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre></p> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>After collecting the required data, create a <code>ClusterDeployment</code> manifest to configure the hosted control plane. It should look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-1-0-13\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"westus\"\n    subscriptionID: ceb131c7-a917-439f-8e19-cd59fe247e03\n    vmSize: Standard_A4_v2\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n</code></pre> </li> <li> <p>Generate the <code>ClusterDeployment</code> Manifest</p> <p>To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dynamically inserts  the appropriate values:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-1-0-13\n  credential: azure-credential\n  config:\n    clusterLabels: {}\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n</code></pre> Save this YAML as <code>clusterdeployment.yaml.tpl</code> and render the manifest with the following command: <pre><code>kubectl get azurecluster &lt;management-cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre></p> </li> <li> <p>Create the <code>ClusterDeployment</code></p> <p>To actually create the cluster, apply the <code>ClusterDeployment</code> manifest to the management cluster, as in:</p> <pre><code>kubectl apply clusterdeployment.yaml -n kcm-system\n</code></pre> </li> <li> <p>Manually update the <code>AzureCluster</code> object</p> <p>Due to a limitation in k0smotron, (see k0sproject/k0smotron#668),  after applying the <code>ClusterDeployment</code> manifest, you must manually update the status of the <code>AzureCluster</code> object.</p> <p>Use the following command to set the <code>AzureCluster</code> object status to <code>Ready</code>:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}'\n</code></pre> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-azure/#important-notes-on-cluster-deletion","title":"Important Notes on Cluster Deletion","text":"<p>Due to these same k0smotron limitations, you must take some manual steps in order to delete a cluster properly:</p> <ol> <li> <p>Add a Custom Finalizer to the AzureCluster Object:</p> <p>To prevent the <code>AzureCluster</code> object from being deleted too early, add a custom finalizer:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": [\"manual\"]}}'\n</code></pre> </li> <li> <p>Delete the ClusterDeployment:</p> <p>After adding the finalizer, delete the <code>ClusterDeployment</code> object as usual. Confirm that all <code>AzureMachines</code> objects have been deleted successfully.</p> </li> <li> <p>Remove Finalizers from Orphaned AzureMachines:</p> <p>If any <code>AzureMachines</code> are left orphaned, delete their finalizers manually after confirming no VMs remain in Azure. Use this command to remove the finalizer:</p> <pre><code>kubectl patch azuremachine &lt;machine-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": []}}'\n</code></pre> </li> <li> <p>Allowing Updates to Orphaned Objects:</p> <p>If Azure admission controls prevent updates to orphaned objects, you must disable the associated <code>MutatingWebhookConfiguration</code> by deleting it:</p> <pre><code>kubectl delete mutatingwebhookconfiguration &lt;webhook-name&gt;\n</code></pre> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-gcp/","title":"GCP Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on Google Cloud:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on GCP with Mirantis k0rdent Enterprise installed.</li> <li>A GCP Cloud Controller Manager installed to manage Services with the <code>Load Balancer</code> type.</li> <li>A default storage class configured    on the management cluster to support Persistent Volumes.</li> </ul> <p>Note</p> <p>All control plane components for managed clusters will run in the management cluster. Make sure the management cluster has sufficient CPU, memory, and storage to handle the additional workload.</p> </li> <li> <p>Create the <code>ClusterDeployment</code> manifest</p> <p>The <code>ClusterDeployment</code> manifest for a GCP-hosted control plane is similar to those for standalone control plane deployments. For a detailed list of parameters, refer to the Template Reference Guide, but here is an example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: gcp-hosted-cp\nspec:\n  template: gcp-hosted-cp-1-0-12\n  credential: gcp-credential\n  config:\n    project: \"PROJECT_NAME\"\n    region: \"us-east4\"\n    network:\n      name: default # Select your desired network name (select new network name to create or find it via `gcloud compute networks list --format=\"value(name)\"`)\n    worker:\n      instanceType: n1-standard-2 # Select your desired instance type (find it via `gcloud compute machine-types list | grep REGION`)\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213 # Select image (find it via `gcloud compute images list --uri`)\n</code></pre> Apply the <code>ClusterDeployment</code> manifest to the management cluster, as in:</p> <pre><code>kubectl apply -f clusterdeployment.yaml -n kcm-system\n</code></pre> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-openstack/","title":"OpenStack Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on OpenStack:</p> <ol> <li> <p>Prerequisites</p> <p>Before proceeding, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28 or later) deployed on OpenStack with Mirantis k0rdent Enterprise installed.</li> <li>An OpenStack Cloud Controller Manager installed to manage Services with the <code>Load Balancer</code> type.</li> <li>A default storage class configured on the management cluster to support Persistent Volumes.</li> <li>An OpenStack Credential object and resource-template ConfigMap must exist in the namespace where the cluster   will be deployed. Refer to steps 4\u20136 in the OpenStack management cluster preparation guide.</li> <li>The name of the existing network and subnet where the worker nodes will be deployed.</li> <li>The name of the existing router that will be used to deploy the worker nodes.</li> <li>(Optional) The name of the existing security group to attach to the worker nodes.</li> </ul> <p>Important</p> <p>All control plane components for your hosted cluster will reside in the management cluster, and the management cluster must have sufficient resources to handle these additional workloads.</p> </li> <li> <p>Networking</p> <p>To deploy a hosted control plane, the necessary OpenStack networking resources must already exist or be created. If you're using the same network, subnet and router as your management cluster, you can reuse these resources.</p> <p>If your management cluster was deployed using the Cluster API Provider OpenStack (CAPO), you can gather the required networking details using the following commands:</p> <p>Retrieve the network name:</p> <pre><code>kubectl -n &lt;cluster-namespace&gt; get openstackcluster &lt;cluster-name&gt; -o go-template='{{.status.network.name}}'\n</code></pre> <p>Retrieve the subnet name:</p> <pre><code>kubectl -n &lt;cluster-namespace&gt; get openstackcluster &lt;cluster-name&gt; -o go-template='{{(index .status.network.subnets 0).name}}'\n</code></pre> <p>Retrieve the router name:</p> <pre><code>kubectl -n &lt;cluster-namespace&gt; get openstackcluster &lt;cluster-name&gt; -o go-template='{{.status.router.name}}'\n</code></pre> </li> <li> <p>Create the <code>ClusterDeployment</code> manifest</p> <p>Once you've collected all the necessary data, you can create the <code>ClusterDeployment</code> manifest. This file tells Mirantis k0rdent Enterprise how to deploy and manage the hosted control plane. For example: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: openstack-hosted-cp\nspec:\n  template: openstack-hosted-cp-1-0-2\n  credential: openstack-credential\nconfig:\n  workersNumber: 2\n  flavor: m1.medium\n  image:\n    filter:\n      name: ubuntu-20.04\n  externalNetwork:\n    filter:\n      name: \"public\"\n  identityRef:\n    name: \"openstack-cloud-config\"\n    cloudName: \"openstack\"\n    region: ${OS_REGION_NAME}\n\n  network:\n    filter:\n      name: ${NETWORK_NAME}\n  router:\n    filter:\n      name: ${ROUTER_NAME}\n  subnets:\n  - filter:\n      name: ${SUBNET_NAME}\n  ports:\n  - network:\n      filter:\n        name: ${NETWORK_NAME}\n</code></pre> You can adjust <code>flavor</code>, <code>image name</code>, <code>region name</code>, <code>network</code>, <code>subnet</code> and <code>router</code> configuration to match your OpenStack environment. For more information about the configuration options, see the Template reference for openstack Alternatively, you can generate the <code>ClusterDeployment</code> manifest. To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dynamically inserts the appropriate values: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: openstack-hosted\nspec:\n  template: openstack-hosted-cp-1-0-2\n  credential: openstack-credential\n  config:\n    workersNumber: 2\n    flavor: m1.medium\n    image:\n      filter:\n        name: ubuntu-20.04\n    externalNetwork:\n      filter:\n        name: \"public\"\n    identityRef:\n      name: \"openstack-cloud-config\"\n      cloudName: \"openstack\"\n      region: \"{{.spec.identityRef.region}}\"\n\n    network:\n      filter:\n        name: \"{{.status.network.name}}\"\n    router:\n      filter:\n        name: \"{{.status.router.name}}\"\n    subnets:\n    - filter:\n        name: \"{{(index .status.network.subnets 0).name}}\"\n    ports:\n    - network:\n        filter:\n          name: \"{{.status.network.name}}\"\n</code></pre> For more information on these and other available parameters, see the Template reference for openstack. Save this template as <code>clusterdeployment.yaml.tpl</code>, then generate your manifest using the following command: <pre><code>kubectl -n &lt;cluster-namespace&gt; get openstackcluster &lt;cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre> 4. Apply the template Nothing actually happens until you apply the <code>ClusterDeployment</code> manifest to create a new cluster deployment: <pre><code>kubectl apply -f clusterdeployment.yaml -n kcm-system\n</code></pre></p> </li> </ol>"},{"location":"admin/hosted-control-plane/hcp-vmware/","title":"vSphere Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on vSphere. </p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on vSphere with Mirantis k0rdent Enterprise installed.</li> </ul> <p>All control plane components for managed clusters will reside in the management cluster, so make sure the management  cluster has sufficient resources (CPU, memory, and storage) to handle these workloads.</p> </li> <li> <p>Create the <code>ClusterDeployment</code> Manifest</p> <p>The <code>ClusterDeployment</code> manifest for vSphere-hosted control planes is similar to standalone control plane deployments.  For a detailed list of parameters, refer to our discussion of Template parameters for vSphere.</p> <p>Important</p> <p>The vSphere provider requires you to specify the control plane endpoint IP before deploying the cluster. This IP  address must match the one assigned to the k0smotron load balancer (LB) service. Use an annotation supported by your load balancer provider to assign the control plane endpoint IP to the k0smotron  service. For example, the manifest below includes a <code>kube-vip</code> annotation.</p> <p><code>ClusterDeployment</code> objects for vSphere-based clusters include a <code>.spec.config.vsphere</code> object that contains vSphere-specific parameters. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp-1-0-10\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"172.16.0.10\"\n</code></pre> </li> </ol> <p>For more information on these parameters, see the Template reference for vsphere. </p>"},{"location":"admin/installation/","title":"Installing Mirantis k0rdent Enterprise","text":"<p>This installation chapter guides you through the essential steps to set up Mirantis k0rdent Enterprise, beginning with the creation of a dedicated management cluster that serves as the control plane. You'll then install the actual Mirantis k0rdent Enterprise software, followed by verifying that it's installed successfully. From there, you'll prepare the management cluster to provision and manage child clusters, laying the groundwork for deploying and scaling your Kubernetes workloads across different infrastructures.</p> <ul> <li>Creating the management cluster</li> <li>Installing Mirantis k0rdent Enterprise</li> <li>Verifying the Mirantis k0rdent Enterprise installation</li> <li>Preparing Mirantis k0rdent Enterprise to create child clusters</li> </ul>"},{"location":"admin/installation/install-k0rdent/","title":"Installation","text":"<p>This section assumes that you already have a kubernetes cluster installed. If you need to setup a cluster you can follow the Create and prepare a Kubernetes cluster with k0s to create a test cluster, or Create and prepare a production grade Kubernetes cluster with EKS to create something more substantial. </p> <p>The actual management cluster is a Kubernetes cluster with the Mirantis k0rdent Enterprise application installed. The simplest way to install Mirantis k0rdent Enterprise is through its Helm chart.  You can find the latest release here, and from there you can deploy the Helm chart, as in:</p> <p>Note</p> <p> Don't forget to verify the installation files.</p>"},{"location":"admin/installation/install-k0rdent/#configure-the-ui","title":"Configure the UI","text":"<p>By default, Mirantis k0rdent Enterprise installs and enables the UI with a pre-determined password. Please change the password (or disable the UI) when installing Mirantis k0rdent Enterprise. If you do not, anyone with access to the management cluster's network can log into the UI and deploy or affect resources in the Mirantis k0rdent Enterprise environment.</p> <p>To change the password, add the following to the installation command below:</p> <pre><code>--set k0rdent-ui.auth.basic.password='&lt;NEW_PASSWORD&gt;'\n</code></pre> <p>To disable the UI, use:</p> <pre><code>--set kordent-ui.enabled=false\n</code></pre> <p>For more information on using and configuring the Mirantis k0rdent Enterprise UI, including additional authentication options, see the UI documentation.</p>"},{"location":"admin/installation/install-k0rdent/#install-mirantis-k0rdent-enterprise","title":"Install Mirantis k0rdent Enterprise","text":"<p>You install Mirantis k0rdent Enterprise via  Helm chart:</p> <p><pre><code>helm install kcm oci://registry.mirantis.com/k0rdent-enterprise/charts/k0rdent-enterprise --version 1.1.0 -n kcm-system --create-namespace &lt;UI_CONFIG&gt;\n</code></pre> </p> <p>Important</p> <p> Don't forget to set the UI configuration to modify the default username and password or disable the UI before deploying, as noted above. If you do not, the UI will be enabled with a default username and password, potentially leaving the management cluster open to attack by anyone able to access the network it's on.</p> <pre><code>Pulled: registry.mirantis.com/k0rdent-enterprise/charts/k0rdent-enterprise:1.1.0\nDigest: sha256:b51693108d167fe0de851ae04ec1caf7865745e8aa27049bc1fab47693ab9f7c\nNAME: kcm\nLAST DEPLOYED: Wed Aug 13 2025 08:42:36 2025\nNAMESPACE: kcm-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nk0rdent enterprise chart has been installed.\nPlease make sure to change the auth method or set a new password for basic auth by\ncreating a secret and referencing it in `k0rdent-ui.auth.basic.secretKeyRef`\n</code></pre> <p>The helm chart deploys the KCM operator and prepares the environment, and KCM then proceeds to deploy the various subcomponents, including CAPI. The entire process takes a few minutes.</p>"},{"location":"admin/installation/install-k0rdent/#cleanup-uninstall-mirantis-k0rdent-enterprise","title":"Cleanup: Uninstall Mirantis k0rdent Enterprise","text":"<p>And of course when you need to clean up, you can use helm as well. Follow these steps:</p> <ol> <li> <p>Remove any <code>ClusterDeployment</code> objects in the cluster.</p> </li> <li> <p>Delete the <code>Management</code> object:</p> <pre><code>kubectl delete management.k0rdent kcm\n</code></pre> </li> <li> <p>Remove the kcm Helm release:</p> <pre><code>helm uninstall kcm -n kcm-system\n</code></pre> </li> <li> <p>Finally, remove the kcm-system namespace:</p> <pre><code>kubectl delete ns kcm-system\n</code></pre> </li> </ol>"},{"location":"admin/installation/sbom/","title":"Verifying Mirantis k0rdent Enterprise Artifacts and Security","text":"<p>Mirantis provides security artifacts for Mirantis k0rdent Enterprise releases to ensure software supply chain transparency and enable users to verify the integrity and composition of the software. These artifacts include cryptographically signed binaries and container images, Software Bills of Materials (SBOMs), and CVE scan reports.</p> <p>Verifying these artifacts is a critical step to ensure you are running genuine, untampered software and to assess its security posture before deployment.</p>"},{"location":"admin/installation/sbom/#artifact-signature-verification-with-cosign","title":"Artifact Signature Verification with Cosign","text":"<p>All Mirantis k0rdent Enterprise release artifacts (container images, binary files, reports) are cryptographically signed. Verification requires the <code>cosign</code> command-line tool.</p>"},{"location":"admin/installation/sbom/#verifying-oci-container-images","title":"Verifying OCI Container Images","text":"<p>Use the <code>cosign verify</code> command, specifying the public key (<code>https://get.mirantis.com/k0rdent-enterprise/cosign.pub</code>) and the full image path, as in:</p> <pre><code>cosign verify --key https://get.mirantis.com/k0rdent-enterprise/cosign.pub registry.mirantis.com/k0rdent-enterprise/&lt;image-name&gt;:&lt;tag&gt;\n</code></pre> <p>For example, you can verify the <code>kcm-controller:1.1.0</code> component with:</p> <pre><code>cosign verify --key https://get.mirantis.com/k0rdent-enterprise/cosign.pub registry.mirantis.com/k0rdent-enterprise/kcm-controller:1.1.0\n</code></pre>"},{"location":"admin/installation/sbom/#verifying-binary-artifacts-reports-binaries","title":"Verifying Binary Artifacts (Reports, Binaries)","text":"<p>Binary artifacts (such as executables) have a corresponding <code>.sig</code> file containing the signature, located alongside the artifact. To verify these artifacts:</p> <ol> <li>Download both the artifact file and its <code>.sig</code> file.</li> <li> <p>Use the <code>cosign verify-blob</code> command:</p> <pre><code>cosign verify-blob --key https://get.mirantis.com/k0rdent-enterprise/cosign.pub --signature &lt;artifact-name&gt;.sig &lt;artifact-name&gt;\n</code></pre> <p>For example, verify the version 1.1.0  <code>release.yaml</code>file:</p> <p><pre><code>wget https://get.mirantis.com/k0rdent-enterprise/1.1.0/release.yaml\nwget https://get.mirantis.com/k0rdent-enterprise/1.1.0/release.yaml.sig\ncosign verify-blob --key https://get.mirantis.com/k0rdent-enterprise/cosign.pub --signature release.yaml.sig release.yaml\n</code></pre> <pre><code>Verified OK\n</code></pre></p> </li> </ol> <p>Successful verification confirms the artifact's authenticity and integrity.</p>"},{"location":"admin/installation/sbom/#software-bill-of-materials-sboms","title":"Software Bill of Materials (SBOMs)","text":"<p>Mirantis provides SBOMs in the SPDX format for Mirantis k0rdent Enterprise components. SBOMs offer a detailed inventory of software ingredients, making it possible to manage vulnerabilities, perform license compliance checks, and understand software dependencies.</p>"},{"location":"admin/installation/sbom/#getting-sboms","title":"Getting SBOMs","text":"<p>Each OCI artifact contains an SBOM attached to it in the SPDX format. To get the SBOM you can use <code>cosign</code>.</p> <p>Note</p> <p> Since <code>cosign</code> returns predicates in json format <code>jq</code> must be used to query specific fields, like contents of the SPDX.</p> <p>For example to get the SPDX for <code>kcm-controller:1.1.0</code> you can use the following command:</p> <pre><code>cosign verify-attestation --key https://get.mirantis.com/k0rdent-enterprise/cosign.pub --type spdx registry.mirantis.com/k0rdent-enterprise/kcm-controller:1.1.0 | jq '.payload | @base64d | fromjson | .predicate' -r\n</code></pre> <p>This will get you a full SPDX file for <code>kcm-controller</code> and also will verify authenticity (attestation) of the attached SPDX.</p>"},{"location":"admin/installation/sbom/#cve-reports","title":"CVE Reports","text":"<p>CVE scans are also attached to the OCI artifacts as a form of attestation. CVE reports are generated using the <code>trivy</code> scanner and are provided in the Cosign Vulnerability Scan Record format.</p> <p>You can use <code>cosign</code> to get the CVE report for a specific artifact.</p> <p>For example, to get CVE vulnerability scan for <code>kcm-controller:1.1.0</code> you can use:</p> <pre><code>cosign verify-attestation --key https://get.mirantis.com/k0rdent-enterprise/cosign.pub --type vuln registry.mirantis.com/k0rdent-enterprise/kcm-controller:1.1.0 | jq '.payload | @base64d | fromjson | .' -r\n</code></pre> <p>This command returns JSON with the cosign vulnerability scan record attestation. It also verifies the authenticity of the data.</p>"},{"location":"admin/installation/verify-install/","title":"Confirming the deployment","text":"<p>Note</p> <p> After running the helm install command, please wait 5 to 10 minutes for the deployment to stabilize. To understand whether installation is complete, start by making sure all pods are ready in the <code>kcm-system</code> namespace. There should be 20 pod entries:</p> <pre><code>kubectl get pods -n kcm-system\n</code></pre> <pre><code>NAME                                                           READY   STATUS    RESTARTS   AGE\nazureserviceoperator-controller-manager-6b4dd86894-4dpfc       1/1     Running   0          7m15s\ncapa-controller-manager-64bbcb9f8-ltj6z                        1/1     Running   0          6m58s\ncapd-controller-manager-7586b6577c-6w4wq                       1/1     Running   0          7m11s\ncapg-controller-manager-774958b9b9-hvxvv                       1/1     Running   0          6m54s\ncapi-controller-manager-5b67d4fc7-dhk7p                        1/1     Running   0          8m49s\ncapo-controller-manager-6f98bb68cd-vsc24                       1/1     Running   0          6m33s\ncapv-controller-manager-69f7fc65d8-jx8s4                       1/1     Running   0          6m40s\ncapz-controller-manager-5b87fdf745-mzm7x                       1/1     Running   0          7m15s\nhelm-controller-746d7db585-x64ld                               1/1     Running   0          11m\nk0smotron-controller-manager-bootstrap-67dd88d848-wfrhm        2/2     Running   0          8m32s\nk0smotron-controller-manager-control-plane-657f5578d4-7vb8t    2/2     Running   0          8m24s\nk0smotron-controller-manager-infrastructure-5867d575f9-t28hp   2/2     Running   0          6m46s\nkcm-cert-manager-6979c67bc4-b6s4w                              1/1     Running   0          11m\nkcm-cert-manager-cainjector-5b97c84fdb-kdsw5                   1/1     Running   0          11m\nkcm-cert-manager-webhook-755796f599-q6727                      1/1     Running   0          11m\nkcm-cluster-api-operator-65c8f75569-rfsp2                      1/1     Running   0          9m30s\nkcm-controller-manager-68b56bff85-6fmsp                        1/1     Running   0          9m30s\nkcm-velero-67bf545995-x6784                                    1/1     Running   0          11m\nsource-controller-74b597b995-kkqqw                             1/1     Running   0          11m\n</code></pre> <pre><code>kubectl get pods -n kcm-system --no-headers | wc -l\n</code></pre> <pre><code>20\n</code></pre> <p>State management is handled by Project Sveltos, so you'll want to make sure that all 10 pods are running/completed in the <code>projectsveltos</code> namespace:</p> <pre><code>kubectl get pods -n projectsveltos\n</code></pre> <pre><code>NAME                                      READY   STATUS    RESTARTS   AGE\naccess-manager-6696df779-pnxjx            1/1     Running   0          10m\naddon-controller-6cb6c5f6df-zmfch         1/1     Running   0          10m\nclassifier-manager-5b47b66fc9-5mtwl       1/1     Running   0          10m\nevent-manager-564d6644b4-wr9cq            1/1     Running   0          10m\nhc-manager-7c56c59d9c-w5gds               1/1     Running   0          10m\nsc-manager-6798cd9d4d-r7z9j               1/1     Running   0          10m\nshard-controller-797965bb58-65lmp         1/1     Running   0          10m\nsveltos-agent-manager-5445f6f57c-wxw2s    1/1     Running   0          10m\ntechsupport-controller-5b666d6884-jfqnp   1/1     Running   0          10m\n</code></pre> <pre><code>kubectl get pods -n projectsveltos --no-headers | wc -l\n</code></pre> <pre><code>10\n</code></pre> <p>If any of these pods are missing, simply give Mirantis k0rdent Enterprise more time. If there's a problem, you'll see pods crashing and restarting, and you can see what's happening by describing the pod, as in:</p> <pre><code>kubectl describe pod classifieclassifier-manager-5b47b66fc9-5mtwl -n projectsveltos\n</code></pre> <p>As long as you're not seeing pod restarts, you just need to wait a few minutes.</p>"},{"location":"admin/installation/verify-install/#verify-that-mirantis-k0rdent-enterprise-itself-is-ready","title":"Verify that Mirantis k0rdent Enterprise itself is ready","text":"<p>The actual measure of whether Mirantis k0rdent Enterprise is ready is the state of the <code>Management</code> object. To check, issue this command:</p> <pre><code>kubectl get Management -n kcm-system\n</code></pre> <pre><code>NAME   READY   RELEASE     AGE\nkcm    True    kcm-1-1-0   9m\n</code></pre>"},{"location":"admin/installation/verify-install/#verify-the-templates","title":"Verify the templates","text":"<p>Next verify whether the KCM templates have been successfully installed and reconciled. Start with the <code>ProviderTemplate</code> objects:</p> <pre><code>kubectl get providertemplate -n kcm-system\n</code></pre> <pre><code>NAME                                   VALID\ncluster-api-1-0-4                                 true\ncluster-api-provider-aws-1-0-3                    true\ncluster-api-provider-azure-1-0-3                  true\ncluster-api-provider-docker-1-0-2                 true\ncluster-api-provider-gcp-1-0-3                    true\ncluster-api-provider-k0sproject-k0smotron-1-0-6   true\ncluster-api-provider-openstack-1-0-3              true\ncluster-api-provider-vsphere-1-0-2                true\nk0smotron-1-0-6                                   true\nkcm-1-1-0                                         true\nprojectsveltos-0-57-2                             true\n</code></pre> <p>Make sure that all templates are not just installed, but valid. Again, this may take a few minutes.</p> <p>You'll also want to make sure the <code>ClusterTemplate</code> objects are installed and valid:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-1-0           true\naws-eks-1-0-2                   true\naws-hosted-cp-1-0-11             true\naws-standalone-cp-1-0-12         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-13           true\nazure-standalone-cp-1-0-13       true\ndocker-hosted-cp-1-0-13           true\ngcp-gke-1-0-2                   true\ngcp-hosted-cp-1-0-11             true\ngcp-standalone-cp-1-0-12         true\nopenstack-standalone-cp-1-0-11   true\nremote-cluster-1-0-11   true\nvsphere-hosted-cp-1-0-10         true\nvsphere-standalone-cp-1-0-11     true\n</code></pre> <p>If this is a new <code>v0.2.0</code> cluster, you will see:</p> <pre><code>No resources found in kcm-system namespace.\n</code></pre> <p>You can then install the <code>ServiceTemplates</code> objects as follows:</p> <pre><code>helm upgrade --install catalog-core oci://ghcr.io/k0rdent/catalog/charts/catalog-core --version 1.0.0 -n kcm-system\n</code></pre> <p>Finally, make sure the <code>ServiceTemplate</code> objects are installed and valid:</p> <pre><code>kubectl get servicetemplate -n kcm-system\n</code></pre> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre>"},{"location":"admin/installation/airgap/","title":"Install Mirantis k0rdent Enterprise in the airgapped environment","text":"<p>Installing Mirantis k0rdent Enterprise normally requires a connection to the Internet, but it is possible to deploy it in an airgapped environment.</p> <p>The general process involves several steps:</p> <ol> <li>Fulfilling the prerequisites.</li> <li>Getting the airgap bundle</li> <li>Mirantis k0rdent Enterprise installation</li> </ol>"},{"location":"admin/installation/airgap/airgap-bundles/","title":"Download airgap bundles","text":"<p>In an airgapped environment, you must make the artifacts for Mirantis k0rdent Enterprise and Kubernetes available to install child clusters.</p>"},{"location":"admin/installation/airgap/airgap-bundles/#prerequisites","title":"Prerequisites","text":"<p>Set your registry hostname:</p> <p>Warning</p> <p> Replace <code>registry.local</code> with your actual registry hostname.</p> <pre><code>export REGISTRY_HOST=\"registry.local\"\nexport REGISTRY=\"${REGISTRY_HOST}/k0rdent-enterprise\"\n</code></pre>"},{"location":"admin/installation/airgap/airgap-bundles/#download-the-mirantis-k0rdent-enterprise-bundle","title":"Download the Mirantis k0rdent Enterprise bundle","text":"<p>Add the Mirantis k0rdent Enterprise bundle to the registry so that Helm can install it.</p>"},{"location":"admin/installation/airgap/airgap-bundles/#download-bundle-and-signature","title":"Download bundle and signature","text":"<pre><code>wget https://get.mirantis.com/k0rdent-enterprise/1.1.0/airgap-bundle-1.1.0.tar.gz\nwget https://get.mirantis.com/k0rdent-enterprise/1.1.0/airgap-bundle-1.1.0.tar.gz.sig\n</code></pre>"},{"location":"admin/installation/airgap/airgap-bundles/#verify-the-bundle","title":"Verify the bundle","text":"<p>Verify the downloaded file using <code>cosign</code>:</p> <pre><code>cosign verify-blob --key https://get.mirantis.com/k0rdent-enterprise/cosign.pub --signature airgap-bundle-1.1.0.tar.gz.sig airgap-bundle-1.1.0.tar.gz\n</code></pre> <p>Note</p> <p> For offline verification, download the public key and transfer it along with the bundle and signature to the airgapped environment. Additionally the <code>--private-infrastructure=true</code> flag must be set to skip online transparency log verification.</p>"},{"location":"admin/installation/airgap/airgap-bundles/#add-bundle-to-the-registry","title":"Add Bundle to the Registry","text":"<p>Extract the bundle to a temporary directory:</p> <pre><code>mkdir airgap-bundle\ntar -xf airgap-bundle-1.1.0.tar.gz -C airgap-bundle\n</code></pre> <p>Upload to the registry using <code>skopeo</code>:</p> <ol> <li> <p>Load <code>skopeo</code> image from the airgap bundle:</p> <pre><code>docker load -i airgap-bundle/skopeo_v1.17.0.tar\n</code></pre> </li> <li> <p>Log in to the registry:</p> <pre><code>docker login ${REGISTRY_HOST}\n</code></pre> </li> <li> <p>Run the following script:</p> <pre><code>cd airgap-bundle\nfor file in $(find . -type f ! -name 'skopeo*' | sed -s s~^./~~g); do\n  echo $file; bn=${file%*.tar};\n  docker run -v ${HOME}/.docker/config.json:/config.json skopeo:v1.17.0 copy -a --authfile /config.json oci-archive:${file} docker://${REGISTRY}/${bn%_*}:${bn#*_};\ndone\n</code></pre> </li> </ol>"},{"location":"admin/installation/airgap/airgap-bundles/#download-k0s-binaries","title":"Download k0s binaries","text":"<p><code>k0s</code> binaries must be available from the HTTP server inside the airgapped environment so they can be used when installing child clusters.</p> <p>Download the binaries and their signatures:</p> <pre><code>wget https://get.mirantis.com/k0rdent-enterprise/1.1.0/k0s-v1.32.6+k0s.0-amd64\nwget https://get.mirantis.com/k0rdent-enterprise/1.1.0/k0s-v1.32.6+k0s.0-amd64.sig\nwget https://get.mirantis.com/k0rdent-enterprise/1.1.0/k0s-v1.32.1+k0s.0-amd64\nwget https://get.mirantis.com/k0rdent-enterprise/1.1.0/k0s-v1.32.1+k0s.0-amd64.sig\n</code></pre>"},{"location":"admin/installation/airgap/airgap-bundles/#verify-the-downloaded-binaries","title":"Verify the downloaded binaries","text":"<p>Verify the signature using <code>cosign</code>:</p> <pre><code>cosign verify-blob --key https://get.mirantis.com/k0rdent-enterprise/cosign.pub --signature k0s-v1.32.6+k0s.0-amd64.sig k0s-v1.32.6+k0s.0-amd64\n</code></pre>"},{"location":"admin/installation/airgap/airgap-bundles/#upload-k0s-binaries-to-http-server","title":"Upload k0s binaries to HTTP server","text":"<p>Upload the <code>k0s</code> binary file to any HTTP server available from within the airgapped environment, such as one running in the management cluster.</p> <p>Warning</p> <p> Do not change the name of the <code>k0s</code> binary, or the deployment will fail.</p>"},{"location":"admin/installation/airgap/airgap-http/","title":"Installing an HTTP server","text":"<p>Optionally, if you don't have an HTTP server available to your airgapped system, you can install one to serve the <code>k0s</code> binary right in the Mirantis k0rdent Enterprise management cluster.</p> <p>The following instructions were tested on an \"airgapped\" AWS instance.</p> <p>Warning</p> <p> Images used in the following example (such as <code>nginx:1.27.5</code>) aren't part of the airgap bundle and must be downloaded separately.</p> <ol> <li> <p>Create the <code>Deployment</code>:</p> <p>Start by creating the YAML for the webserver, say, in <code>k0s-bundle-ag.yaml</code>:</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k0s-ag-image\n  labels:\n    app: k0s-ag-image\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k0s-ag-image\n  template:\n    metadata:\n      labels:\n        app: k0s-ag-image\n    spec:\n      containers:\n      - name: k0s-ag-image\n        image: nginx:1.27.5\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nginx/conf.d\n        - name: local-volume\n          mountPath: /var/www/html\n      volumes:\n      - name: config-volume\n        configMap:\n          name: k0s-ag-config\n      - name: local-volume\n        hostPath:\n          path: /home/ubuntu/k0s\n          type: Directory\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: k0s-ag-image\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: k0s-ag-image\n  type: NodePort\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: k0s-ag-config\ndata:\n  default.conf: |\n    server {\n      proxy_max_temp_file_size 0;\n      listen       80;\n      sendfile        on;\n      tcp_nopush     on;\n      tcp_nodelay        on;\n      server_name localhost;\n\n      keepalive_timeout  70;\n\n      root   /var/www/html;\n      location / {\n      }\n\n      client_max_body_size 512m;\n\n      location /heathz {\n        return 200 'OK';\n      }\n    }\n</code></pre> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f k0s-bundle-ag.yaml\n</code></pre> </li> <li> <p>Check for the <code>k0s-ag-image*</code> pod:</p> <pre><code>kubectl get pods -A\nNAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\ndefault       k0s-ag-image-67d7d95964-drd6h    1/1     Running   0          21h\nkube-system   coredns-58cb98cf9c-8zdph         1/1     Running   0          18h\nkube-system   kube-proxy-vpvk5                 1/1     Running   0          21h\nkube-system   kube-router-zdkvp                1/1     Running   0          21h\nkube-system   metrics-server-7db8586f5-blh6w   1/1     Running   0          21h\n</code></pre> </li> <li> <p>The <code>k0s-ag-image*</code> pod runs an Nginx HTTP server that serves files from a <code>hostPath</code> volume:</p> <pre><code>hostPath:\n  path: /home/ubuntu/k0s\n</code></pre> </li> <li> <p>Place the k0s binary in that directory, as in:</p> <pre><code>/home/ubuntu/k0s/k0rdent-enterprise/k0s-v1.32.5+k0s.1-amd64\n</code></pre> </li> <li> <p>Get the HTTP port:</p> <p><pre><code>sudo k0s kubectl get service -A\n</code></pre> <pre><code>NAMESPACE     NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE\ndefault       k0s-ag-image     NodePort    10.105.121.68    &lt;none&gt;        80:32538/TCP             50m\ndefault       kubernetes       ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                  50m\nkube-system   kube-dns         ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   50m\nkube-system   metrics-server   ClusterIP   10.109.193.120   &lt;none&gt;        443/TCP                  50m\n</code></pre></p> <p>In this case, files made availabe by the HTTP server are availabe on port <code>32538</code>.</p> </li> <li> <p>Test the HTTP server:</p> <p><pre><code>curl -I http://&lt;HOST_IP&gt;:&lt;HOST_PORT&gt;/k0rdent-enterprise/k0s-v1.32.5+k0s.1-amd64\n</code></pre> <pre><code>HTTP/1.1 200 OK\nServer: nginx/1.27.5\nDate: Tue, 17 Jun 2025 21:32:28 GMT\nContent-Type: application/octet-stream\nContent-Length: 268006461\nLast-Modified: Tue, 17 Jun 2025 21:26:59 GMT\nConnection: keep-alive\nETag: \"6851dda3-ff9743d\"\nAccept-Ranges: bytes\n</code></pre></p> </li> </ol> <p>Once you have uploaded all the necessary images, you can start the Mirantis k0rdent Enterprise installation process.</p>"},{"location":"admin/installation/airgap/airgap-install/","title":"Installation","text":"<p>Before installing Mirantis k0rdent Enterprise prepare a special <code>values.yaml</code> file to identify the registry and k0s binaries URL for the main components.</p> <p>The following is an example of the <code>values.yaml</code> file. In this example, the <code>k0s</code> binaries are placed on the HTTP host <code>binary.local</code>. Thus it's expected that the <code>k0s</code> binary will be available on the URL <code>http://binary.local/k0rdent-enterprise/k0s-v1.32.6+k0s.0-amd64</code> In addition, the registry is assumed to be at <code>registry.local</code>. Adjust your <code>values.yaml</code> file accordingly.</p> <p>Warning</p> <p> If you're using a registry with a self-signed certificate you must first create a secret with <code>ca.crt</code> with the value of the CA certificate used to sign the registry's certificate. The name of this secret must be set in the <code>.controller.registryCertSecret</code> value. You can find more details in the Configuring a Custom OCI Registry section.</p> <pre><code>controller:\n  templatesRepoURL: \"oci://registry.local/k0rdent-enterprise/charts\"\n  globalRegistry: \"registry.local/k0rdent-enterprise\"\n  globalK0sURL: \"http://binary.local/k0rdent-enterprise\"\n\nimage:\n  repository: registry.local/k0rdent-enterprise/kcm-controller\n\ncert-manager:\n  image:\n    repository: registry.local/k0rdent-enterprise/jetstack/cert-manager-controller\n  webhook:\n    image:\n      repository: registry.local/k0rdent-enterprise/jetstack/cert-manager-webhook\n  cainjector:\n    image:\n      repository: registry.local/k0rdent-enterprise/jetstack/cert-manager-cainjector\n  startupapicheck:\n    image:\n      repository: registry.local/k0rdent-enterprise/jetstack/cert-manager-startupapicheck\n\nflux2:\n  helmController:\n    image: registry.local/k0rdent-enterprise/fluxcd/helm-controller\n  sourceController:\n    image: registry.local/k0rdent-enterprise/fluxcd/source-controller\n  cli:\n    image: registry.local/k0rdent-enterprise/fluxcd/flux-cli\n\ncluster-api-operator:\n  image:\n    manager:\n      repository: registry.local/k0rdent-enterprise/capi-operator/cluster-api-operator\n\nvelero:\n  image:\n    repository: registry.local/k0rdent-enterprise/velero/velero\n</code></pre> <p>Start the installation itself with the following command:</p> <pre><code>helm install kcm oci://registry.local/k0rdent-enterprise/charts/k0rdent-enterprise --version 1.1.0 -n kcm-system --create-namespace -f kcm-values.yaml\n</code></pre> <p>From here you can verify the installation as normal.</p>"},{"location":"admin/installation/airgap/airgap-prereq/","title":"Airgapped Install Prerequisites","text":"<p>Before starting an airgapped install of Mirantis k0rdent Enterprise, make sure the following tools and infrastructure are in place.</p>"},{"location":"admin/installation/airgap/airgap-prereq/#tools","title":"Tools","text":"<p>The following tools will be needed on the machine inside the airgapped environment that will be used for the Mirantis k0rdent Enterprise installation.</p> <p>Note</p> <p> The following tools will be used in the scripts and examples throughout this section. The exact versions are listed for reference purposes; you may be able to run everything on older versions.</p> <ul> <li><code>bash &gt;=4.2</code>, recommended <code>&gt;=5.1</code></li> <li>GNU Coreutils <code>&gt;=8.32</code> for basic file manipulations</li> <li><code>tar 1.34</code> with gzip support</li> <li><code>wget</code> or any other tool to download bundle via HTTP</li> <li><code>skopeo &gt;=1.17.0</code></li> <li><code>cosign &gt;=2.4.1</code></li> <li><code>helm &gt;= 3.16.3</code></li> </ul> <p>The correct version of <code>skopeo</code> is crucial to correctly upload all images in the bundle. It's included in the airgap bundle in the form of a Docker image.</p>"},{"location":"admin/installation/airgap/airgap-prereq/#infrastructure","title":"Infrastructure","text":"<p>Prepare all of the following infrastructure before installing Mirantis k0rdent Enterprise:</p> <ul> <li>A container registry with OCI support, such as Harbor.</li> <li> <p>An HTTP server to serve the k0s binary to enable child   cluster creation.</p> <p>Note</p> <p> You can follow the optional HTTP server setup guide to setup an HTTP server if you don't already have one in your airgapped environment.</p> </li> <li> <p>A Kubernetes cluster on which to install   Mirantis k0rdent Enterprise. This management cluster should be using   Kubernetes version <code>&gt;=1.30</code>.</p> </li> <li>Working networking with external IPAM (DHCP) and connectivity to all necessary   resources (such as the registry and Kubernetes API endpoint).</li> </ul>"},{"location":"admin/installation/auth/","title":"OpenID Connect (OIDC)","text":"<p>OpenID Connect (OIDC) is an identity layer built on top of the OAuth 2.0 authorization framework. It streamlines the authentication process by enabling applications to verify a user's identity and retrieve essential profile information in a standardized and secure manner. Mirantis k0rdent Enterprise integrates with multiple OIDC providers.</p>"},{"location":"admin/installation/auth/#core-concepts-and-components","title":"Core Concepts and Components","text":"<p>OIDC enables Mirantis k0rdent Enterprise to provide a standardized way to handle authentication. </p> <ul> <li> <p>Authentication vs. Authorization:  It's easy to conflate authetication and authorization, but while they are related, they are not the same. Authentication establishes the identity of an individual or application, while authorization defines what that identity has permission to do.  While OAuth 2.0 is focused on authorization, granting applications access to resources, OIDC specifically addresses the authentication piece of the puzzle. It enables applications to confirm that a user is who they claim to be before allowing access.</p> </li> <li> <p>ID Tokens and JWTs:   At the heart of OIDC is the ID token, a JSON Web Token (JWT) that carries critical information (claims) about the user, such as a unique identifier, name, email, and the token\u2019s expiration. These tokens are digitally signed (often with RS256), which means they can be validated by any service that has access to the public key, eliminating the need for constant re-validation of credentials.</p> </li> <li> <p>Discovery and Dynamic Configuration:   OIDC features a discovery mechanism via the <code>/.well-known/openid-configuration</code> endpoint. This endpoint automatically provides metadata about the identity provider, including available authentication endpoints, supported scopes, and the necessary public keys for token verification. This simplifies setup and integration, reducing the need for manual configuration.</p> </li> <li> <p>Scopes and Claims:   The protocol defines standard scopes\u2014such as <code>openid</code>, <code>profile</code>, and <code>email</code>\u2014that specify what user data the client wishes to access. In response, the ID token will include corresponding claims, delivering verified user information in a secure and structured format. Custom scopes and claims can also be configured to meet specific application needs.</p> </li> <li> <p>Flexible Authentication Flows:   OIDC supports multiple flows tailored to different application types:</p> </li> <li>Authorization Code Flow: Ideal for server-side applications, where the client exchanges an authorization code for tokens in a secure manner.</li> <li>Implicit Flow: Suited for single-page or mobile applications, delivering tokens directly through the browser or app interface.</li> <li>Hybrid Flow: Combines elements of both the authorization code and implicit flows, balancing security and performance.</li> </ul>"},{"location":"admin/installation/auth/#oidc-in-the-context-of-kubernetes","title":"OIDC in the Context of Kubernetes","text":"<p>Integrating OIDC with Kubernetes enables the cluster to delegate authentication to an external identity provider. Here\u2019s how it works:</p> <ul> <li> <p>Delegated Authentication:   The Kubernetes API server is configured with OIDC parameters (like <code>--oidc-issuer-url</code> and <code>--oidc-client-id</code>). (In the context of Mirantis k0rdent Enterprise these are configured with templates.) When a user makes a request (for example, via <code>kubectl</code>), the API server validates the accompanying OIDC token by checking its signature, issuer, and expiration date. This offloads the responsibility of authentication to a trusted external provider.</p> </li> <li> <p>User Identity and RBAC:   Upon successful token validation, Kubernetes extracts user details (such as username or group memberships) from the token\u2019s claims. These details are then used to enforce Role-Based Access Control (RBAC) policies, ensuring  users have the proper permissions to access resources within the cluster.</p> </li> <li> <p>Simplified and Centralized User Management:   By integrating with a centralized OIDC provider, Kubernetes can manage user identities and access rights more efficiently across multiple clusters. This not only enhances security but also streamlines administrative tasks.</p> </li> </ul>"},{"location":"admin/installation/auth/#oidc-in-mirantis-k0rdent-enterprise","title":"OIDC in Mirantis k0rdent Enterprise","text":"<p>Mirantis k0rdent Enterprise supports multiple OIDC providers to accommodate various organizational needs. These options are:</p> <ul> <li>Okta</li> <li>Entra-ID</li> <li>Pinniped</li> </ul>"},{"location":"admin/installation/auth/entra-id/","title":"Microsoft Entra ID","text":"<p>This section explains how to configure Mirantis k0rdent Enterprise to use Microsoft Entra ID (formerly Azure Active Directory) as an OIDC provider for authentication. While the examples use KinD (Kubernetes in Docker) for demonstration purposes, the concepts and procedures are fully applicable to any Mirantis k0rdent Enterprise management cluster (for example, Minikube, MicroK8s, or a cloud-based cluster) that meets the minimum requirements for k0rdent.</p>"},{"location":"admin/installation/auth/entra-id/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that your environment meets the following prerequisites.</p>"},{"location":"admin/installation/auth/entra-id/#1-required-software","title":"1. Required Software","text":"<p>Make sure your development machine has the following installed:</p> <ul> <li>Docker: Container runtime to build and run containerized applications.</li> <li>Mirantis k0rdent Enterprise Management Cluster: Although KinD is used in this guide, you may use any Mirantis k0rdent Enterprise management cluster that supports deploying k0rdent.</li> <li>Helm: A package manager for Kubernetes to install and manage applications.</li> <li>Coreutils: Standard UNIX utilities for various file operations.</li> <li>jq: A lightweight and flexible command-line JSON processor.</li> <li>jwt: A CLI tool to decode and inspect JSON Web Tokens.</li> </ul>"},{"location":"admin/installation/auth/entra-id/#2-microsoft-entra-id-setup","title":"2. Microsoft Entra ID Setup","text":"<p>Prepare your Microsoft Entra ID environment by completing the following steps:</p> <ul> <li> <p>Register an OIDC-Enabled Application:   In the Microsoft Entra ID (formerly Azure AD) portal, register an application with OIDC enabled. Ensure that you configure the appropriate redirect URIs (for example, <code>http://localhost:8000</code>).</p> </li> <li> <p>Assign Users to an Entra ID Group:   Ensure that the users who will authenticate are assigned to an Entra ID group that you will use for Kubernetes RBAC.</p> </li> <li> <p>Configure Claims:   Verify that the OIDC application returns the necessary claims\u2014preferred_username (or email/upn if preferred), groups, and profile.  </p> <p>Note: Some Entra ID configurations might not return the <code>email</code> claim. In such cases, the <code>preferred_username</code> claim is used as the username.</p> </li> <li> <p>Obtain Entra ID Credentials:   After registering your application, note the following:</p> </li> <li>Your Tenant ID.</li> <li>The Authorization Server URL (for example, <code>https://login.microsoftonline.com/&lt;tenant-id&gt;/v2.0</code>).</li> <li>The Client ID and, if applicable, the Client Secret generated for your Kubernetes application.</li> </ul>"},{"location":"admin/installation/auth/entra-id/#installation-steps","title":"Installation Steps","text":"<p>Follow these steps to deploy Microsoft Entra ID as your OIDC provider.</p>"},{"location":"admin/installation/auth/entra-id/#1-install-krew-kubectl-plugin-manager","title":"1. Install Krew (Kubectl Plugin Manager)","text":"<p>Krew is a package manager for kubectl plugins. Installing it ensures you can manage the OIDC login plugin easily.</p> <p>You can install Krew by running the following command in your terminal. This script detects your OS and architecture, downloads the latest Krew release, extracts it, and installs it:</p> <pre><code>(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n</code></pre> <p>For additional details, troubleshooting tips, and instructions for Windows installations, see the official Krew Installation Guide.</p>"},{"location":"admin/installation/auth/entra-id/#2-install-the-oidc-login-plugin","title":"2. Install the OIDC Login Plugin","text":"<p>The OIDC login plugin for kubectl simplifies the process of obtaining and refreshing tokens from your Entra ID instance.</p> <pre><code>kubectl krew update\nkubectl krew install oidc-login\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#3-create-the-structured-authentication-configuration","title":"3. Create the Structured Authentication Configuration","text":"<p>This configuration file tells your Kubernetes API server how to validate JWT tokens issued by Entra ID. Create a file named <code>authentication-config.yaml</code> with the following content:</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt:\n  - issuer:\n      url: \"https://login.microsoftonline.com/&lt;tenant-id&gt;/v2.0\"\n      audiences:\n        - \"&lt;client-id&gt;\"\n    claimMappings:\n      username:\n        claim: preferred_username  # Use 'email' or 'upn' if preferred and available\n        prefix: \"\"\n      groups:\n        claim: groups\n        prefix: \"\"\n    claimValidationRules:\n      - expression: \"has(claims.preferred_username)\"\n        message: \"preferred_username claim must be present\"\n      - expression: \"claims.preferred_username != ''\"\n        message: \"preferred_username claim must be non-empty\"\n      - expression: \"has(claims.groups)\"\n        message: \"groups claim must be present\"\n      - expression: \"type(claims.groups) == list ? size(claims.groups) &gt; 0 : true\"\n        message: \"groups list must be non-empty\"\n      - expression: \"type(claims.groups) == string ? claims.groups.size() &gt; 0 : true\"\n        message: \"groups string must be non-empty\"\n    userValidationRules:\n      - expression: \"!user.username.startsWith('system:')\"\n        message: \"username cannot use reserved system: prefix\"\n      - expression: \"user.groups.all(group, !group.startsWith('system:'))\"\n        message: \"groups cannot use reserved system: prefix\"\n</code></pre> <p>Note: Replace <code>&lt;tenant-id&gt;</code> and <code>&lt;client-id&gt;</code> with the actual values from your Microsoft Entra ID application registration. Adjust the claim mappings if your token uses different claim names (for example, <code>email</code> or <code>upn</code>).</p>"},{"location":"admin/installation/auth/entra-id/#4-configure-your-kubernetes-cluster","title":"4. Configure Your Kubernetes Cluster","text":"<p>Below is an example KinD cluster configuration that mounts the authentication configuration file. Adapt these instructions if you are using another Kubernetes system.</p>"},{"location":"admin/installation/auth/entra-id/#create-the-kind-cluster-configuration","title":"Create the KinD Cluster Configuration","text":"<p>Create a file named <code>kind-config.yaml</code>:</p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nfeatureGates:\n  \"StructuredAuthenticationConfiguration\": true\nnodes:\n  - role: control-plane\n    kubeadmConfigPatches:\n      - |\n        kind: ClusterConfiguration\n        apiServer:\n          extraArgs:\n            authentication-config: /etc/kubernetes/authentication-config.yaml\n          extraVolumes:\n            - name: authentication-config\n              hostPath: /etc/kubernetes/authentication-config.yaml\n              mountPath: /etc/kubernetes/authentication-config.yaml\n              readOnly: true\n              pathType: File\n    extraMounts:\n      - hostPath: ./authentication-config.yaml\n        containerPath: /etc/kubernetes/authentication-config.yaml\n        readOnly: true\n</code></pre> <p>For other Kubernetes distributions, the concept remains the same\u2014you need to configure your API server to load the <code>authentication-config.yaml</code> file. Consult your cluster\u2019s documentation for mounting configuration files and setting extra API server arguments.</p>"},{"location":"admin/installation/auth/entra-id/#5-cluster-management-with-kind-example","title":"5. Cluster Management with KinD (Example)","text":"<p>If you are using KinD, execute the following commands. Otherwise, adjust these steps to match your Kubernetes provider\u2019s procedures.</p>"},{"location":"admin/installation/auth/entra-id/#create-the-kind-cluster","title":"Create the KinD Cluster","text":"<pre><code>kind create cluster --verbosity 99 --config kind-config.yaml --retain\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#retrieve-api-server-pod-information","title":"Retrieve API Server Pod Information","text":"<p>To inspect the API server configuration, use:</p> <pre><code>kubectl describe pod -n kube-system kube-apiserver-$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#debugging-the-control-plane","title":"Debugging the Control Plane","text":"<p>For troubleshooting, you can view logs and container status:</p> <pre><code>docker exec kind-control-plane ls /var/log/containers/\ndocker exec kind-control-plane crictl ps\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#rbac-configuration","title":"RBAC Configuration","text":"<p>For Mirantis k0rdent Enterprise to work properly with OIDC-authenticated users, configure RBAC policies. Below is a sample RoleBinding configuration that grants permissions to a specific group.</p> <p>Create a file named <code>rolebinding.yaml</code>:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kcm-ns-viewer\n  namespace: kcm-system\nsubjects:\n  - kind: Group\n    name: \"&lt;group-id-from-token&gt;\"\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kcm-namespace-viewer-role\n</code></pre> <p>Note: Replace <code>&lt;group-id-from-token&gt;</code> with the group identifier returned in the Entra ID token that should have access to the <code>kcm-system</code> namespace.</p> <p>Apply this configuration using:</p> <pre><code>kubectl apply -f rolebinding.yaml\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#token-management","title":"Token Management","text":""},{"location":"admin/installation/auth/entra-id/#obtaining-a-microsoft-entra-id-oidc-token","title":"Obtaining a Microsoft Entra ID OIDC Token","text":"<p>Use the <code>kubectl oidc-login</code> plugin to retrieve a token from your Entra ID instance. The token is used for authenticating with your Kubernetes API server.</p> <pre><code>export K8S_TOKEN=$(kubectl oidc-login get-token \\\n  --oidc-issuer-url=https://login.microsoftonline.com/&lt;tenant-id&gt;/v2.0 \\\n  --oidc-client-id=&lt;client-id&gt; \\\n  --oidc-client-secret=&lt;client-secret&gt; \\\n  --oidc-redirect-url-hostname=localhost \\\n  --listen-address=localhost:8000 \\\n  --skip-open-browser=true \\\n  --oidc-extra-scope=\"email profile openid\" \\\n  --force-refresh | jq -r '.status.token' \\\n) &amp;&amp; echo $K8S_TOKEN | jwt decode -\n</code></pre> <p>Tip: Replace <code>&lt;tenant-id&gt;</code>, <code>&lt;client-id&gt;</code>, and <code>&lt;client-secret&gt;</code> with the actual values from your Entra ID application registration. The <code>--oidc-redirect-url-hostname</code> should match the redirect URI configured in Entra ID.</p>"},{"location":"admin/installation/auth/entra-id/#debug-token-validation","title":"Debug Token Validation","text":"<p>Validate the token by performing a simple API call that includes a higher verbosity level for debugging:</p> <pre><code>kubectl --token=$K8S_TOKEN get secrets -n kcm-system -v=9\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#configuring-kubernetes-cli","title":"Configuring Kubernetes CLI","text":"<p>After obtaining your token, update your kubectl configuration to use these OIDC credentials.</p>"},{"location":"admin/installation/auth/entra-id/#1-set-user-credentials","title":"1. Set User Credentials","text":"<pre><code>kubectl config set-credentials user --token=$K8S_TOKEN\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#2-create-a-new-context","title":"2. Create a New Context","text":"<p>Set up a context that references your cluster and the new user credentials. For KinD, you might use:</p> <pre><code>kubectl config set-context user --cluster=\"kind-$(kind get clusters | head -1)\" --user=user --namespace=kcm-system\n</code></pre> <p>For other Kubernetes clusters, replace the cluster name appropriately.</p>"},{"location":"admin/installation/auth/entra-id/#3-verify-access","title":"3. Verify Access","text":"<p>Confirm that your OIDC credentials provide the necessary access:</p> <pre><code>kubectl --context=user auth can-i get namespaces\nkubectl --context=user auth can-i get secrets -n kcm-system\nkubectl --context=user auth can-i get pods -n kcm-system\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#4-switch-contexts","title":"4. Switch Contexts","text":"<p>Switch to the OIDC context when needed:</p> <pre><code>kubectl config use-context user\n</code></pre> <p>To revert to your default context, use the standard context name (for example, the KinD default):</p> <pre><code>kubectl config use-context \"kind-$(kind get clusters | head -1)\"\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#5-view-kubeconfig-details","title":"5. View Kubeconfig Details","text":"<p>Inspect your current kubeconfig to confirm the setup:</p> <pre><code>kubectl config view --context=user\n</code></pre>"},{"location":"admin/installation/auth/entra-id/#6-debug-inspect-api-server-logs","title":"6. [DEBUG] Inspect API Server Logs","text":"<p>For further troubleshooting, review the API server logs:</p> <pre><code>kubectl --context=\"kind-$(kind get clusters | head -1)\" logs -n kube-system kube-apiserver-kind-control-plane | grep authentication.go\n</code></pre>"},{"location":"admin/installation/auth/okta/","title":"OIDC Authentication Setup Guide for Okta","text":"<p>This section explains how to configure Mirantis k0rdent Enterprise to use Okta as an OIDC provider for authentication. While the examples use KinD (Kubernetes in Docker) for demonstration purposes, the concepts and procedures are fully applicable to any Mirantis k0rdent Enterprise management cluster (for example, Minikube, MicroK8s, or a cloud-based cluster) that meets the minimum requirements for k0rdent.</p>"},{"location":"admin/installation/auth/okta/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that your environment meets the following prerequisites.</p>"},{"location":"admin/installation/auth/okta/#1-required-software","title":"1. Required Software","text":"<p>Make sure your development machine has the following installed:</p> <ul> <li>Docker: Container runtime to build and run containerized applications.</li> <li>Mirantis k0rdent Enterprise Management Cluster: Although KinD is used in this guide, you may use any Mirantis k0rdent Enterprise management cluster (for example, Minikube, MicroK8s, or a cloud-based cluster) that supports deploying k0rdent.</li> <li>Helm: A package manager for Kubernetes to install and manage applications. </li> <li>Coreutils: Standard UNIX utilities for various file operations. </li> <li>jq: A lightweight and flexible command-line JSON processor. </li> <li>jwt: A CLI tool to decode and inspect JSON Web Tokens.</li> </ul>"},{"location":"admin/installation/auth/okta/#2-okta-setup","title":"2. Okta Setup","text":"<p>Prepare your Okta environment by completing the following steps:</p> <ul> <li> <p>Create an Okta Developer Account:   Visit the Okta Developer Signup page and create an account if you don\u2019t already have one.</p> </li> <li> <p>Review Okta Configuration Guides:   Familiarize yourself with the Okta user interface and setup procedures using guides such as the UI Guide. Although this guide focuses on Kubernetes, these resources provide valuable context on configuring your Okta organization, applications, and OIDC settings.</p> </li> <li> <p>Obtain Okta Credentials:   After setting up your Okta account, note the following:</p> </li> <li>Your Okta Domain (for example, <code>https://your-okta-domain.okta.com</code>)</li> <li>The Authorization Server URL (typically something like <code>https://your-okta-domain.okta.com/oauth2/default</code>)</li> <li>The Client ID generated when you register your Kubernetes application in Okta</li> <li>Any additional scopes or API keys as required by your integration</li> </ul>"},{"location":"admin/installation/auth/okta/#installation-steps","title":"Installation Steps","text":"<p>Follow these steps to deloy Okta.</p>"},{"location":"admin/installation/auth/okta/#1-install-krew-kubectl-plugin-manager","title":"1. Install Krew (Kubectl Plugin Manager)","text":"<p>Krew is a package manager for kubectl plugins. Installing it ensures you can manage the OIDC login plugin easily.</p> <p>You can install Krew by running the following command in your terminal. This script detects your OS and architecture, downloads the latest Krew release, extracts it, and installs it:</p> <pre><code>(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n</code></pre> <p>For additional details, troubleshooting tips, and instructions for Windows installations, see the official Krew Installation Guide.</p>"},{"location":"admin/installation/auth/okta/#2-install-the-oidc-login-plugin","title":"2. Install the OIDC Login Plugin","text":"<p>The OIDC login plugin for kubectl simplifies the process of obtaining and refreshing tokens from your Okta instance.</p> <pre><code>kubectl krew update\nkubectl krew install oidc-login\n</code></pre>"},{"location":"admin/installation/auth/okta/#3-create-the-structured-authentication-configuration","title":"3. Create the Structured Authentication Configuration","text":"<p>This configuration file tells your Kubernetes API server how to validate JWT tokens issued by Okta. Create a file named <code>authentication-config.yaml</code> with the following content:</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt:\n  - issuer:\n      url: \"https://trial-***.okta.com/oauth2/***\"\n      audiences:\n        - 0oa***697\n    claimMappings:\n      username:\n        claim: email\n        prefix: \"\"\n      groups:\n        claim: groups\n        prefix: \"\"\n    claimValidationRules:\n      - expression: \"has(claims.email)\"\n        message: \"email claim must be present\"\n      - expression: \"claims.email != ''\"\n        message: \"email claim must be non-empty\"\n      - expression: \"has(claims.groups)\"\n        message: \"groups claim must be present\"\n      - expression: \"type(claims.groups) == list ? size(claims.groups) &gt; 0 : true\"\n        message: \"groups list must be non-empty\"\n      - expression: \"type(claims.groups) == string ? claims.groups.size() &gt; 0 : true\"\n        message: \"groups string must be non-empty\"        \n    userValidationRules:\n      - expression: \"!user.username.startsWith('system:')\"\n        message: \"username cannot use reserved system: prefix\"\n      - expression: \"user.groups.all(group, !group.startsWith('system:'))\"\n        message: \"groups cannot use reserved system: prefix\"\n</code></pre> <p>Note: Replace placeholder values (e.g., <code>trial-***</code>, <code>0oa***697</code>) with the actual values from your Okta configuration.</p>"},{"location":"admin/installation/auth/okta/#4-configure-your-kubernetes-cluster","title":"4. Configure Your Kubernetes Cluster","text":"<p>Below is an example KinD cluster configuration that mounts the authentication configuration file. Adapt these instructions if you are using another Kubernetes system.</p>"},{"location":"admin/installation/auth/okta/#create-the-kind-cluster-configuration","title":"Create the KinD Cluster Configuration","text":"<p>Create a file named <code>kind-config.yaml</code>:</p> <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nfeatureGates:\n  \"StructuredAuthenticationConfiguration\": true\nnodes:\n  - role: control-plane\n    kubeadmConfigPatches:\n      - |\n        kind: ClusterConfiguration\n        apiServer:\n          extraArgs:\n            authentication-config: /etc/kubernetes/authentication-config.yaml\n          extraVolumes:\n            - name: authentication-config\n              hostPath: /etc/kubernetes/authentication-config.yaml\n              mountPath: /etc/kubernetes/authentication-config.yaml\n              readOnly: true\n              pathType: File\n    extraMounts:\n      - hostPath: ./authentication-config.yaml\n        containerPath: /etc/kubernetes/authentication-config.yaml\n        readOnly: true\n</code></pre> <p>For other Kubernetes distributions, the concept remains the same \u2014 you need to configure your API server to load the <code>authentication-config.yaml</code> file. Check your cluster\u2019s documentation for mounting configuration files and setting extra API server arguments.</p>"},{"location":"admin/installation/auth/okta/#5-cluster-management-with-kind-example","title":"5. Cluster Management with KinD (Example)","text":"<p>If you are using KinD, execute the following commands. Otherwise, adjust these steps to match your Kubernetes provider\u2019s procedures.</p>"},{"location":"admin/installation/auth/okta/#create-the-kind-cluster","title":"Create the KinD Cluster","text":"<pre><code>kind create cluster --verbosity 99 --config kind-config.yaml --retain\n</code></pre>"},{"location":"admin/installation/auth/okta/#retrieve-api-server-pod-information","title":"Retrieve API Server Pod Information","text":"<p>To inspect the API server configuration, use:</p> <pre><code>kubectl describe pod -n kube-system kube-apiserver-$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')\n</code></pre>"},{"location":"admin/installation/auth/okta/#debugging-the-control-plane","title":"Debugging the Control Plane","text":"<p>For troubleshooting, you can view logs and container status:</p> <pre><code>docker exec kind-control-plane ls /var/log/containers/\ndocker exec kind-control-plane crictl ps\n</code></pre>"},{"location":"admin/installation/auth/okta/#rbac-configuration","title":"RBAC Configuration","text":"<p>For Mirantis k0rdent Enterprise to work properly with OIDC-authenticated users, configure RBAC policies. Below is a sample RoleBinding configuration that grants permissions to a specific group.</p> <p>Create a file named <code>rolebinding.yaml</code>:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kcm-ns-viewer\n  namespace: kcm-system\nsubjects:\n  - kind: Group\n    name: kcm-ns-viewer\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kcm-namespace-viewer-role\n</code></pre> <p>Apply this configuration using:</p> <pre><code>kubectl apply -f rolebinding.yaml\n</code></pre>"},{"location":"admin/installation/auth/okta/#token-management","title":"Token Management","text":""},{"location":"admin/installation/auth/okta/#obtaining-an-okta-oidc-token","title":"Obtaining an Okta OIDC Token","text":"<p>Use the <code>kubectl oidc-login</code> plugin to retrieve a token from your Okta instance. The token is used for authenticating with your Kubernetes API server.</p> <pre><code>export K8S_TOKEN=$(kubectl oidc-login get-token \\\n  --oidc-issuer-url=https://trial-***.okta.com/oauth2/*** \\\n  --oidc-client-id=0oa***697 \\\n  --listen-address=127.0.0.1:8000 \\\n  --skip-open-browser=true \\\n  --oidc-extra-scope=email \\\n  --force-refresh | jq -r '.status.token' \\\n) &amp;&amp; echo $K8S_TOKEN | jwt decode -\n</code></pre> <p>Tip: Replace the placeholder values with your actual Okta configuration details. The command pipes the token to <code>jwt decode</code> so you can inspect its contents.</p>"},{"location":"admin/installation/auth/okta/#debug-token-validation","title":"Debug Token Validation","text":"<p>Validate the token by performing a simple API call that includes a higher verbosity level for debugging:</p> <pre><code>kubectl --token=$K8S_TOKEN get secrets -n kcm-system -v=9\n</code></pre>"},{"location":"admin/installation/auth/okta/#configuring-kubernetes-cli","title":"Configuring Kubernetes CLI","text":"<p>After obtaining your token, update your kubectl configuration to use these OIDC credentials.</p>"},{"location":"admin/installation/auth/okta/#1-set-user-credentials","title":"1. Set User Credentials","text":"<pre><code>kubectl config set-credentials user --token=$K8S_TOKEN\n</code></pre>"},{"location":"admin/installation/auth/okta/#2-create-a-new-context","title":"2. Create a New Context","text":"<p>Set up a context that references your cluster and the new user credentials. For KinD, you might use:</p> <pre><code>kubectl config set-context user --cluster=\"kind-$(kind get clusters | head -1)\" --user=user --namespace=kcm-system\n</code></pre> <p>For other Kubernetes clusters, replace the cluster name appropriately.</p>"},{"location":"admin/installation/auth/okta/#3-verify-access","title":"3. Verify Access","text":"<p>Confirm that your OIDC credentials provide the necessary access:</p> <pre><code>kubectl --context=user auth can-i get namespaces\nkubectl --context=user auth can-i get secrets -n kcm-system\nkubectl --context=user auth can-i get pods -n kcm-system\n</code></pre>"},{"location":"admin/installation/auth/okta/#4-switch-contexts","title":"4. Switch Contexts","text":"<p>Switch to the OIDC context when needed:</p> <pre><code>kubectl config use-context user\n</code></pre> <p>To revert to your default context, use the standard context name (for example, the KinD default):</p> <pre><code>kubectl config use-context \"kind-$(kind get clusters | head -1)\"\n</code></pre>"},{"location":"admin/installation/auth/okta/#5-view-kubeconfig-details","title":"5. View Kubeconfig Details","text":"<p>Inspect your current kubeconfig to confirm the setup:</p> <pre><code>kubectl config view --context=user\n</code></pre>"},{"location":"admin/installation/auth/okta/#6-inspect-api-server-logs","title":"6. Inspect API Server Logs","text":"<p>If you encounter issues, it can be helpful to review the API server logs:</p> <pre><code>kubectl --context=\"kind-$(kind get clusters | head -1)\" logs -n kube-system kube-apiserver-kind-control-plane | grep authentication.go\n</code></pre>"},{"location":"admin/installation/auth/pinniped/","title":"Pinniped","text":"<p>This section explains how to configure Mirantis k0rdent Enterprise to use the [Pinniped0(https://pinniped.dev/) components as an OIDC provider for authentication. While the examples use KinD (Kubernetes in Docker) for demonstration purposes, the concepts and procedures are fully applicable to any Mirantis k0rdent Enterprise management cluster (for example, Minikube, MicroK8s, or a cloud-based cluster) that meets the minimum requirements for k0rdent.</p>"},{"location":"admin/installation/auth/pinniped/#prerequisites","title":"Prerequisites","text":"<ul> <li>Mirantis k0rdent Enterprise Management Cluster: Although KinD is used in this guide, you may use any Mirantis k0rdent Enterprise management cluster that supports deploying k0rdent.</li> <li>Helm: A package manager for Kubernetes to install and manage applications.</li> <li>cURL: A versatile command-line tool that enables users to send requests to network endpoints, helping to verify connectivity and inspect API responses.</li> <li>Familiarity with basic Kubernetes operations as well as working knowledge of TLS, DNS, and RBAC concepts.</li> </ul>"},{"location":"admin/installation/auth/pinniped/#step-1-create-a-the-mirantis-k0rdent-enterprise-management-cluster","title":"Step 1. Create a the Mirantis k0rdent Enterprise management cluster","text":"<p>Follow instructions in the documentation to install the management cluster.  </p>"},{"location":"admin/installation/auth/pinniped/#step-2-install-the-pinniped-supervisor-on-the-management-cluster","title":"Step 2. Install the <code>Pinniped</code> Supervisor on the management cluster","text":"<p>The Pinniped Supervisor is the core component that handles authentication requests and token validation.</p> <p><pre><code>kubectl apply -f https://get.pinniped.dev/v0.37.0/install-pinniped-supervisor.yaml\n</code></pre> The Pinniped Supervisor installs the necessary resources and configures the Supervisor to manage authentication across clusters.</p>"},{"location":"admin/installation/auth/pinniped/#step-3-expose-the-pinniped-supervisor-deployment","title":"Step 3. Expose the pinniped-supervisor deployment","text":"<p>To enable internal access to the Supervisor, expose its service:</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: pinniped-supervisor\n  namespace: pinniped-supervisor\nspec:\n  type: ClusterIP\n  selector:\n    app: pinniped-supervisor\n  ports:\n    - protocol: TCP\n      port: 443\n      # TLS port of the Supervisor pods\n      targetPort: 8443\nEOF\n</code></pre> <p>This YAML creates a ClusterIP service that maps port 443 on the service to port 8443 on the Supervisor pods, enabling internal cluster communication over TLS.</p>"},{"location":"admin/installation/auth/pinniped/#step-4-install-an-ingress-controller-contour","title":"Step 4. Install an Ingress Controller (Contour)","text":"<p>You'll need an ingress controller, such as Contour, to manage external access and TLS termination.</p> <pre><code>kubectl apply -f https://projectcontour.io/quickstart/contour.yaml\n</code></pre> <p>Contour handles ingress traffic and route requests to the Supervisor. {{ no such element: super_collections.SuperDict object['k0rdentname'] }} uses it for managing external access with secure, TLS-passthrough configurations. You can accomplish the same thing with other Ingress controllers such as Nginx.</p>"},{"location":"admin/installation/auth/pinniped/#step-5-create-an-ingress-for-the-supervisor-which-uses-tls-passthrough-to-allow-the-supervisor-to-terminate-tls","title":"Step 5. Create an Ingress for the Supervisor which uses TLS passthrough to allow the Supervisor to terminate TLS","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: projectcontour.io/v1\nkind: HTTPProxy\nmetadata:\n  name: supervisor-proxy\n  namespace: pinniped-supervisor\nspec:\n  virtualhost:\n    fqdn: pinniped-supervisor.pinniped-supervisor.svc.cluster.local\n    tls:\n      passthrough: true\n  tcpproxy:\n    services:\n      - name: pinniped-supervisor\n        port: 443\nEOF\n</code></pre> <p>This configuration sets up an ingress route that directs traffic for the specified FQDN to the Supervisor service on port 443. TLS passthrough ensures that TLS termination happens at the Supervisor, preserving end-to-end encryption.</p>"},{"location":"admin/installation/auth/pinniped/#step-6-check-the-status-of-the-http-proxy","title":"Step 6. Check the status of the HTTP proxy","text":"<p>To access the Supervisor via a web browser, add a custom DNS record on your local machine.</p> <p>Check that the HTTP proxy has been correctly configured and is running as expected:</p> <pre><code>kubectl get httpproxy supervisor-proxy --namespace pinniped-supervisor -o yaml\n</code></pre>"},{"location":"admin/installation/auth/pinniped/#step-7-add-a-custom-dns-record-to-make-it-available-to-reach-the-supervisor-through-dns","title":"Step 7. Add a custom DNS record to make it available to reach the supervisor through DNS","text":"<p>Do this on the machine where you will be accessing authorization via Web Browser.</p>"},{"location":"admin/installation/auth/pinniped/#linux-etchosts","title":"Linux <code>/etc/hosts</code>","text":"<pre><code>sudo bash -c \\\n\"echo '127.0.0.1 pinniped-supervisor.pinniped-supervisor.svc.cluster.local' &gt;&gt; /etc/hosts\"\n</code></pre>"},{"location":"admin/installation/auth/pinniped/#windows-cwindowssystem32driversetchosts","title":"Windows <code>c:\\Windows\\System32\\Drivers\\etc\\hosts</code>","text":"<p>In a Powershell window with elevated privileges, type:</p> <pre><code>Add-Content -Path \"C:\\Windows\\System32\\drivers\\etc\\hosts\" -Value \"127.0.0.1 pinniped-supervisor.pinniped-supervisor.svc.cluster.local\"\n</code></pre> <p>These commands add a local DNS entry so that requests to the specified FQDN are resolved to your local machine, enabling access to the Pinniped Supervisor.</p>"},{"location":"admin/installation/auth/pinniped/#step-8-install-cert-manager","title":"Step 8. Install cert-manager","text":"<p>cert-manager is used to manage TLS certificates within Kubernetes.</p> <p><pre><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.15.0/cert-manager.yaml\n</code></pre> cert-manager automates the issuance and renewal of TLS certificates. This installation is essential for generating and managing certificates used by the Pinniped Supervisor.</p>"},{"location":"admin/installation/auth/pinniped/#step-9-generate-a-self-signed-certificate","title":"Step 9. Generate a self-signed certificate","text":"<p>Create a self-signed certificate to secure the Supervisor\u2019s communications.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\n---\n# Create a self-signed issuer\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: selfsigned-issuer\n  namespace: pinniped-supervisor\nspec:\n  selfSigned: {}\n---\n# Use the self-signed issuer to create a self-signed CA\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: demo-ca\n  namespace: pinniped-supervisor\nspec:\n  isCA: true\n  commonName: demo-ca\n  subject:\n    organizations:\n      - Project Pinniped\n    organizationalUnits:\n      - Demo\n  secretName: demo-ca-secret\n  privateKey:\n    algorithm: ECDSA\n    size: 256\n  issuerRef:\n    group: cert-manager.io\n    kind: Issuer\n    name: selfsigned-issuer\n---\n# Create an issuer that will sign certs with our self-signed CA\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: demo-ca-issuer\n  namespace: pinniped-supervisor\nspec:\n  ca:\n    secretName: demo-ca-secret\n---\n# Create serving certificate using our CA, this Secret will be used by Supervisor\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: supervisor-tls-cert-request\n  namespace: pinniped-supervisor\nspec:\n  secretName: supervisor-tls-cert\n  issuerRef:\n    name: demo-ca-issuer\n  dnsNames:\n    - pinniped-supervisor.pinniped-supervisor.svc.cluster.local\nEOF\n</code></pre> <p>This multi-part YAML creates a self-signed issuer, generates a CA, and issues a serving certificate for the Pinniped Supervisor. The certificate is stored in a <code>Secret</code> and later referenced in the <code>FederationDomain</code> configuration.</p>"},{"location":"admin/installation/auth/pinniped/#step-10-read-the-cas-public-key-from-the-secret-and-save-it-locally-for-later-use","title":"Step 10. Read the CA\u2019s public key from the Secret and save it locally for later use","text":"<p>Retrieve the public key of the CA to use later when configuring the Pinniped Concierge.</p> <pre><code>kubectl get secret supervisor-tls-cert \\\n    --namespace pinniped-supervisor \\\n    -o jsonpath=\"{.data['ca\\.crt']}\" | base64 -d &gt; /tmp/supervisor-ca.crt\n</code></pre> <p>Storing the CA's public key locally allows you to verify TLS connections and configure trust for downstream components like Pinniped Concierge.</p>"},{"location":"admin/installation/auth/pinniped/#step-11-configure-an-identity-provider-in-the-pinniped-supervisor","title":"Step 11. Configure an Identity Provider in the Pinniped Supervisor","text":"<p>In this step, you configure an OIDC client application (for example, using Okta) to integrate with Pinniped. Although the configuration below references Okta, you can adjust it to use your preferred identity provider.</p>"},{"location":"admin/installation/auth/pinniped/#create-an-oidc-client-application","title":"Create an OIDC Client Application","text":"<ol> <li>In the Okta Admin Console (or your identity provider):</li> <li>Navigate to Applications and create a new OIDC application.</li> <li>Set the sign-in redirect URI to <code>https://pinniped-supervisor.pinniped-supervisor.svc.cluster.local/demo-issuer/callback</code>.</li> <li>Note the Client ID and Client Secret.</li> <li> <p>Configure the token to include claims such as <code>email</code>, <code>groups</code>, and <code>profile</code>.</p> </li> <li> <p>Create a Group:</p> </li> <li>In your identity provider\u2019s admin console, create a group (e.g., <code>k8s-group</code>) and assign users who should have access to Kubernetes.</li> </ol>"},{"location":"admin/installation/auth/pinniped/#export-the-secrets-as-environment-variables","title":"Export the Secrets as Environment Variables","text":"<pre><code>export OKTA_APP_CLIENT_ID=\"EXAMPLE_OKTA_APP_CLIENT_ID\"\nexport OKTA_APP_CLIENT_SECRET=\"EXAMPLE_OKTA_APP_CLIENT_SECRET\"\n</code></pre>"},{"location":"admin/installation/auth/pinniped/#create-an-oidcidentityprovider-resource","title":"Create an OIDCIdentityProvider Resource","text":"<p>Create the identity provider resource in Pinniped by running:</p> <p><pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: idp.supervisor.pinniped.dev/v1alpha1\nkind: OIDCIdentityProvider\nmetadata:\n  namespace: pinniped-supervisor\n  name: okta\nspec:\n  # Specify the upstream issuer URL (no trailing slash). Change this to the actual issuer provided by your identity provider.\n  issuer: https://my-company.okta.com\n  authorizationConfig:\n    additionalScopes: [offline_access, groups, email]\n    allowPasswordGrant: false\n  claims:\n    username: email\n    groups: groups\n  client:\n    secretName: okta-client-credentials\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  namespace: pinniped-supervisor\n  name: okta-client-credentials\ntype: secrets.pinniped.dev/oidc-client\nstringData:\n  clientID: \"$OKTA_APP_CLIENT_ID\"\n  clientSecret: \"$OKTA_APP_CLIENT_SECRET\"\nEOF\n</code></pre> This resource defines how Pinniped should interact with your OIDC identity provider. It maps token claims to Kubernetes identities and securely references client credentials stored in a secret.</p>"},{"location":"admin/installation/auth/pinniped/#step-12-validate-the-oidcidentityprovider-status","title":"Step 12. Validate the OIDCIdentityProvider Status","text":"<p>Ensure that the identity provider configuration is active and error-free:</p> <pre><code>kubectl describe OIDCIdentityProvider -n pinniped-supervisor okta\n</code></pre> <p>Reviewing the resource status confirms that Pinniped has successfully registered your OIDC identity provider.</p>"},{"location":"admin/installation/auth/pinniped/#step-13-create-and-configure-a-federationdomain","title":"Step 13. Create and Configure a FederationDomain","text":"<p>The <code>FederationDomain</code> resource ties together the TLS configuration and identity providers to expose a unified issuer endpoint.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: config.supervisor.pinniped.dev/v1alpha1\nkind: FederationDomain\nmetadata:\n  name: demo-federation-domain\n  namespace: pinniped-supervisor\nspec:\n  issuer: \"https://pinniped-supervisor.pinniped-supervisor.svc.cluster.local/demo-issuer\"\n  tls:\n    secretName: supervisor-tls-cert\n  identityProviders:\n    - displayName: okta\n      objectRef:\n        apiGroup: idp.supervisor.pinniped.dev\n        kind: OIDCIdentityProvider\n        name: okta\nEOF\n</code></pre> <p>This <code>FederationDomain</code> configuration declares the external issuer URL, references the TLS certificate, and links the previously configured OIDC identity provider.</p>"},{"location":"admin/installation/auth/pinniped/#step-14-verify-the-federationdomain-status","title":"Step 14. Verify the <code>FederationDomain</code> Status","text":"<p>Check the status of the <code>FederationDomain</code> to ensure that all components are correctly integrated:</p> <pre><code>kubectl get federationdomain demo-federation-domain --namespace pinniped-supervisor -o yaml\n</code></pre>"},{"location":"admin/installation/auth/pinniped/#step-15-verify-dns-certificate-ingress-and-federationdomain-integration","title":"Step 15. Verify DNS, Certificate, Ingress, and FederationDomain Integration","text":"<p>Test that the endpoint is accessible and properly serving OIDC configuration by using curl:</p> <pre><code>curl --cacert /tmp/supervisor-ca.crt \\\n    \"https://pinniped-supervisor.pinniped-supervisor.svc.cluster.local/demo-issuer/.well-known/openid-configuration\"\n</code></pre> <p>This command retrieves the OIDC discovery document, confirming that DNS resolution, TLS, ingress routing, and FederationDomain configuration are all working in unison.</p>"},{"location":"admin/installation/auth/pinniped/#step-16-install-pinniped-concierge-agent","title":"Step 16. Install Pinniped Concierge (Agent)","text":"<p>The Pinniped Concierge component is responsible for issuing and refreshing authentication tokens on behalf of end users.</p> <pre><code>kubectl apply -f \"https://get.pinniped.dev/v0.37.0/install-pinniped-concierge-crds.yaml\"\nkubectl apply -f \"https://get.pinniped.dev/v0.37.0/install-pinniped-concierge-resources.yaml\"\n</code></pre> <p>These commands install the necessary Custom Resource Definitions (CRDs) and resources for Pinniped Concierge, enabling it to integrate with your authentication flow.</p>"},{"location":"admin/installation/auth/pinniped/#step-17-prepare-the-ca-certificate-for-concierge","title":"Step 17. Prepare the CA Certificate for Concierge","text":"<p>Copy the base64-encoded version of the CA certificate saved earlier. This certificate is needed so that the Concierge can trust the Supervisor\u2019s <code>FederationDomain</code>.</p> <pre><code>cat /tmp/supervisor-ca.crt | base64 -w0\n</code></pre> <p>This command outputs the CA certificate in a single base64-encoded line. Save this output for later use when configuring the Concierge.</p>"},{"location":"admin/installation/auth/pinniped/#step-18-configure-the-concierge-with-a-jwtauthenticator","title":"Step 18. Configure the Concierge with a <code>JWTAuthenticator</code>","text":"<p>Create a <code>JWTAuthenticator</code> resource to allow the Concierge to trust the Supervisor\u2019s <code>FederationDomain</code>.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: authentication.concierge.pinniped.dev/v1alpha1\nkind: JWTAuthenticator\nmetadata:\n  name: demo-supervisor-jwt-authenticator\nspec:\n  issuer: \"https://pinniped-supervisor.pinniped-supervisor.svc.cluster.local/demo-issuer\"\n  audience: workload1-dd9de13c370982f61e9f\n  tls:\n    certificateAuthorityData: \"---&gt; Paste the base64 encoded CA here &lt;---\"\nEOF\n</code></pre> <p>The <code>JWTAuthenticator</code> resource configures the Concierge to trust tokens issued by the Supervisor. Replace the placeholder with the base64-encoded CA certificate you generated in the previous step.</p>"},{"location":"admin/installation/auth/pinniped/#step-19-verify-the-jwtauthenticator-status","title":"Step 19. Verify the <code>JWTAuthenticator</code> Status","text":"<p>Ensure that the JWTAuthenticator resource is ready:</p> <p><pre><code>kubectl get jwtauthenticator demo-supervisor-jwt-authenticator -o yaml\n</code></pre> The status should indicate that the resource is in the \u201cReady\u201d phase, confirming that the Concierge is configured correctly to validate tokens.</p>"},{"location":"admin/installation/auth/pinniped/#step-20-create-a-rolebinding-for-end-user-access","title":"Step 20. Create a <code>RoleBinding</code> for End-User Access","text":"<p>Define RBAC policies to grant authenticated users appropriate permissions. In this example, a <code>RoleBinding</code> is created to bind users from a specified identity provider group to a <code>ClusterRole</code>.</p> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kcm-ns-viewer\n  namespace: kcm-system\nsubjects:\n  - kind: Group\n    name: \" ---&gt; Here specify the name of your Okta group &lt;---\"\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kcm-namespace-viewer-role\nEOF\n</code></pre> <p>This RoleBinding grants read-only access (as defined by the <code>kcm-namespace-viewer-role</code>) to users in the specified group, ensuring that only authorized users can view resources in the <code>kcm-system</code> namespace.</p>"},{"location":"admin/installation/auth/pinniped/#step-21-install-the-pinniped-cli-for-amd64","title":"Step 21. Install the Pinniped CLI (for AMD64)","text":"<p>Download and install the Pinniped CLI to interact with Pinniped resources more easily.</p> <pre><code>curl -Lso pinniped https://get.pinniped.dev/v0.37.0/pinniped-cli-linux-amd64 \\\n    &amp;&amp; chmod +x pinniped \\\n    &amp;&amp; sudo mv pinniped /usr/local/bin/pinniped\n</code></pre> <p>This command downloads the CLI binary, makes it executable, and moves it to a directory in your PATH for convenient access.</p>"},{"location":"admin/installation/auth/pinniped/#step-22-generate-kubeconfig-files-for-end-users","title":"Step 22. Generate Kubeconfig Files for End Users","text":"<p>Use the Pinniped CLI to generate a kubeconfig file that end users will use for authentication.</p> <pre><code>pinniped get kubeconfig &gt; /tmp/developer.yaml\n</code></pre> <p>If you have multiple identity providers configured, specify the desired one:</p> <pre><code>pinniped get kubeconfig --upstream-identity-provider-name okta &gt; /tmp/developer.yaml\n</code></pre> <p>The generated kubeconfig file contains the necessary configuration to authenticate with the cluster using Pinniped. When the command is run, it will open an authentication link in your browser. If a browser is not available, follow the command-line instructions to complete authentication.</p>"},{"location":"admin/installation/auth/pinniped/#step-23-authenticate-and-complete-the-flow","title":"Step 23. Authenticate and Complete the Flow","text":"<p>Upon running the kubeconfig generation command, your default browser should open to an authentication page. - Action: Log in using your identity provider credentials (e.g., Okta). - Outcome: Once authenticated, an authorization code is provided and processed, generating a kubeconfig file with the proper token.</p> <pre><code>Current cluster info:\nName: kind-kind-pinniped\nURL: https://127.0.0.1:38395\n\nCurrent user info:\nUsername: &lt;user@example.com&gt;\nGroups: Everyone, k8s-group, system:authenticated\n</code></pre> <p>This output confirms that the authentication flow is successful and that the RBAC permissions are applied as expected.</p>"},{"location":"admin/installation/auth/pinniped/#step-24-test-the-created-kubeconfig","title":"Step 24. Test the Created Kubeconfig","text":"<p>Verify that the new kubeconfig file works by checking access to various resources.</p> <pre><code>export KUBECONFIG=/tmp/developer.yaml\n\nkubectl auth can-i get secrets --namespace=kcm-system\nkubectl auth can-i create secrets --namespace=kcm-system\n</code></pre> <ul> <li><code>kubectl auth can-i get secrets</code> should return \"yes\".</li> <li><code>kubectl auth can-i create secrets</code> should return \"no\", reflecting the read-only access provided.</li> </ul> <p>These tests confirm that the RBAC configuration is working correctly and that the authenticated user only has the permissions granted by the RoleBinding.</p> <p>By following these detailed steps, you have set up a fully functional Pinniped-based federated authentication solution on your Mirantis k0rdent Enterprise management cluster. This configuration integrates TLS, Ingress, and your chosen identity provider to provide secure, centralized access management across multiple Kubernetes clusters. The Pinniped system also periodically checks and refreshes authentication tokens, ensuring that access permissions remain up to date.</p>"},{"location":"admin/installation/create-mgmt-clusters/","title":"Creating the management cluster","text":"<p>The type of cluster you create for as a Mirantis k0rdent Enterprise management cluster is going to depend on how you're going to use it. For example, a simple single-node k0s install is sufficient for testing and evaluation, but you will want a multi-node, etcd-backed cluster such as one created using k0sctl or EKS for production.</p> <p>Note</p> <p> For best results use Kubernetes version 1.32. or above.</p> <p>In a production environment, you will always want to ensure that your management cluster is backed up. There are a few caveats and things you need to take into account when backing up Mirantis k0rdent Enterprise. More info can be found in the guide at use Velero as a backup provider.</p> <ul> <li>Create a single node k0s cluster</li> <li>Create a multi-node k0s cluster</li> <li>Create a multinode EKS cluster</li> </ul>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-eks-multi/","title":"Create and prepare a production-grade Kubernetes cluster with EKS","text":"<p>Follow these steps to install and prepare an Amazon EKS management cluster:</p> <ol> <li> <p>The basic AWS tools</p> <p>Start by installing and configuring the Amazon tools. First download and install the <code>aws</code> tool:</p> <p><pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre>  Then configure it using environment variables:</p> <p><pre><code>export AWS_ACCESS_KEY_ID=\"EXAMPLE_ACCESS_KEY_ID\"\nexport AWS_SECRET_ACCESS_KEY=\"EXAMPLE_SECRET_ACCESS_KEY\"\nexport AWS_SESSION_TOKEN=\"EXAMPLE_SESSION_TOKEN\"\naws configure\n</code></pre> <pre><code>AWS Access Key ID [EXAMPLE_ACCESS_KEY_ID]:\nAWS Secret Access Key [EXAMPLE_SECRET_ACCESS_KEY]:\nDefault region name [YOUR_AWS_REGION]:\nDefault output format [json]:\n</code></pre>  Once <code>aws</code> is installed you can install <code>eksctl</code>:</p> <p><pre><code>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\neksctl version\n</code></pre> <pre><code>0.211.0\n</code></pre></p> </li> <li> <p>Create the cluster</p> <p>You can use the eksctl tool to create the basic Kubernetes cluster: <pre><code>eksctl create cluster --name &lt;CLUSTER_NAME&gt; \\\n--version 1.31 \\\n--region &lt;YOUR_AWS_REGION&gt; \\\n--without-nodegroup\n</code></pre> <pre><code>2025-03-05 22:26:02 [\u2139]  eksctl version 0.211.0\n...\n2025-03-05 22:36:14 [\u2714]  all EKS cluster resources for \"CLUSTER_NAME\" have been created\n2025-03-05 22:36:15 [\u2139]  kubectl command should work with \"/home/username/.kube/config\", try 'kubectl get nodes'\n2025-03-05 22:36:15 [\u2714]  EKS cluster \"CLUSTER_NAME\" in \"YOUR_AWS_REGION\" region is ready\n</code></pre></p> </li> <li> <p>Add controllers</p> <p>While the cluster is now created, it doesn't actually have any nodes.  Start by adding controllers: <pre><code>eksctl create nodegroup --cluster &lt;CLUSTER_NAME&gt; \\\n--name &lt;CONTROLLER_NODE_GROUP&gt; \\\n--node-type t3.medium \\\n--nodes 3 \\\n--nodes-min 3 \\\n--nodes-max 3 \\\n--node-labels \"role=control-plane\" \n</code></pre> <pre><code>2025-03-05 22:57:15 [\u2139]  will use version 1.31 for new nodegroup(s) based on control plane version\n2025-03-05 22:57:18 [\u2139]  nodegroup \"nickchasek0rdentcontroller-group\" will use \"\" [AmazonLinux2/1.31]\n2025-03-05 22:57:19 [\u2139]  1 nodegroup (nickchasek0rdentcontroller-group) was included (based on the include/exclude rules)\n2025-03-05 22:57:19 [\u2139]  will create a CloudFormation stack for each of 1 managed nodegroups in cluster \"NickChasek0rdentControlCluster\"\n...\n2025-03-05 23:00:27 [\u2139]  all nodegroups have up-to-date cloudformation templates\n</code></pre></p> </li> <li> <p>Install kubectl</p> <p>Everything you do in Mirantis k0rdent Enterprise is done by creating and manipulating Kubernetes objects, so you'll need to have <code>kubectl</code> installed. You can find the full install docs here, or just follow these instructions:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> <li> <p>Get the KUBECONFIG</p> <p>You don't have to access the KUBECONFIG directly; you can use the aws tool to get access to it:</p> <p><pre><code>aws eks update-kubeconfig --region &lt;YOUR_AWS_REGION&gt; --name &lt;CLUSTER_NAME&gt;\n</code></pre> <pre><code>Updated context arn:aws:eks:ca-central-1:026090528175:cluster/NickChasek0rdentControlCluster in /home/nick/.kube/config\n</code></pre></p> </li> <li> <p>Taint controllers</p> <p>To prevent workloads from being scheduled on the controllers, add the <code>node-role.kubernetes.io/control-plane=true:NoSchedule</code> taint. First list the started nodes:</p> <p><pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                                              STATUS   ROLES    AGE    VERSION\nnodename1.compute.internal   Ready    &lt;none&gt;   4h1m   v1.31.7-eks-5d632ec\nnodename2.compute.internal   Ready    &lt;none&gt;   4h1m   v1.31.7-eks-5d632ec\nnodename3.compute.internal   Ready    &lt;none&gt;   4h1m   v1.31.7-eks-5d632ec\n</code></pre> For each node, go ahead and set the taint:</p> <p><pre><code>kubectl taint nodes nodename1.compute.internal node-role.kubernetes.io/control-plane=true:NoSchedule\n</code></pre> <pre><code>node/nodename1.ca-central-1.compute.internal tainted\n</code></pre> Now verify the taints: <pre><code>kubectl describe nodes | grep -A 5 \"Taints:\"\n</code></pre> <pre><code>Taints:             node-role.kubernetes.io/control-plane=true:NoSchedule\nUnschedulable:      false\nLease:\nHolderIdentity:  nodename1.compute.internal\nAcquireTime:     &lt;unset&gt;\nRenewTime:       Fri, 25 Jul 2025 21:16:56 -0500\n--\nTaints:             node-role.kubernetes.io/control-plane=true:NoSchedule\nUnschedulable:      false\nLease:\nHolderIdentity:  nodename2.compute.internal\nAcquireTime:     &lt;unset&gt;\nRenewTime:       Fri, 25 Jul 2025 21:16:56 -0500\n--\nTaints:             node-role.kubernetes.io/control-plane=true:NoSchedule\nUnschedulable:      false\nLease:\nHolderIdentity:  nodename3.ca-central-1.compute.internal\nAcquireTime:     &lt;unset&gt;\nRenewTime:       Fri, 25 Jul 2025 21:16:56 -0500\n</code></pre></p> </li> <li> <p>Add worker nodes</p> <p>Adding worker nodes is simpler than controllers:</p> <p><pre><code>eksctl create nodegroup --cluster &lt;CLUSTER_NAME&gt; \\\n--name &lt;WORKER_NODE_GROUP&gt; \\\n--node-type t3.medium \\\n--nodes 3 \\\n--nodes-min 3 \\\n--nodes-max 5 \\\n--node-labels \"role=worker\"\n</code></pre> <pre><code>2025-03-06 03:10:48 [\u2139]  will use version 1.31 for new nodegroup(s) based on control plane version\n...\n2025-03-06 03:13:38 [\u2714]  created 1 managed nodegroup(s) in cluster \"CLUSTER_NAME\"\n2025-03-06 03:13:39 [\u2139]  checking security group configuration for all nodegroups\n2025-03-06 03:13:39 [\u2139]  all nodegroups have up-to-date cloudformation templates\n</code></pre> Verify the nodes: <pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                                              STATUS   ROLES    AGE     VERSION\nnodename1.compute.internal   Ready    &lt;none&gt;   4h14m   v1.31.7-eks-5d632ec\nnodename4.compute.internal   Ready    &lt;none&gt;   79s     v1.31.7-eks-5d632ec\nnodename2.compute.internal   Ready    &lt;none&gt;   4h14m   v1.31.7-eks-5d632ec\nnodename5.compute.internal   Ready    &lt;none&gt;   82s     v1.31.7-eks-5d632ec\nnodename3.compute.internal   Ready    &lt;none&gt;   4h14m   v1.31.7-eks-5d632ec\nnodename6.compute.internal   Ready    &lt;none&gt;   4h14m   v1.31.7-eks-5d632ec\n</code></pre></p> </li> <li> <p>Verify pods</p> <p>Make sure pods will run properly by deploying a test pod: <pre><code>kubectl run test-pod --image=nginx --restart=Never\nkubectl get pods -o wide\n</code></pre> <pre><code>NAME       READY   STATUS    RESTARTS   AGE   IP               NODE                                              NOMINATED NODE   READINESS GATES\ntest-pod   1/1     Running   0          15s   192.168.76.104   ip-192-168-68-189.ca-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\n</code></pre> Clean up so you can start fresh: <pre><code>kubectl delete pod test-pod\n</code></pre> <pre><code>pod \"test-pod\" deleted\n</code></pre> <pre><code>kubectl get pods -o wide\n</code></pre> <pre><code>No resources found in default namespace.\n</code></pre></p> </li> <li> <p>Install Helm</p> <p>Finally, the easiest way to install Mirantis k0rdent Enterprise is through its Helm chart, so let's get Helm installed. You can find the full instructions here, or use these instructions:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Helm will be installed into <code>/usr/local/bin/helm</code>.</p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/","title":"Create and prepare a multinode Kubernetes cluster with k0s","text":"<p>Many features of Mirantis k0rdent Enterprise require a Kubernetes cluster that uses etcd for its persistence layer. While this is the case for many implementations, the single-node k0s cluster uses SQLite instead. Fortunately, multi-node k0s uses etcd by default, so you can solve this problem by creating controller and worker nodes and linking them together. </p> <p>Follow these steps to install and prepare a multinode k0s kubernetes management cluster:</p>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/#create-the-first-controller","title":"Create the first controller","text":"<ol> <li> <p>Download k0s:</p> <p>Run the k0s download script to download the latest stable version of k0s and make it executable from <code>/usr/bin/k0s</code>:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\n</code></pre> </li> <li> <p>Create a configuration file:</p> <p>Note</p> <p> Depending on your configuration, you may need to execute these commands as root rather than using <code>sudo</code>. If so, executing <code>sudo su</code> will be sufficient. Don't forget to exit the root user afterwards!</p> <pre><code>mkdir -p /etc/k0s\nk0s config create &gt; /etc/k0s/k0s.yaml\n</code></pre> </li> <li> <p>If you need to access the cluster from outside the specific node on which you're installing it, add the public hostname as the <code>spec.api.externalAddress</code> and as an instance under <code>spec.api.sans</code>, as in:</p> <pre><code>...\nspec:\n    api:\n        address: 172.31.7.199\n        externalAddress: myhost.example.com\n        ca:\n        certificatesExpireAfter: 8760h0m0s\n        expiresAfter: 87600h0m0s\n        k0sApiPort: 9443\n        port: 6443\n        sans:\n        - 172.31.7.199\n        - fe80::c4:e6ff:fecc:9739\n        - myhost.example.com\n    controllerManager: {}\n    extensions:\n        helm:\n...\n</code></pre> <p>You can also make other configuration changes. For more information see the k0s documentation.</p> </li> <li> <p>Install the controller:</p> <pre><code>sudo k0s install controller -c /etc/k0s/k0s.yaml\nsudo k0s start\n</code></pre> <p>The k0s process acts as a \"supervisor\" for all of the control plane components. In moments the control plane will be up and running, but because of the k0s architecture, you won't see any nodes running, because only worker nodes show up, and we haven't created any yet. You can, however, use the k0s-installed <code>kubectl</code> to make sure the cluster is up and running by looking for existing namespaces:</p> <p><pre><code>sudo k0s kubectl get namespaces\n</code></pre> <pre><code>NAME              STATUS   AGE\ndefault           Active   5m15s\nk0s-autopilot     Active   5m11s\nkube-node-lease   Active   5m15s\nkube-public       Active   5m15s\nkube-system       Active   5m15s\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/#add-workers-to-the-cluster","title":"Add workers to the cluster","text":"<p>Once the contoller is up, you can add a worker node by creating a \"join token\" and using it to start k0s on a new server. Follow these instructions:</p> <ol> <li> <p>Create a join token:</p> <p>To join additional nodes to the cluster, you need a join token, which is a base64-encoded <code>KUBECONFIG</code>. The token embeds information that enables mutual trust between the worker and controller(s) and allows the node to join the cluster.</p> <p>To get a worker token, run the following command on newly created existing controller node:</p> <pre><code>sudo k0s token create --role=worker &gt; token-file\n</code></pre> <p>The resulting output is a long token string, which we'll use in a moment to add a worker to the cluster.</p> <p>For enhanced security, set an expiration time for the token:</p> <pre><code>sudo k0s token create --role=worker --expiry=100h &gt; token-file\n</code></pre> </li> <li> <p>Create the new node:</p> <p>Start by instantiating the new node and downloading k0s, as before:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\n</code></pre> </li> <li> <p>Copy the token file you created in step 1 to the new server.</p> </li> <li> <p>Start the new worker: </p> <p>Now you can start the new worker, feeding it the token file so that it automatically joins the existing cluster:</p> <p>Note</p> <p> Don't forget to use the actual path to your token file.</p> <pre><code>sudo k0s install worker --token-file /path/to/token/file/token-file\nsudo k0s start\n</code></pre> </li> <li> <p>Check the node status:</p> <p>Now you can check the status of the node by going to the controller and once again checking for nodes. After a minute or two, you'll see the new node in a <code>Ready</code> state:</p> <p><pre><code>sudo k0s kubectl get nodes\n</code></pre> <pre><code>NAME              STATUS     ROLES    AGE     VERSION\nip-172-31-9-107   Ready      &lt;none&gt;   3m39s   v1.33.1+k0s\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/#add-controllers-to-the-cluster","title":"Add controllers to the cluster","text":"<p>To create a join token for the new controller, follow these steps.</p> <ol> <li> <p>Copy the <code>k0s.yaml</code> file to the new controller server at <code>/etc/k0s/k0s.yaml</code>. Each controller in the cluster must have this <code>k0s.yaml</code> file, or some cluster nodes will use default config values, which will lead to inconsistent behavior. </p> <p>If your configuration file includes IP addresses (node address, sans, etcd peerAddress), remember to update them accordingly for this specific controller node.</p> </li> <li> <p>Run the following command on an existing controller:</p> <pre><code>sudo k0s token create --role=controller --expiry=1h &gt; controller-token-file\n</code></pre> <p>Copy the <code>controller-token-file</code> file to the new controller server.</p> </li> <li> <p>As before, download and install k0s in the new controller:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\n</code></pre> </li> <li> <p>On the new controller, run:</p> <pre><code>sudo k0s install controller --token-file /path/to/token/file -c /etc/k0s/k0s.yaml\nsudo k0s start\n</code></pre> </li> <li> <p>This time, check k0s status by running this command on the new controller:</p> <p><pre><code>sudo k0s status\n</code></pre> <pre><code>Version: v1.33.1+k0s.1\nProcess ID: 1489\nRole: controller\nWorkloads: false\nSingleNode: false\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-multi/#access-your-cluster","title":"Access your cluster","text":"<p>Use the Kubernetes 'kubectl' command-line tool that comes with k0s binary to deploy your application or check your node status:</p> <p><pre><code>sudo k0s kubectl get nodes\n</code></pre> <pre><code>NAME   STATUS   ROLES    AGE    VERSION\nk0s    Ready    &lt;none&gt;   4m6s   v1.33.1+k0s\n</code></pre></p> <p>You can also install <code>kubectl</code> directly. You can find the full install docs here, or just follow these instructions:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> <p>To use <code>kubectl</code> directly or to access the cluster from another machine, copy the <code>KUBECONFIG</code>, which is located on the controllers at:</p> <pre><code>/var/lib/k0s/pki/admin.conf\n</code></pre> <p>to your target machine.  Note that to access the cluster from an external machine you must replace <code>localhost</code> in the <code>KUBECONFIG</code> with the host IP address or hostname for your controller. Make sure to use the address you added to the <code>sans</code> field, and also that port <code>6443</code> is accessible.</p>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-single/","title":"Create and prepare a Kubernetes cluster with k0s","text":"<p>Follow these steps to install and prepare a k0s kubernetes management cluster:</p> <ol> <li> <p>Deploy a Kubernetes cluster</p> <p>The first step is to create the actual cluster itself. Again, the actual distribution used for the management cluster isn't important, as long as it's a CNCF-compliant distribution. That means you can use an existing EKS cluster, or whatever is your normal corporate standard. </p> <p>To make things simple this guide uses k0s, a small, convenient, and fully-functional distribution. For more granular instructions, including those for creating a cluster accessible from a different server, see the k0s multi-node instructions:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --enable-worker --no-taints\nsudo k0s start\n</code></pre> <p>k0s includes its own preconfigured version of <code>kubectl</code> so make sure the cluster is running:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>After 2-3 minutes you should see a single <code>control-plane</code> node with a status of <code>Ready</code>, as in:</p> <pre><code>NAME              STATUS   ROLES            AGE   VERSION\nip-172-31-29-61   Ready    control-plane    46s   v1.31.2+k0s\n</code></pre> </li> <li> <p>Install kubectl</p> <p>Everything you do in Mirantis k0rdent Enterprise is done by creating and manipulating Kubernetes objects, so you'll need to have <code>kubectl</code> installed. You can find the full install docs here, or just follow these instructions:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> <li> <p>Get the kubeconfig</p> <p>In order to access the management cluster you will, of course, need the kubeconfig. Again, if you're using another Kubernetes distribution follow those instructions to get the kubeconfig, but for k0s, the process involves simply copying the existing file and adding it to an environment variable so <code>kubectl</code> knows where to find it.</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>Now you should be able to use the non-k0s <code>kubectl</code> to see the status of the cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Again, you should see the single k0s node, but by this time it should have had its role assigned, as in:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre> <p>Now the cluster is ready for installation, which we'll do using Helm.</p> </li> <li> <p>Install Helm</p> <p>The easiest way to install Mirantis k0rdent Enterprise is through its Helm chart, so let's get Helm installed. You can find the full instructions here, or use these instructions:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Helm will be installed into <code>/usr/local/bin/helm</code>.</p> </li> </ol>"},{"location":"admin/installation/create-mgmt-clusters/mgmt-create-k0s-single/#access-your-cluster-from-another-machine","title":"Access your cluster from another machine","text":"<p>To use a tool like Lens or to access the cluster from another machine, copy the <code>KUBECONFIG</code>, which is located at:</p> <pre><code>/var/lib/k0s/pki/admin.conf\n</code></pre> <p>to your target machine.  Note that to access the cluster from an external machine you must replace <code>localhost</code> in the <code>KUBECONFIG</code> with the host IP address or hostname for your controller. Make sure to use the address you added to the <code>sans</code> field, and also that port <code>6443</code> is accessible.</p>"},{"location":"admin/installation/prepare-mgmt-cluster/","title":"Prepare Mirantis k0rdent Enterprise to create child clusters on multiple providers","text":"<p>Managed clusters can be hosted on a number of different platforms. At the time of this writing, those platforms include:</p> <ul> <li>AWS</li> <li>Azure</li> <li>Bare Metal</li> <li>OpenStack</li> <li>VMware</li> </ul>"},{"location":"admin/installation/prepare-mgmt-cluster/aws/","title":"AWS","text":"<p>Mirantis k0rdent Enterprise can deploy managed clusters as both EC2-based Kubernetes clusters and EKS clusters. In both cases, you'll need to create the relevant credentials, and to do that you'll need to configure an IAM user. Follow these steps to make it possible to deploy to AWS:</p> <ol> <li> <p>Install Mirantis k0rdent Enterprise</p> <p>Follow the instructions in Install Mirantis k0rdent Enterprise to create a management cluster with Mirantis k0rdent Enterprise running.</p> </li> <li> <p>Install <code>clusterawsadm</code></p> <p>Mirantis k0rdent Enterprise uses the Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. <code>clusterawsadm</code>, a CLI tool created by CAPA project, helps with AWS-specific tasks such as creating IAM roles and policies, as well as credential configuration. To install clusterawsadm on Ubuntu on x86 hardware, execute these commands:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre> </li> <li> <p>Configure AWS IAM</p> <p>Next you'll need to create the IAM policies and service account Mirantis k0rdent Enterprise will use to take action within the AWS infrastructure. (Note that you only need to do this once.)</p> <p>The first step is to create the IAM CloudFormation stack based on your admin user. Start by specifying the environment variables <code>clusterawsadm</code> will use as AWS credentials:</p> <pre><code>export AWS_REGION=&lt;EXAMPLE_AWS_REGION&gt;\nexport AWS_ACCESS_KEY_ID=&lt;EXAMPLE_ACCESS_KEY_ID&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;EXAMPLE_SECRET_ACCESS_KEY&gt;\nexport AWS_SESSION_TOKEN=&lt;YOUR_SESSION_TOKEN&gt; # Optional. If you are using Multi-Factor Auth.\n</code></pre> </li> <li> <p>Create the IAM CloudFormation stack</p> <p>Now use <code>clusterawsadm</code> to create the IAM CloudFormation stack:</p> <p><code>shell   clusterawsadm bootstrap iam create-cloudformation-stack</code></p> </li> <li> <p>Install the AWS CLI</p> <p>With the stack in place you can create the AWS IAM user. You can do this in the UI, but it's also possible to do it from the command line using the <code>aws</code> CLI tool.  Start by installing it, if you haven't already:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre> <p>The tool recognizes the environment variables you created earlier, so there's no need to login.</p> </li> <li> <p>Check for available IPs</p> <p>Because Mirantis k0rdent Enterprise has 3 availablilty zone NAT gateways, each cluster needs 3 public IPs. Unfortunately, the default <code>EC2-VPC Elastic IPs</code> quota per region is 5, so while you likely won't have issues with a first cluster, if you try to deplay a  second to the same region, you are likely to run into issues.  </p> <p>You can determine how many elastic IPs are available from the command line:</p> <p><pre><code>LIMIT=$(aws ec2 describe-account-attributes --attribute-names vpc-max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)\nUSED=$(aws ec2 describe-addresses --query 'Addresses[*].PublicIp' --output text | wc -w)\nAVAILABLE=$((LIMIT - USED))\necho \"Available Public IPs: $AVAILABLE\"\n</code></pre> <pre><code>Available Public IPs: 5\n</code></pre></p> <p>If you have less than 3 available public IPs, you can request an increase in your quota:</p> <pre><code>aws service-quotas request-service-quota-increase \\\n    --service-code ec2 \\\n    --quota-code L-0263D0A3 \\\n    --desired-value 20\n</code></pre> <p>You can check on the status of your request:</p> <p><pre><code>aws service-quotas list-requested-service-quota-change-history \\\n    --service-code ec2\n</code></pre> <pre><code>{\n    \"RequestedQuotas\": [\n        {\n            \"Id\": \"EXAMPLE_ACCESS_KEY_ID\",\n            \"ServiceCode\": \"ec2\",\n            \"ServiceName\": \"Amazon Elastic Compute Cloud (Amazon EC2)\",\n            \"QuotaCode\": \"L-0263D0A3\",\n            \"QuotaName\": \"EC2-VPC Elastic IPs\",\n            \"DesiredValue\": 20.0,\n            \"Status\": \"PENDING\",\n            \"Created\": \"2025-02-09T02:27:01.573000-05:00\",\n            \"LastUpdated\": \"2025-02-09T02:27:01.956000-05:00\",\n            \"Requester\": \"{\\\"accountId\\\":\\\"EXAMPLE_ACCESS_KEY_ID\\\",\\\"callerArn\\\":\\\"arn:aws:iam::EXAMPLE_ACCESS_KEY_ID:user/nchase\\\"}\",\n            \"QuotaArn\": \"arn:aws:servicequotas:EXAMPLE_AWS_REGION:EXAMPLE_ACCESS_KEY_ID:ec2/L-0263D0A3\",\n            \"GlobalQuota\": false,\n            \"Unit\": \"None\",\n            \"QuotaRequestedAtLevel\": \"ACCOUNT\"\n        }\n    ]\n}\n</code></pre></p> </li> <li> <p>Create the IAM user. </p> <p>The actual <code>user-name</code> parameter is arbitrary; you can specify it as anything you like:</p> <p><pre><code>aws iam create-user --user-name k0rdentUser\n</code></pre> <pre><code>{\n  \"User\": {\n    \"Path\": \"/\",\n    \"UserName\": \"k0rdentUser\",\n    \"UserId\": \"EXAMPLE_USER_ID\",\n    \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentUser\",\n    \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Assign the relevant policies</p> <p>You'll need to assign the following policies to the user you just created:</p> <p><pre><code>control-plane.cluster-api-provider-aws.sigs.k8s.io\ncontrollers.cluster-api-provider-aws.sigs.k8s.io\nnodes.cluster-api-provider-aws.sigs.k8s.io\ncontrollers-eks.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> To do that, you'll need the ARNs for each policy.  You can get them with the <code>list-policies</code> command, as in:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> <pre><code>{\n  \"Policies\": [\n    {\n      \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 2,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n    },\n    {\n      \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 3,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n    },\n    {\n      \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 3,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n    },\n    {\n      \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n      \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n      \"Path\": \"/\",\n      \"DefaultVersionId\": \"v1\",\n      \"AttachmentCount\": 2,\n      \"PermissionsBoundaryUsageCount\": 0,\n      \"IsAttachable\": true,\n      \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n      \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n    }\n  ]\n}\n</code></pre></p> <p>Now you can add the policies using the <code>attach-user-policy</code> command and the ARNs you retrieved in the previous step:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentUser --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> </li> <li> <p>Create an access key and secret</p> <p>To access AWS as this new user, you'll need to create an access key:</p> <p><pre><code>aws iam create-access-key --user-name k0rdentUser \n</code></pre> <pre><code>{\n  \"AccessKey\": {\n    \"UserName\": \"k0rdentUser\",\n    \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n    \"Status\": \"Active\",\n    \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n    \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Create the IAM Credentials <code>Secret</code> on the Mirantis k0rdent Enterprise Management Cluster</p> <p>Create a YAML file called <code>aws-cluster-identity-secret.yaml</code> and add the following text, including the <code>AccessKeyId</code> and <code>SecretAccessKey</code> you created in the previous step:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nstringData:\n  AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n  SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Apply the YAML to your cluster, making sure to add it to the namespace where the CAPA provider is running (currently <code>kcm-system</code>) so the controller can read it:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>AWSClusterStaticIdentity</code></p> <p>Create the <code>AWSClusterStaticIdentity</code> object in a file named <code>aws-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Notice that the <code>secretRef</code> references the <code>Secret</code> you created in the previous step.</p> <p>Apply the YAML to your cluster, again adding it to the <code>kcm-system</code> namespace.</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre> </li> <li> <p>Create the Cluster Identity resource template <code>ConfigMap</code></p> <p>Now we create Cluster Identity resource template <code>ConfigMap</code>. As in prior steps, create a YAML file called <code>aws-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-cluster-identity-resource-template\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\n</code></pre> <p>Note that <code>ConfigMap</code> is empty, this is expected, we don't need to template any object inside child cluster(s), but we can use that object in the future if need arises.</p> <p>Apply the YAML to your cluster, again keeping it in the <code>kcm-system</code> namespace:</p> <pre><code>kubectl apply -f aws-cluster-identity-resource-template.yaml -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>Credential</code></p> <p>Finally, create the KCM <code>Credential</code> object, making sure to reference the <code>AWSClusterStaticIdentity</code> you just created:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> Apply the YAML to your cluster, again keeping it in the <code>kcm-system</code> namespace:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre> </li> <li> <p>Deploy a cluster</p> <p>Make sure everything is configured properly by creating a <code>ClusterDeployment</code>. Start with a YAML file specifying the <code>ClusterDeployment</code>, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-12\n  credential: aws-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n      rootVolumeSize: 32          \n    worker:\n      instanceType: t3.small\n      rootVolumeSize: 32          \n</code></pre> </p> <p>Note</p> <ul> <li>You're giving it an arbitrary name in <code>.metadata.name</code> (<code>my-aws-clusterdeployment1</code>)</li> <li>You're referencing the credential you created in the previous step, <code>aws-cluster-identity-cred</code>. This enables you to set up a system where users can take advantage of having access to the credentials to the AWS account without actually having those credentials in hand.</li> <li>You need to choose a template to use for the cluster, in this case <code>aws-standalone-cp-1-0-12</code>. You can get a list of available templates using:</li> </ul> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-1-0           true\naws-eks-1-0-2                   true\naws-hosted-cp-1-0-11             true\naws-standalone-cp-1-0-12         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-13           true\nazure-standalone-cp-1-0-13       true\nopenstack-standalone-cp-1-0-11   true\nvsphere-hosted-cp-1-0-10         true\nvsphere-standalone-cp-1-0-11     true\n</code></pre> Apply the YAML to your management cluster: <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> As before, there will be a delay as the cluster finishes provisioning. Follow the provisioning process with: <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> When the cluster is <code>Ready</code>, you can access it via the kubeconfig, as in: <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre></p> </li> <li> <p>Cleanup</p> <p>When you've established that it's working properly, you can delete the managed cluster and its AWS objects:</p> <pre><code>kubectl delete clusterdeployments my-aws-clusterdeployment1 \n</code></pre> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/azure/","title":"Azure","text":"<p>Standalone clusters can be deployed on Azure instances. Follow these steps to make Azure clusters available to your users:</p> <ol> <li> <p>Install Mirantis k0rdent Enterprise</p> <p>Follow the instructions in Install Mirantis k0rdent Enterprise to create a management cluster with Mirantis k0rdent Enterprise running.</p> </li> <li> <p>The Azure CLI</p> <p>The Azure CLI (<code>az</code>) is required to interact with Azure resources. You can install it on Ubuntu as follows:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre> </li> <li> <p>Log in to Azure</p> <p>Run the <code>az login</code> command to authenticate your session with Azure:</p> <pre><code>az login\n</code></pre> <p>Make sure that the account you're using has at least one active subscription.</p> </li> <li> <p>Register resource providers</p> <p>In order for Mirantis k0rdent Enterprise to deploy and manage clusters, it needs to be able to work with Azure resources such as  compute, network, and identity. Make sure the subscription you're using has the following resource providers registered:</p> <pre><code>Microsoft.Compute\nMicrosoft.Network\nMicrosoft.ContainerService\nMicrosoft.ManagedIdentity\nMicrosoft.Authorization\n</code></pre> <p>To register these providers, run the following commands in the Azure CLI:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre> </li> <li> <p>Find Your Subscription ID</p> <p>Creating a child cluster requires a structure of credentials that link to user identities on the provider system without exposing the actual username and password to users. You can find more information on Mirantis k0rdent Enterprise  Credentials, but for Azure, this involves creating an <code>AzureClusterIdentity</code> and a  Service Principal (SP) to let CAPZ (Cluster API Azure) communicate with the cloud provider. </p> <p>On Azure, the lowest level of this hierarchy is the subscription, which ties to your billing information for Azure. Your Azure user must have at least one subscription for you to use it with Mirantis k0rdent Enterprise, so if you're working with a new account make sure to create a new subscription with billing information before you start.</p> <p>To get the information you need, list all your Azure subscriptions: </p> <p><pre><code>az account list -o table\n</code></pre> <pre><code>Name                     SubscriptionId                        TenantId\n-----------------------  -------------------------------------  --------------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID        TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre></p> <p>Make note of the <code>SubscriptionId</code> for the subscription you want to use.</p> </li> <li> <p>Create a Service Principal (SP)</p> <p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. Create the Service Principal, making sure to replace  with the <code>SubscriptionId</code> from step 1. <p><pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt;\"\n</code></pre> <pre><code>{\n\"appId\": \"SP_APP_ID_SP_APP_ID\",\n\"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n\"password\": \"SP_PASSWORD_SP_PASSWORD\",\n\"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> Note that this information provides access to your Azure account, so make sure to treat these strings  like passwords. Do not share them or check them into a repository.</p> <li> <p>Use the password to create a <code>Secret</code> object</p> <p>The <code>Secret</code> stores the <code>clientSecret</code> (password) from the Service Principal. Save the <code>Secret</code> YAML in a file called <code>azure-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clientSecret: &lt;SP_PASSWORD_SP_PASSWORD&gt; # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>You can then apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>AzureClusterIdentity</code> objects</p> <p>The <code>AzureClusterIdentity</code> object defines the credentials CAPZ uses to manage Azure resources.  It references the <code>Secret</code> you just created, so make sure that <code>.spec.clientSecret.name</code> matches  the name of that <code>Secret</code>.</p> <p>Save the following YAML into a file named <code>azure-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n    k0rdent.mirantis.com/component: \"kcm\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: &lt;SP_APP_ID_SP_APP_ID&gt; # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: &lt;SP_TENANT_SP_TENANT&gt; # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre></p> </li> <li> <p>Create the Mirantis k0rdent Enterprise <code>Credential</code> Object</p> <p>Create the YAML for the specification of the <code>Credential</code> and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>You're referencing the <code>AzureClusterIdentity</code> object you just created, so make sure that <code>.spec.name</code> matches  <code>.metadata.name</code> of that object. Also, note that while the overall object's <code>kind</code> is <code>Credential</code>, the  <code>.spec.identityRef.kind</code> must be <code>AzureClusterIdentity</code> to match that object.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre></p> </li> <li> <p>Create the <code>ConfigMap</code> resource-template Object</p> <p>Create a YAML with the specification of our resource-template and save it as <code>azure-cluster-identity-resource-template.yaml</code></p> <p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: azure-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    {{- $subnetName := \"\" -}}\n    {{- $securityGroupName := \"\" -}}\n    {{- $routeTableName := \"\" -}}\n    {{- range $cluster.spec.networkSpec.subnets -}}\n      {{- if eq .role \"node\" -}}\n        {{- $subnetName = .name -}}\n        {{- $securityGroupName = .securityGroup.name -}}\n        {{- $routeTableName = .routeTable.name -}}\n        {{- break -}}\n      {{- end -}}\n    {{- end -}}\n    {{- $cloudConfig := dict\n      \"aadClientId\" $identity.spec.clientID\n      \"aadClientSecret\" (index $secret.data \"clientSecret\" | b64dec)\n      \"cloud\" $cluster.spec.azureEnvironment\n      \"loadBalancerName\" \"\"\n      \"loadBalancerSku\" \"Standard\"\n      \"location\" $cluster.spec.location\n      \"maximumLoadBalancerRuleCount\" 250\n      \"resourceGroup\" $cluster.spec.resourceGroup\n      \"routeTableName\" $routeTableName\n      \"securityGroupName\" $securityGroupName\n      \"securityGroupResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n      \"subnetName\" $subnetName\n      \"subscriptionId\" $cluster.spec.subscriptionID\n      \"tenantId\" $identity.spec.tenantID\n      \"useInstanceMetadata\" true\n      \"useManagedIdentityExtension\" false\n      \"vmType\" \"vmss\"\n      \"vnetName\" $cluster.spec.networkSpec.vnet.name\n      \"vnetResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n    -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: azure-cloud-provider\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-config: {{ $cloudConfig | toJson | b64enc }}\n</code></pre> Object name needs to be exactly <code>azure-cluster-identity-resource-template.yaml</code>, <code>AzureClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-resource-template.yaml\n</code></pre> <pre><code>configmap/azure-cluster-identity-resource-template created\n</code></pre></p> </li> <p>Now you're ready to deploy the cluster.</p> <ol> <li> <p>Create a <code>ClusterDeployment</code></p> <p>To test the configuration, deploy a child cluster by following these steps:</p> <p>First get a list of available locations/regions:</p> <p><pre><code>az account list-locations -o table\n</code></pre> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n. . .\n</code></pre></p> <p>Make note of the location you want to use, such as <code>eastus</code>.</p> <p>To create the actual child cluster, create a <code>ClusterDeployment</code> that references the appropriate template as well as the location, credentials, and <code>subscriptionId</code>.</p> <p>You can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-1-0           true\naws-eks-1-0-2                   true\naws-hosted-cp-1-0-11             true\naws-standalone-cp-1-0-12         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-13           true\nazure-standalone-cp-1-0-13       true\nopenstack-standalone-cp-1-0-11   true\nvsphere-hosted-cp-1-0-10         true\nvsphere-standalone-cp-1-0-11     true\n</code></pre></p> <p>Create the yaml:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-1-0-13\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    subscriptionID: &lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt; # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre></p> <p>Note that although the <code>ClusterDeployment</code> object has been created, there will be a delay as actual Azure instances are provisioned and added to the cluster. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre> <p>If the provisioning process continues for a more than a few minutes, check to make sure Mirantis k0rdent Enterprise isn't trying to exceed your quotas. If you are near the top of your quotas, requesting an increase can \"unstick\" the provisioning process.</p> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up Azure resources, delete the child cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete clusterdeployments my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/","title":"Bare Metal","text":"<p>Mirantis k0rdent Enterprise can deploy managed clusters on bare metal servers using the Metal3 infrastructure provider. This implementation is based on the bare metal Cluster API Provider, Metal3 CAPM3, and provides out-of-tree (OOT) bare metal provisioning capabilities. </p> <p>CAPM3 works by enabling you to add a representation of each bare metal server as a Kubernetes object. Mirantis k0rdent Enterprise can then assemble these machine objects into a cluster.</p>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#structure","title":"Structure","text":"<p>The bare metal infrastructure provider is represented as a set of Helm charts. It includes the following charts:</p> <ul> <li><code>baremetal-operator</code> installs the Bare Metal Operator</li> <li><code>capm3-crds</code> installs the <code>CustomResourceDefinition</code> objects for the Metal3 CAPM3 and IPAM components</li> <li><code>cluster-api-provider-metal3</code> installs the Metal3 CAPM3 provider and Metal3 IP Address Manager</li> <li><code>ironic</code> installs OpenStack Ironic and accompanying components needed for management of bare metal machines:       MariaDB, keepalived, HTTP server, DHCP server, TFTP server, NTP server, dynamic-IPXE controller, resource controller. </li> </ul>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#prerequisites","title":"Prerequisites","text":"<ul> <li>An installed Mirantis k0rdent Enterprise management cluster. Follow the instructions in Install Mirantis k0rdent Enterprise to create a management cluster with Mirantis k0rdent Enterprise running. Prepare this cluster according to the Metal3 host configuration guide.</li> <li>Supported hardware as documented in the Metal3 hardware compatibility guide</li> <li>You should still have Helm installed from your installation of Mirantis k0rdent Enterprise. If not, install it again.</li> </ul>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#prepare-mirantis-k0rdent-enterprise-for-bare-metal-clusters","title":"Prepare Mirantis k0rdent Enterprise for Bare Metal clusters","text":"<p>Follow these instructions to make Mirantis k0rdent Enterprise capable of deploying bare metal clusters:</p> <ol> <li> <p>Create the required objects for the OOT CAPM3 provider</p> <p>Create the necessary Kubernetes objects to install the out-of-tree CAPM3 provider. Just as with other providers, these include a <code>HelmRepository</code>, <code>ProviderTemplate</code>, and <code>ClusterTemplate</code>.</p> <pre><code>kubectl create -f - &lt;&lt;EOF\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: oot-capm3-repo\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\nspec:\n  type: oci\n  url: 'oci://registry.mirantis.com/k0rdent-bm/charts/'\n  interval: 10m0s\n---\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ProviderTemplate\nmetadata:\n  name: cluster-api-provider-metal3-0-2-0\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  helm:\n    chartSpec:\n      chart: cluster-api-provider-metal3\n      version: 0.2.0\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: oot-capm3-repo\n---\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\nmetadata:\n  annotations:\n    helm.sh/resource-policy: keep\n  labels:\n    k0rdent.mirantis.com/component: kcm\n  name: capm3-standalone-cp-0-2-0\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: capm3-standalone-cp\n      version: 0.2.0\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: oot-capm3-repo\nEOF\n</code></pre> </li> <li> <p>Verify the <code>ProviderTemplate</code> is valid</p> <p>Check that the <code>ProviderTemplate</code> has been created successfully:</p> <p><pre><code>kubectl get providertemplates cluster-api-provider-metal3-0-2-0\n</code></pre> <pre><code>NAME                              VALID\ncluster-api-provider-metal3-0-2-0 true\n</code></pre></p> </li> <li> <p>Configure the <code>Management</code> object</p> <p>Edit the <code>Management</code> object to add the CAPM3 provider configuration:</p> <pre><code>kubectl edit managements.k0rdent.mirantis.com\n</code></pre> <p>Add the following configuration to the <code>providers</code> section:</p> <pre><code>- name: cluster-api-provider-metal3\n  template: cluster-api-provider-metal3-0-2-0\n  config:\n    global:\n      ironic:\n        enabled: true # networking configuration (\"ironic.networking\" section) should be defined prior to enabling ironic\n    ironic:\n      networking:\n        dhcp: # used by DHCP server to assign IPs to hosts during PXE boot\n          rangeBegin: &lt;DHCP_RANGE_START&gt;      # e.g., 10.0.1.51\n          rangeEnd: &lt;DHCP_RANGE_END&gt;          # e.g., 10.0.1.55\n          netmask: &lt;DHCP_SUBNET_MASK&gt;         # e.g., 255.255.255.192 (default is 255.255.255.0)\n          options:                            # DHCP options, used during PXE boot and by IPA\n            - \"option:router,&lt;ROUTER_IP&gt;\"     # e.g., 10.0.1.1. It's a mandatory option. \n            - \"option:dns-server,&lt;DNS_IP[,DNS2_IP...]&gt;\" # can be set to KEEPALIVED_VIP (dnsmasq can serve as a DNS server with user-defined DNS records) or to IP of your preferred server. Optional.\n            - \"option:ntp-server,&lt;NTP_IP&gt;\"    # can be set to KEEPALIVED_VIP (internal ntp server) or to IP of your preferred server. That ntp server will be used on PXE boot stage then. Optional.\n        interface: &lt;PROVISION_INTERFACE&gt;      # e.g., bond0 - interface of the management cluster node connected to BM hosts provision network\n        ipAddress: &lt;KEEPALIVED_VIP&gt;          # e.g., 10.0.1.50 - keepalived VIP for DHCP server and Ironic services. This VIP will be configured on the &lt;PROVISION_INTERFACE&gt;, it must be in the same L3 network as DHCP range if no dhcp-relay used between management cluster and child cluster hosts.\n    # By default, \"ubuntu-noble-hwe-2025-05-15-15-22-56.qcow2\" is the only image available.\n    # You can define custom OS images here if needed by adding new resources:\n    # resources:\n    #   images_target:\n    #     - name: ubuntu-noble-hwe-2025-05-15-15-22-56.qcow2\n    #       url: https://get.mirantis.com/k0rdent-bm/targetimages/ubuntu-noble-hwe-2025-05-15-15-22-56.qcow2\n    #       checksum: 581a672e494fcda3297cc8917a91d827157ddcfa3997ad552a914f207b3603c3\n</code></pre> </li> <li> <p>Wait for the <code>Management</code> object to be ready</p> <p>Monitor the <code>Management</code> object status:</p> <pre><code>kubectl get managements.k0rdent.mirantis.com -w\n</code></pre> <p>This process usually takes up to 5 minutes. If the <code>Management</code> object doesn't become <code>Ready</code>, refer to the Troubleshooting section.</p> </li> <li> <p>Verify the <code>ClusterTemplate</code> is valid</p> <p>Check that the <code>ClusterTemplate</code> has been created successfully:</p> <p><pre><code>kubectl -n kcm-system get clustertemplates capm3-standalone-cp-0-2-0\n</code></pre> <pre><code>NAME                        VALID\ncapm3-standalone-cp-0.2.0   true\n</code></pre></p> </li> <li> <p>Optional. Tune the DHCP server.</p> </li> </ol> <p>Note</p> <p> Modification of this configuration should be done with special care. It's not recommended to change it during provisioning/deprovisioning of bare metal machines.</p> <p>After CAPM3 provider is deployed, you can reconfigure DHCP server.</p> <ol> <li> <p>Using <code>dnsmasq</code> object, you can change configuration of the DHCP server and monitor DHCP leases related to your bare metal machines.</p> <p>Note</p> <p> Modification of this configuration requires good knowledge of DHCP basics.</p> <pre><code>kubectl -n kcm-system edit dnsmasq\n</code></pre> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#enroll-bare-metal-machines","title":"Enroll bare metal machines","text":"<p>The next step is to create <code>BareMetalHost</code> objects to represent your bare metal machines so Mirantis k0rdent Enterprise can manage them. For each bare metal machine, create two objects: a <code>Secret</code> and a <code>BareMetalHost</code>. For detailed instructions, see the Metal3 BareMetalHost enrollment guide (just <code>Enrolling</code>, not <code>Provisioning</code>), or follow these instructions.</p> <p>Note</p> <p>You don't need to provision bare metal hosts at this stage. Provisioning should happen later as part of a cluster deployment.</p> <ol> <li> <p>Create credential <code>Secret</code> objects</p> <p>You need to provide BMC credentials for every <code>BareMetalHost</code> using <code>Secret</code> objects. For example:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;BMH_NAME&gt;-bmc-secret\n  namespace: &lt;NAMESPACE&gt;\ntype: Opaque\ndata:\n  username: &lt;BASE64_ENCODED_BMC_USERNAME&gt;\n  password: &lt;BASE64_ENCODED_BMC_PASSWORD&gt;\n</code></pre> </li> </ol> <p>Note</p> <p> <code>namespace</code> of all the objects used to describe bare metal machines and corresponding cluster must be equal to the <code>namespace</code> of the <code>ClusterTemplate</code> object used for deployment of that cluster.</p> <ol> <li> <p>Create <code>BareMetalHost</code> objects</p> <p>A <code>BareMetalHost</code> object represents the physical machine. It contains a reference to the <code>Secret</code> created above. For example:</p> <pre><code>apiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: &lt;BMH_NAME&gt;\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  online: true\n  bmc:\n    address: &lt;BMC_ADDRESS&gt;  # e.g., ipmi://192.168.1.100:623\n    credentialsName: &lt;BMH_NAME&gt;-bmc-secret\n    #disableCertificateVerification: true # only needed when using redfish protocol\n  bootMACAddress: &lt;MAC_ADDRESS&gt; # MAC address that is used for booting. It\u2019s a MAC address of an actual NIC of the host, not the BMC MAC address.\n  #bootMode: legacy # UEFI or legacy BIOS. UEFI is the default and should be used unless there are serious reasons not to.\n</code></pre> </li> <li> <p>Wait for <code>BareMetalHost</code> objects to complete enrollment</p> <p>Monitor your <code>BareMetalHost</code> objects until they are <code>available</code>:</p> <p><pre><code>kubectl get bmh -n &lt;NAMESPACE&gt;\n</code></pre> <pre><code>NAME      STATE       CONSUMER   ONLINE   ERROR   AGE\nchild-1   available              true             4d17h\nchild-2   available              true             4d17h\nchild-3   available              true             4d17h\n</code></pre></p> <p>For more information about <code>BareMetalHost</code> states, see the Metal3 state machine documentation.</p> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#support-for-raid-disk-arrays-on-baremetal-hosts","title":"Support for RAID disk arrays on baremetal hosts","text":"<p>The Ironic python agent (IPA) provides limited functionality for software RAID arrays. These limits do not allow  creating a RAID disk as <code>rootfs</code>. One limit is the requirement to use a whole block device as a RAID array part. During RAID initialization, IPA creates a single partition on target disks and uses it as part of the RAID array. For example, the RAID array will be assembled not from the raw block device, but from the full disk partition on this block device.</p> <p>The disk layout looks something like this:</p> <pre><code>disk0 -&gt; part0 -,\ndisk1 -&gt; part0 -+-&gt; RAID -&gt; [ p0, p1, p2 ... ]\n</code></pre> <p>So UEFI/BIOS has access only to the \"root\" partition, and it cannot access the EFI/boot partition defined in the \"nested\" partition table--that is, the partition table created inside <code>part0</code> (inside the RAID array). So if the RAID array is defined as the root device, the system will be unbootable.</p> <p>Consider this example of a <code>BareMetalHost</code> object that declares the RAID1 array:</p> <pre><code>---\napiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: child-master-1\nspec:\n  bmc:\n    address: ipmi://10.0.1.1:6234\n    credentialsName: child-master-1.ipmi-auth\n  bootMACAddress: 52:54:1f:8b:19:15\n  # bootMode: legacy\n  online: true\n  raid:\n    softwareRAIDVolumes:\n      - level: \"1\"\n        physicalDisks:\n          - deviceName: /dev/disk/by-path/pci-0000:00:07.0-scsi-0:0:0:1\n          - deviceName: /dev/disk/by-path/pci-0000:00:07.0-scsi-0:0:0:2\n  rootDeviceHints:\n    deviceName: /dev/disk/by-path/pci-0000:00:07.0-scsi-0:0:0:0\n</code></pre> <p>The host has three hard disks. The first one will be used as the root device, and the second and third disks will be assembled into the  RAID1 array. The <code>rootDeviceHint</code> in the <code>BareMetalHost</code> <code>spec</code> must be defined, because if it has a RAID definition and doesn't have the  <code>rootDeviceHint</code>, the first RAID array will be marked as the root device automatically.</p> <p>For more information about software RAID support in IPA, see the Ironic documentation.</p>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#create-the-cluster","title":"Create the cluster","text":"<p>You need to create several objects before Mirantis k0rdent Enterprise can create a bare metal cluster.</p> <ol> <li> <p>Create the credential objects</p> <p>Since CAPM3 doesn't require cloud credentials, create dummy <code>Secret</code> and <code>Credential</code> objects to satisfy <code>ClusterDeployment</code> requirements:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: capm3-cluster-secret\n  namespace: &lt;NAMESPACE&gt;\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\n---\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: capm3-stub-credential\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  description: CAPM3 Credentials\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: capm3-cluster-secret\n    namespace: &lt;NAMESPACE&gt;\n</code></pre> </li> <li> <p>Create the <code>ConfigMap</code> resource-template</p> <p>Create an empty <code>ConfigMap</code> resource-template:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: capm3-cluster-credential-resource-template\n  namespace: &lt;NAMESPACE&gt;\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\n</code></pre> </li> <li> <p>Deploy a test cluster</p> <p>Create a <code>ClusterDeployment</code> template to deploy a cluster using your bare metal machines. Start with a <code>capm3-example.yaml</code> file. This one creates a cluster with 1 control node and 2 workers:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: capm3-example\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  template: capm3-standalone-cp-0-2-0\n  credential: capm3-stub-credential\n  dryRun: false\n  config:\n    clusterAnnotations: {}\n    clusterLabels: {}\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n          - 10.243.0.0/16\n      services:\n        cidrBlocks:\n          - 10.95.0.0/16\n    controlPlane:\n      # the image that was uploaded by default\n      checksum: 581a672e494fcda3297cc8917a91d827157ddcfa3997ad552a914f207b3603c3\n      image: http://&lt;IRONIC_HTTP_ENDPOINT&gt;:6180/images/ubuntu-noble-hwe-2025-05-15-15-22-56.qcow2\n      keepalived:\n        authPass: &lt;VRRP_PASSWORD&gt; # optional, from 4 to 8 letters\n        enabled: true\n        virtualIP: &lt;CLUSTER_API_VIP&gt;/&lt;SUBNET_PREFIX&gt;  # e.g., 10.0.1.70/24. must match k0s.api.externalAddress\n      preStartCommands:\n        - sudo useradd -G sudo -s /bin/bash -d /home/user1 -p $(openssl passwd -1 myuserpass) user1 # define your user here. it can be used e.g. for debugging.\n        - sudo apt update # for Ubuntu\n        - sudo apt install jq -y # for Ubuntu\n        #- sudo dnf makecache # for RedHat\n        #- sudo dnf install jq -y # for RedHat\n        # jq is used in K0sControlPlane object to parse cloud-init data that is required for Metal3 provider\n      files:\n        - path: /home/user1/.ssh/authorized_keys\n          permissions: \"0644\"\n          content: \"&lt;SSH_PUBLIC_KEY&gt;\"\n    controlPlaneNumber: 1\n    dataTemplate:\n      metaData:\n        ipAddressesFromIPPool:\n          - key: provisioningIP\n            name: pool-pxe\n        objectNames:\n          - key: name\n            object: machine\n          - key: local-hostname\n            object: machine\n          - key: local_hostname\n            object: machine\n        prefixesFromIPPool:\n          - key: provisioningCIDR\n            name: pool-pxe\n      networkData:\n        links:\n          ethernets:\n            - id: &lt;INTERFACE_NAME&gt;  # e.g., ens3\n              type: phy\n              macAddress:\n                fromHostInterface: &lt;INTERFACE_NAME&gt;\n        networks:\n          ipv4:\n            - id: pxe\n              ipAddressFromIPPool: pool-pxe\n              link: &lt;INTERFACE_NAME&gt;\n              routes:\n              - gateway:\n                  fromIPPool: pool-pxe\n                network: 0.0.0.0\n                prefix: 0\n        services:\n          dns:\n            - &lt;DNS_SERVER_IP&gt;   # e.g., 8.8.8.8\n    ipPools:\n      - name: pool-pxe\n        pools:\n          - end: &lt;IP_POOL_END&gt;      # e.g., 10.0.1.65\n            gateway: &lt;GATEWAY_IP&gt;   # e.g., 10.0.1.1\n            prefix: &lt;SUBNET_PREFIX&gt; # e.g, \"/24\"\n            start: &lt;IP_POOL_START&gt;  # e.g., 10.0.1.61\n    k0s:\n      api:\n        externalAddress: &lt;CLUSTER_API_VIP&gt;  # e.g., 10.0.1.70\n      telemetry:\n        enabled: false\n      version: v1.32.3+k0s.0\n    worker:\n      # the image that was uploaded by default\n      checksum: 581a672e494fcda3297cc8917a91d827157ddcfa3997ad552a914f207b3603c3\n      image: http://&lt;IRONIC_HTTP_ENDPOINT&gt;:6180/images/ubuntu-noble-hwe-2025-05-15-15-22-56.qcow2\n      preStartCommands:\n        - sudo useradd -G sudo -s /bin/bash -d /home/user1 -p $(openssl passwd -1 myuserpass) user1 # define your user here. it can be used e.g. for debugging.\n        - sudo apt update # for Ubuntu\n        - sudo apt install jq -y # for Ubuntu\n        #- sudo dnf makecache # for RedHat\n        #- sudo dnf install jq -y # for RedHat\n        # jq is used in K0sControlPlane object to parse cloud-init data that is required for Metal3 provider\n      files:\n        - path: /home/user1/.ssh/authorized_keys\n          permissions: \"0644\"\n          content: \"&lt;SSH_PUBLIC_KEY&gt;\"\n    workersNumber: 2\n</code></pre> <p>Create a <code>ClusterDeployment</code> object on your management cluster from the YAML file:</p> <pre><code>kubectl create -f capm3-example.yaml\n</code></pre> </li> <li> <p>Monitor the provisioning process</p> <p>Watch the <code>BareMetalHost</code> objects as they transition through provisioning states:</p> <pre><code>kubectl -n &lt;NAMESPACE&gt; get bmh -w\n</code></pre> <p>You should see the hosts transition from <code>available</code> to <code>provisioning</code> to <code>provisioned</code>:</p> <pre><code>NAME      STATE         CONSUMER                     ONLINE   ERROR   AGE\nchild-1   available                                  true             16m\nchild-2   available                                  true             16m\nchild-3   available                                  true             16m\nchild-2   provisioning  capm3-example-cp-templ-0     true             16m\nchild-2   provisioned   capm3-example-cp-templ-0     true             18m\nchild-1   provisioning  capm3-example-md-txr9f-k8z9d true             18m\nchild-3   provisioning  capm3-example-md-txr9f-lkc5c true             18m\nchild-3   provisioned   capm3-example-md-txr9f-lkc5c true             21m\nchild-1   provisioned   capm3-example-md-txr9f-k8z9d true             21m\n</code></pre> <p>Also monitor the <code>Metal3Machine</code> objects that are part of the cluster:</p> <p><pre><code>kubectl -n &lt;NAMESPACE&gt; get metal3machine -w\n</code></pre> <pre><code>NAME                           AGE     PROVIDERID                                               READY   CLUSTER        PHASE\ncapm3-example-cp-templ-0       5m17s   metal3://kcm-system/child-2/capm3-example-cp-templ-0     true    capm3-example\ncapm3-example-md-txr9f-k8z9d   2m40s   metal3://kcm-system/child-1/capm3-example-md-txr9f-k8z9d true    capm3-example\ncapm3-example-md-txr9f-lkc5c   2m40s   metal3://kcm-system/child-3/capm3-example-md-txr9f-lkc5c true    capm3-example\n</code></pre></p> </li> <li> <p>Access the deployed cluster</p> <p>Once the first control plane machine is ready, retrieve the <code>KUBECONFIG</code> for your new cluster:</p> <pre><code>kubectl get secret -n &lt;NAMESPACE&gt; capm3-example-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; capm3-example-kubeconfig\nKUBECONFIG=\"capm3-example-kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up bare metal resources, delete the child cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME            READY   STATUS\nbm-example   capm3-example   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete clusterdeployments capm3-example -n &lt;NAMESPACE&gt;\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"capm3-example\" deleted\n</code></pre></p> <p>Cluster deletion may take several minutes. Bare metal machines are deprovisioned at this time. </p> <p>Watch the <code>BareMetalHost</code> objects as they transition through provisioning states:</p> <pre><code>kubectl -n &lt;NAMESPACE&gt; get bmh -w\n</code></pre> <p>You should see the hosts transition from <code>provisioned</code> to <code>available</code>:</p> <pre><code>NAME      STATE          CONSUMER                     ONLINE   ERROR   AGE\nchild-1   deprovisioning capm3-example-md-txr9f-k8z9d true             31m\nchild-2   provisioned    capm3-example-cp-templ-0     true             31m\nchild-3   deprovisioning capm3-example-md-txr9f-lkc5c true             31m\nchild-1   available                                   true             36m\nchild-3   available                                   true             37m\nchild-2   deprovisioning capm3-example-cp-templ-0     true             37m\nchild-2   available                                   true             43m\n</code></pre> <p>Then, the available bare metal machines can be used to deploy another cluster, using the same <code>BareMetalHost</code> objects.</p> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#the-oot-capm3-provider-upgrade-notes","title":"The OOT CAPM3 provider upgrade notes","text":"<p>To upgrade the OOT CAPM3 provider from v0.1.x to v0.2.0, you need to proceed through the same steps as you do in case of installing the provider from scratch. Pay attention to the parameters that are defined in the <code>management</code> object:</p> <ul> <li> <p>the following parameters are new to v0.2.0 and <code>\"option:router,&lt;ROUTER_IP&gt;\"</code> is a required one:    <pre><code>  config:\n    ironic:\n      networking:\n        dhcp: # used by DHCP server to assign IPs to hosts during PXE boot\n          netmask: &lt;DHCP_SUBNET_MASK&gt;         # e.g., 255.255.255.192 (default is 255.255.255.0)\n          options:                            # DHCP options, used during PXE boot and by IPA\n            - \"option:router,&lt;ROUTER_IP&gt;\"     # e.g., 10.0.1.1. It's a mandatory option. \n            - \"option:dns-server,&lt;DNS_IP[,DNS2_IP...]&gt;\" # can be set to KEEPALIVED_VIP (dnsmasq can serve as a DNS server with user-defined DNS records) or to IP of your preferred server. Optional.\n            - \"option:ntp-server,&lt;NTP_IP&gt;\"    # can be set to KEEPALIVED_VIP (internal ntp server) or to IP of your preferred server. That ntp server will be used on PXE boot stage then. Optional.\n</code></pre></p> </li> <li> <p>if you changed the set of OS images used for provisioning of bare metal machines, ensure that the new format is used: <pre><code>  # v0.1.x\n  config:\n    ironic:\n      resources:\n        static:\n          images:\n            ubuntu-noble-hwe-2025-05-15-15-22-56.qcow2:\n              sha256sum: 581a672e494fcda3297cc8917a91d827157ddcfa3997ad552a914f207b3603c3\n              url: https://get.mirantis.com/k0rdent-bm/targetimages/ubuntu-noble-hwe-2025-05-15-15-22-56.qcow2\n  # v0.2.0\n  config:\n    ironic:\n      resources:\n        images_target:\n          - name: ubuntu-noble-hwe-2025-05-15-15-22-56.qcow2\n            url: https://get.mirantis.com/k0rdent-bm/targetimages/ubuntu-noble-hwe-2025-05-15-15-22-56.qcow2\n            checksum: 581a672e494fcda3297cc8917a91d827157ddcfa3997ad552a914f207b3603c3\n</code></pre></p> </li> </ul>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#troubleshooting","title":"Troubleshooting","text":"<p>If you run into difficulties, you might find the solution here.</p>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#providertemplate-is-not-valid","title":"<code>ProviderTemplate</code> is not valid","text":"<p>If the <code>ProviderTemplate</code> shows as invalid, inspect the object for error messages:</p> <pre><code>kubectl get providertemplates cluster-api-provider-metal3-0-2-0 -oyaml\n</code></pre> <p>Common issues include incorrect credentials for accessing artifacts or connection problems.</p>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#management-object-does-not-become-ready","title":"<code>Management</code> object does not become ready","text":"<p>If the <code>Management</code> object remains in a non-ready state, inspect it for error messages:</p> <pre><code>kubectl get managements.k0rdent.mirantis.com -oyaml\n</code></pre> <p>If you see Ironic-related errors, check the Ironic deployment:</p> <pre><code>kubectl -n kcm-system get deployment/cluster-api-provider-metal3-ironic\n</code></pre> <p>If Ironic is not ready, verify its configuration:</p> <pre><code>kubectl -n kcm-system get cm ironic-bmo-config -oyaml\n</code></pre> <p>Ensure the configuration matches your network environment, particularly the <code>PROVISIONING_INTERFACE</code>, <code>PROVISIONING_IP</code>, and <code>DHCP_RANGE</code> settings.</p>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#the-helmrelease-does-not-become-ready","title":"The <code>HelmRelease</code> does not become ready","text":"<p>Check to see if the <code>HelmRelease</code> object is <code>Ready</code>, and if not, why:</p> <p><pre><code>kubectl -n kcm-system get helmrelease cluster-api-provider-metal3\n</code></pre> <pre><code>NAME                          AGE    READY   STATUS\ncluster-api-provider-metal3   164m   False   Helm install failed for release kcm-system/cluster-api-provider-metal3 with chart cluster-api-provider-metal3@0.1.0-9d6d9c8: context deadline exceeded\n</code></pre></p> <p>If you see this error, delete the <code>HelmRelease</code>:</p> <pre><code>kubectl -n kcm-system delete helmrelease cluster-api-provider-metal3\n</code></pre> <p>Kubernetes will automatically reinstall the <code>HelmRelease</code>.</p>"},{"location":"admin/installation/prepare-mgmt-cluster/bare-metal/#useful-resources","title":"Useful resources","text":"<p>For additional troubleshooting guidance, refer to the Metal3 troubleshooting documentation.</p> <p>For more information about bare metal cluster configuration options, see:</p> <ul> <li>CAPM3 ClusterTemplate reference</li> <li>k0s configuration documentation</li> <li>CAPM3 API reference</li> <li>Metal3 network configuration guides</li> <li>Metal3 IPAM documentation</li> </ul>"},{"location":"admin/installation/prepare-mgmt-cluster/gcp/","title":"GCP","text":"<p>Available starting in Mirantis k0rdent Enterprise 0.2.0 and later</p> <p>Standalone clusters can be deployed on GCP instances. Follow these steps to make GCP clusters available to your users:</p> <ol> <li> <p>Install Mirantis k0rdent Enterprise</p> <p>Follow the instructions in Install Mirantis k0rdent Enterprise to create a management cluster with Mirantis k0rdent Enterprise running.</p> </li> <li> <p>The gcloud CLI</p> <p>The gcloud CLI (<code>gcloud</code>) is required to interact with GCP resources. You can install it by following  the Install the gcloud CLI instructions.</p> </li> <li> <p>Log in to GCP</p> <p>Authenticate your session with GCP:</p> <pre><code>gcloud auth login\n</code></pre> </li> <li> <p>Enable the required API for your Google Cloud project (if it wasn't previously enabled)</p> <p>The proper API to enable depends on how you plan to deploy Mirantis k0rdent Enterprise:</p> <ul> <li>Standalone/hosted GCP clusters: Enable the <code>Compute Engine API</code>.</li> <li>GKE clusters: Enable the <code>Compute Engine API</code> and the <code>Kubernetes Engine API</code>.</li> </ul> <p>To enable <code>Compute Engine API</code> using the Google Cloud Console (UI):</p> <ol> <li>Go to the Google Cloud Console.</li> <li>Select your project. </li> <li>In the top navigation bar, click on the project selector (drop-down menu). </li> <li>Choose the project where you want to enable the <code>Compute Engine API</code>. </li> <li>Navigate to the <code>API Library</code> (click on the Navigation Menu in the upper-left corner and select <code>APIs &amp; Services</code> \u2192 <code>Library</code>). </li> <li>Search for Compute Engine API (in the API Library, type <code>Compute Engine API</code> in the search bar and press Enter).</li> <li>Enable the API. Click on <code>Compute Engine API</code> from the search results. Click the <code>Enable</code> button.</li> </ol> </li> <li> <p>Create a GCP Service Account</p> <p>Note</p> <p> Skip this step if the Service Account already configured</p> <p>Follow the GCP Service Account creation guide and create a new service account with <code>Editor</code> permissions. If you have plans to deploy <code>GKE</code>, the Service Account will also need the <code>iam.serviceAccountTokenCreator</code> role.</p> </li> <li> <p>Generate a JSON Key for the GCP Service Account</p> <p>Note</p> <p> Skip this step if you're going to use an existing key</p> <p>Follow the Create a service account key guide and create a new key with the JSON key type.</p> <p>A JSON file will automatically download to your computer. Keep it somewhere safe.</p> <p>The example of the JSON file:</p> <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"GCP_PROJECT_ID\",\n  \"private_key_id\": \"GCP_PRIVATE_KEY_ID\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nGCP_PRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"name@project_id.iam.gserviceaccount.com\",\n  \"client_id\": \"GCP_CLIENT_ID\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/user%40project_id.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre> </li> <li> <p>Create a <code>Secret</code> object</p> <p>Create a <code>Secret</code> object that stores the credentials under <code>data</code> section. Create a YAML file called <code>gcp-cluster-identity-secret.yaml</code>, as follows, inserting the base64-encoded GCP credentials (represented by the placeholder <code>GCP_B64ENCODED_CREDENTIALS</code> below) that you get on the previous step. To get base64 encoded credentials, run:</p> <pre><code>cat &lt;gcpJSONCredentialsFileName&gt; | base64 -w 0\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: gcp-cloud-sa\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ndata:\n  # the secret key should always equal `credentials`\n  credentials: GCP_B64ENCODED_CREDENTIALS\ntype: Opaque\n</code></pre> <p>You can then apply the YAML to your cluster:</p> <pre><code>kubectl apply -f gcp-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the Mirantis k0rdent Enterprise <code>Credential</code> Object</p> <p>Create a YAML document with the specification of the <code>Credential</code> and save it as <code>gcp-cluster-identity-cred.yaml</code>.</p> <p>Note that <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>Secret</code> object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: gcp-credential\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: gcp-cloud-sa\n    namespace: kcm-system\n</code></pre> <pre><code>kubectl apply -f gcp-cluster-identity-cred.yaml\n</code></pre> <p>You should see output of:</p> <pre><code>credential.k0rdent.mirantis.com/gcp-cluster-identity-cred created\n</code></pre> </li> <li> <p>Create the <code>ConfigMap</code> resource-template Object</p> <p>Create a YAML with the specification of our resource-template and save it as <code>gcp-cloud-sa-resource-template.yaml</code></p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: gcp-cloud-sa-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $secret := (getResource \"InfrastructureProviderIdentity\") -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: gcp-cloud-sa\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-sa.json: {{ index $secret \"data\" \"credentials\" }}\n</code></pre> <p>Object name needs to be exactly <code>gcp-cloud-sa-resource-template</code> (credentials <code>Secret</code> object name + <code>-resource-template</code> string suffix).</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f gcp-cloud-sa-resource-template.yaml\n</code></pre> <pre><code>configmap/gcp-cloud-sa-resource-template created\n</code></pre></p> </li> </ol> <p>Now you're ready to deploy the cluster.</p> <ol> <li> <p>Create a <code>ClusterDeployment</code></p> <p>To test the configuration, deploy a child cluster by following these steps: First get a list of available regions:</p> <pre><code>gcloud compute regions list\n</code></pre> <p>You'll see output like this:</p> <pre><code>NAME                     CPUS    DISKS_GB  ADDRESSES  RESERVED_ADDRESSES  STATUS  TURNDOWN_DATE\nafrica-south1            0/300   0/102400  0/575      0/175               UP\nasia-east1               0/3000  0/102400  0/575      0/175               UP\nasia-east2               0/1500  0/102400  0/575      0/175               UP\nasia-northeast1          0/1500  0/102400  0/575      0/175               UP\nasia-northeast2          0/750   0/102400  0/575      0/175               UP\n...\n</code></pre> <p>Make note of the region you want to use, such as <code>us-east4</code>.</p> <p>To create the actual child cluster, create a <code>ClusterDeployment</code> that references the appropriate template  as well as the region, credentials, and cluster configuration.</p> <p>You can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-1-0           true\naws-eks-1-0-2                   true\naws-hosted-cp-1-0-11             true\naws-standalone-cp-1-0-12         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-13           true\nazure-standalone-cp-1-0-13       true\nopenstack-standalone-cp-1-0-11   true\nvsphere-hosted-cp-1-0-10         true\nvsphere-standalone-cp-1-0-11     true\ngcp-standalone-cp-1-0-12     true\ngcp-gke-1-0-3     true\n</code></pre></p> <p>Create the YAML for the <code>ClusterDeployment</code> and save it as my-gcp-clusterdeployment1.yaml:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-gcp-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: gcp-standalone-cp-1-0-12\n  credential: gcp-credential\n  config:\n    project: PROJECT_NAME # Your project name\n    region: \"GCP_REGION\" # Select your desired GCP region (find it via `gcloud compute regions list`)\n    network:\n      name: default # Select your desired network name (select new network name to create or find it via `gcloud compute networks list --format=\"value(name)\"`)\n    controlPlane:\n      instanceType: n1-standard-2 # Select your desired instance type (find it via `gcloud compute machine-types list | grep REGION`)\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213 # Select image (find it via `gcloud compute images list --uri`)\n      publicIP: true\n    worker:\n      instanceType: n1-standard-2 \n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n</code></pre> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-gcp-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-gcp-clusterdeployment1 created\n</code></pre></p> <p>Note that although the <code>ClusterDeployment</code> object has been created, there will be a delay as actual GCP instances  are provisioned and added to the cluster. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-gcp-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-gcp-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-gcp-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-gcp-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up GCP resources, delete the child cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-gcp-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete clusterdeployments my-gcp-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-gcp-clusterdeployment1\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/openstack/","title":"OpenStack","text":"<p>Mirantis k0rdent Enterprise can deploy child clusters on OpenStack virtual machines. Follow these steps to configure and deploy OpenStack clusters for your users:</p> <ol> <li> <p>Install Mirantis k0rdent Enterprise</p> <p>Follow the instructions in Install Mirantis k0rdent Enterprise to create a management cluster with Mirantis k0rdent Enterprise running.</p> </li> <li> <p>OpenStack CLI (optional)</p> <p>If you plan to access OpenStack directly, go ahead and  install the OpenStack CLI.</p> </li> <li> <p>Configure the OpenStack Application Credential</p> <p>The exported list of variables should include:</p> <pre><code>OS_AUTH_URL\nOS_APPLICATION_CREDENTIAL_ID\nOS_APPLICATION_CREDENTIAL_SECRET\nOS_REGION_NAME\nOS_INTERFACE\nOS_IDENTITY_API_VERSION\nOS_AUTH_TYPE\n</code></pre> <p>While it's possible to use a username and password instead of the Application Credential \u2014 adjust your YAML accordingly \u2014 an Application Credential is strongly recommended because it limits scope and improves security over a raw username/password approach.</p> </li> <li> <p>Create the OpenStack Credentials Secret</p> <p>Create a Kubernetes <code>Secret</code> containing the <code>clouds.yaml</code> that defines your OpenStack environment, substituting real values where appropriate. Save this as <code>openstack-cloud-config.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openstack-cloud-config\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clouds.yaml: |\n    clouds:\n      openstack:\n        auth:\n          auth_url: &lt;OS_AUTH_URL&gt;\n          application_credential_id: &lt;OS_APPLICATION_CREDENTIAL_ID&gt;\n          application_credential_secret: &lt;OS_APPLICATION_CREDENTIAL_SECRET&gt;\n        region_name: &lt;OS_REGION_NAME&gt;\n        interface: &lt;OS_INTERFACE&gt;\n        identity_api_version: &lt;OS_IDENTITY_API_VERSION&gt;\n        auth_type: &lt;OS_AUTH_TYPE&gt;\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cloud-config.yaml\n</code></pre> </li> <li> <p>Create the Mirantis k0rdent Enterprise Credential Object</p> <p>Next, define a <code>Credential</code> that references the <code>Secret</code> from the previous step. Save this as <code>openstack-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: openstack-cluster-identity-cred\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"  \nspec:\n  description: \"OpenStack credentials\"\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: openstack-cloud-config\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-cred.yaml\n</code></pre> <p>Note that <code>.spec.identityRef.name</code> must match the <code>Secret</code> you created in the previous step, and  <code>.spec.identityRef.namespace</code> must be the same as the one that includes the <code>Secret</code> (<code>kcm-system</code>).</p> </li> <li> <p>Create the ConfigMap resource-template object</p> <p>Create a YAML file with the specification of the resource-template and save it as <code>openstack-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: openstack-cloud-config-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n\n    {{- $clouds := fromYaml (index $identity \"data\" \"clouds.yaml\" | b64dec) -}}\n    {{- if not $clouds }}\n      {{ fail \"failed to decode clouds.yaml\" }}\n    {{ end -}}\n\n    {{- $openstack := index $clouds \"clouds\" \"openstack\" -}}\n\n    {{- if not (hasKey $openstack \"auth\") }}\n      {{ fail \"auth key not found in openstack config\" }}\n    {{- end }}\n    {{- $auth := index $openstack \"auth\" -}}\n\n    {{- $auth_url := index $auth \"auth_url\" -}}\n    {{- $app_cred_id := index $auth \"application_credential_id\" -}}\n    {{- $app_cred_name := index $auth \"application_credential_name\" -}}\n    {{- $app_cred_secret := index $auth \"application_credential_secret\" -}}\n\n    {{- $network_id := $cluster.status.externalNetwork.id -}}\n    {{- $network_name := $cluster.status.externalNetwork.name -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: openstack-cloud-config\n      namespace: kube-system\n    type: Opaque\n    stringData:\n      cloud.conf: |\n        [Global]\n        auth-url=\"{{ $auth_url }}\"\n\n        {{- if $app_cred_id }}\n        application-credential-id=\"{{ $app_cred_id }}\"\n        {{- end }}\n\n        {{- if $app_cred_name }}\n        application-credential-name=\"{{ $app_cred_name }}\"\n        {{- end }}\n\n        {{- if $app_cred_secret }}\n        application-credential-secret=\"{{ $app_cred_secret }}\"\n        {{- end }}\n\n        {{- if and (not $app_cred_id) (not $app_cred_secret) }}\n        username=\"{{ index $openstack \"username\" }}\"\n        password=\"{{ index $openstack \"password\" }}\"\n        {{- end }}\n        region=\"{{ index $openstack \"region_name\" }}\"\n\n        [LoadBalancer]\n        {{- if $network_id }}\n        floating-network-id=\"{{ $network_id }}\"\n        {{- end }}\n\n        [Networking]\n        {{- if $network_name }}\n        public-network-name=\"{{ $network_name }}\"\n        {{- end }}\n</code></pre> <p>Object needs to be named <code>openstack-cluster-identity-resource-template.yaml</code>, <code>OpenStackClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-resource-template.yaml\n</code></pre> </li> <li> <p>Create Your First Child Cluster</p> <p>To test the configuration, create a YAML file with the specification of your Managed Cluster and save it as <code>my-openstack-cluster-deployment.yaml</code>.  Note that you can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-1-0           true\naws-eks-1-0-2                   true\naws-hosted-cp-1-0-11             true\naws-standalone-cp-1-0-12         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-13           true\nazure-standalone-cp-1-0-13       true\nopenstack-standalone-cp-1-0-11   true\nvsphere-hosted-cp-1-0-10         true\nvsphere-standalone-cp-1-0-11     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> should look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-1-0-11\n  credential: openstack-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    externalNetwork:\n      filter:\n        name: \"public\"\n    authURL: ${OS_AUTH_URL}\n    identityRef:\n      name: \"openstack-cloud-config\"\n      cloudName: \"openstack\"\n      region: ${OS_REGION_NAME}\n</code></pre> <p>You can adjust <code>flavor</code>, <code>image name</code>, <code>region name</code>, and <code>authURL</code> to match your OpenStack environment. For more information about the configuration options, see the OpenStack Template Parameters Reference.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-openstack-cluster-deployment.yaml\n</code></pre> <p>This will trigger the provisioning process where Mirantis k0rdent Enterprise will create a bunch of OpenStack resources such as OpenStackCluster, OpenStackMachineTemplate, OpenStackMachineDeployment, etc. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-openstack-cluster-deployment --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, just like any other Kubernetes cluster:</p> <pre><code>kubectl -n kcm-system get secret my-openstack-cluster-deployment-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-openstack-cluster-deployment-kubeconfig.kubeconfig\nKUBECONFIG=\"my-openstack-cluster-deployment-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up OpenStack resources, delete the managed cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get clusterdeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-openstack-cluster-deployment   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete clusterdeployments my-openstack-cluster-deployment -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-openstack-cluster-deployment\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin/installation/prepare-mgmt-cluster/vmware/","title":"vSphere","text":"<p>To enable users to deploy child clusers on vSphere, follow these steps:</p> <ol> <li> <p>Create a Mirantis k0rdent Enterprise management cluster</p> <p>Follow the instructions in Install Mirantis k0rdent Enterprise to create a management cluster with Mirantis k0rdent Enterprise running, as well as a local install of <code>kubectl</code>.</p> </li> <li> <p>Install a vSphere instance version <code>6.7.0</code> or higher.</p> </li> <li> <p>Create a vSphere account with appropriate privileges</p> <p>To function properly, the user assigned to the vSphere Provider should be able to manipulate vSphere resources. The user should have the following  required privileges:</p> <pre><code>Virtual machine: Full permissions are required\nNetwork: Assign network is sufficient\nDatastore: The user should be able to manipulate virtual machine files and metadata\n</code></pre> <p>In addition to that, specific CSI driver permissions are required. See the official doc for more information on CSI-specific permissions.</p> </li> <li> <p>Image template</p> <p>You can use pre-built image templates from the CAPV project or build your own.</p> <p>When building your own image, make sure that VMware tools and cloud-init are installed and properly configured.</p> <p>You can follow the official open-vm-tools guide on how to correctly install VMware tools.</p> <p>When setting up cloud-init, you can refer to the official docs and specifically the VMware datasource docs for extended information regarding cloud-init on vSphere.</p> </li> <li> <p>vSphere network</p> <p>When creating a network, make sure that it has the DHCP service.</p> <p>Also, ensure that part of your network is out of the DHCP range (for example, the network <code>172.16.0.0/24</code> should have a DHCP range of <code>172.16.0.100-172.16.0.254</code> only) so that LoadBalancer services will not create any IP conflicts in the network.</p> </li> <li> <p>vSphere Credentials</p> <p>To enable Mirantis k0rdent Enterprise to access vSphere resources, create the appropriate credentials objects. For a full explanation of how <code>Credential</code> objects work, see the main Credentials chapter, but for now, follow these steps:</p> <p>Create a <code>Secret</code> object with the username and password.</p> <p>Warning</p> <p> The <code>username</code> parameter should be set to the vCenter username along with the domain name. For example, <code>userName@domainName</code>. If you don't specify the domain name for active directory users, the vSphere Container Storage Plug-in will not function properly.</p> <p>The <code>Secret</code> stores the username and password for your vSphere instance. Save the <code>Secret</code> YAML in a file named <code>vsphere-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  username: &lt;USERNAME&gt;\n  password: &lt;PASSWORD&gt;\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>VSphereClusterIdentity</code> Object</p> <p>The <code>VSphereClusterIdentity</code> object defines the credentials CAPV will use to manage vSphere resources.</p> <p>Save the <code>VSphereClusterIdentity</code> YAML into a file named <code>vsphere-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: VSphereClusterIdentity\nmetadata:\n  name: vsphere-cluster-identity\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretName: vsphere-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>The <code>VSphereClusterIdentity</code> object references the <code>Secret</code> you created in the previous step, so <code>.spec.secretName</code>  needs to match the <code>.metadata.name</code> for the <code>Secret</code>.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity.yaml\n</code></pre> </li> <li> <p>Create the <code>Credential</code> Object</p> <p>Create a YAML with the specification of our credential and save it as <code>vsphere-cluster-identity-cred.yaml</code></p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: vsphere-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: VSphereClusterIdentity\n    name: vsphere-cluster-identity\n    namespace: kcm-system\n</code></pre> Again, <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>VSphereClusterIdentity</code> object you just created.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-cred.yaml\n</code></pre> </li> <li> <p>Create the <code>ConfigMap</code> resource-template Object</p> <p>Create a YAML with the specification of our resource-template and save it as <code>vsphere-cluster-identity-resource-template.yaml</code></p> <p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vsphere-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: vsphere-cloud-secret\n      namespace: kube-system\n    type: Opaque\n    data:\n      {{ printf \"%s.username\" $cluster.spec.server }}: {{ index $secret.data \"username\" }}\n      {{ printf \"%s.password\" $cluster.spec.server }}: {{ index $secret.data \"password\" }}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: vcenter-config-secret\n      namespace: kube-system\n    type: Opaque\n    stringData:\n      csi-vsphere.conf: |\n        [Global]\n        cluster-id = \"{{ $cluster.metadata.name }}\"\n\n        [VirtualCenter \"{{ $cluster.spec.server }}\"]\n        insecure-flag = \"true\"\n        user = \"{{ index $secret.data \"username\" | b64dec }}\"\n        password = \"{{ index $secret.data \"password\" | b64dec }}\"\n        port = \"443\"\n        datacenters = ${VSPHERE_DATACENTER}\n    ---\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: cloud-config\n      namespace: kube-system\n    data:\n      vsphere.conf: |\n        global:\n          insecureFlag: true\n          port: 443\n          secretName: vsphere-cloud-secret\n          secretNamespace: kube-system\n        labels:\n          region: k8s-region\n          zone: k8s-zone\n        vcenter:\n          {{ $cluster.spec.server }}:\n            datacenters:\n              - ${VSPHERE_DATACENTER}\n            server: {{ $cluster.spec.server }}\n</code></pre> Object name needs to be exactly <code>vsphere-cluster-identity-resource-template</code>, <code>VSphereClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-resource-template.yaml\n</code></pre> </li> <li> <p>Create your first Cluster Deployment</p> <p>Test the configuration by deploying a cluster. Create a YAML document with the specification of your Cluster Deployment and save it as <code>my-vsphere-clusterdeployment1.yaml</code>.</p> <p>You can get a list of available templates:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                            VALID\nadopted-cluster-1-1-0           true\naws-eks-1-0-2                   true\naws-hosted-cp-1-0-11             true\naws-standalone-cp-1-0-12         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-13           true\nazure-standalone-cp-1-0-13       true\nopenstack-standalone-cp-1-0-11   true\nvsphere-hosted-cp-1-0-10         true\nvsphere-standalone-cp-1-0-11     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> YAML file should look something like this. Make sure to replace the placeholders with your specific information:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-vsphere-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: vsphere-standalone-cp-1-0-11\n  credential: vsphere-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    controlPlaneNumber: 1\n    workersNumber: 1\n    vsphere:\n      server: &lt;VSPHERE_SERVER&gt;\n      thumbprint: &lt;VSPHERE_THUMBPRINT&gt;\n      datacenter: &lt;VSPHERE_DATACENTER&gt;\n      datastore: &lt;VSPHERE_DATASTORE&gt;\n      resourcePool: &lt;VSPHERE_RESOURCEPOOL&gt;\n      folder: &lt;VSPHERE_FOLDER&gt;\n      username: ${VSPHERE_USER}\n      password: ${VSPHERE_PASSWORD}\n    controlPlaneEndpointIP: &lt;VSPHERE_CONTROL_PLANE_ENDPOINT&gt;\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      rootVolumeSize: 50\n      cpus: 4\n      memory: 4096\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      rootVolumeSize: 50\n      cpus: 4\n      memory: 4096\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n</code></pre> <p>For more information about the available configuration options, see the vSphere Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-vsphere-clusterdeployment1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-vsphere-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-vsphere-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To delete the provisioned cluster and free consumed vSphere resources run:</p> <pre><code>kubectl -n kcm-system delete cluster my-vsphere-clusterdeployment1\n</code></pre> </li> </ol>"},{"location":"admin/kof/","title":"Mirantis k0rdent Enterprise Observability and FinOps","text":"<p>Mirantis k0rdent Enterprise Observability and FinOps (kof) provides enterprise-grade observability and FinOps capabilities for k0rdent-managed child Kubernetes clusters. It enables centralized metrics, logging, and cost management through a unified OpenTelemetry-based architecture.</p> <ul> <li> <p>Observability: KOF collects metrics from various sources and stores them in a time series database based on Victoria Metrics, allowing for real-time and historical analysis. It includes log management features to aggregate, store, and analyze logs from different  components of the Kubernetes ecosystem. This helps in troubleshooting and understanding the behavior of applications and infrastructure. KOF can evaluate alerting rules and send notifications based on these collected metrics and logs helping to identify and respond to issues before they impact users.</p> </li> <li> <p>FinOps: KOF helps with cost management by tracking and managing the costs associated with running applications on Kubernetes.  It provides insights into resource utilization and helps in optimizing costs by identifying underutilized or over-provisioned resources. With this information, you can set budgets and forecast future costs based on historical data and current  usage patterns. KOF enables chargeback and showback mechanisms, enabling organizations to attribute costs to  specific teams, departments, or projects, and promotes accountability and transparency in resource usage.</p> </li> <li> <p>Centralized Management: KOF provides a unified control plane for managing Kubernetes clusters at scale, with a  centralized view of all clusters, making it possible to use Mirantis k0rdent Enterprise to manage and operate large-scale deployments. It also offers comprehensive lifecycle management capabilities, including provisioning,  configuration, and maintenance of Kubernetes clusters, ensuring clusters are consistently managed and adhere to best practices.</p> </li> <li> <p>Scalability and Performance: KOF leverages components such as VictoriaMetrics to provide high-performance monitoring and analytics.  It can handle millions of metrics per second and provides low-latency query responses. It's also designed to scale horizontally, enabling it to manage large volumes of data and support growing environments. It can be deployed on-premises, in the cloud, or in hybrid environments.</p> </li> <li> <p>Compliance and Security: KOF helps ensure compliance with organizational policies and industry standards, providing audit trails and reporting features to meet regulatory requirements. It includes security features to protect data and ensure  the integrity of monitoring and FinOps processes. It supports role-based access control (RBAC) and secure communication protocols.</p> </li> </ul>"},{"location":"admin/kof/#use-cases","title":"Use Cases","text":"<p>KOF can be used by both technical and non-technical arms of a company.</p> <ul> <li>Platform Engineering: KOF is ideal for platform engineers who need to manage and monitor Kubernetes  clusters at scale. It provides the tools and insights required to ensure the reliability and performance of applications.</li> <li>DevOps Teams: DevOps teams can use KOF to gain visibility into the deployment and operation of applications,  helping them to identify and resolve issues quickly.</li> <li>Finance Teams: Finance teams can leverage KOF's FinOps capabilities to track and manage cloud spending,  ensuring resources are used efficiently and costs are optimized.</li> </ul>"},{"location":"admin/kof/#guides","title":"Guides","text":"<p>Get started with the basic documentation:</p> <ul> <li>Architecture</li> <li>Installing KOF</li> <li>Verifying the KOF installation</li> <li>Storing KOF data</li> <li>Using KOF</li> <li>KOF Alerts</li> <li>Scaling KOF</li> <li>Maintaining KOF</li> </ul> <p>Once you have KOF up and running, check k0rdent/kof/docs for advanced guides.</p>"},{"location":"admin/kof/kof-alerts/","title":"KOF Alerts","text":""},{"location":"admin/kof/kof-alerts/#summary","title":"Summary","text":"<p>At this point you have metrics collected and visualized. It is important to check them manually, but it is even better to automate detection and notification about the issues found in the data.</p> <p>We believe the rules should be configured using YAML IaC (Infrastructure as Code), while you can perform temporary management such as Silences using the UI.</p> <p>Alerting rules and recording rules in KOF are based on the PrometheusRules from the kube-prometheus-stack chart with per-cluster customization options.</p> <p>KOF uses the data source managed rules to store and execute recording rules in regional clusters closer to the source data, and to reduce the load on Grafana, even for alerting rules executed by Promxy in the management cluster.</p> <p>Promxy is used as a data source and executor of alerting rules instead of VMAlert because:</p> <ul> <li> <p>As the Promxy FAQ says, \"for example, if you wanted to know that the global error rate was &lt; 10%,     this would be impossible on the individual prometheus hosts     (without federation, or re-scraping) but trivial in promxy.\"</p> </li> <li> <p>It fixes the \"See graph\" button in the Grafana Alerting rules UI,     as Grafana gets the metrics from all regional clusters via Promxy.</p> </li> </ul> <p>VMAlertManager aggregates and sends alerts to various receivers like Slack with advanced routing options.</p> <p>Let's start with the demo of an alert sent and received.</p>"},{"location":"admin/kof/kof-alerts/#alertmanager-demo","title":"Alertmanager Demo","text":"<ol> <li> <p>Open the https://webhook.site/ and save \"Your unique URL\"     for the next step.</p> </li> <li> <p>Add the following to the <code>mothership-values.yaml</code> file, replacing <code>$WEBHOOK_URL</code> with the URL from step 1:     <pre><code>victoriametrics:\n  vmalert:\n    vmalertmanager:\n      config: |\n        route:\n          receiver: webhook\n        receivers:\n          - name: webhook\n            webhook_configs:\n              - url: $WEBHOOK_URL\n</code></pre></p> </li> <li> <p>Apply the <code>mothership-values.yaml</code> file as described in the Management Cluster section.</p> </li> <li> <p>Wait until the https://webhook.site/     shows the <code>Watchdog</code> alert, as in:     <pre><code>{\n  \"receiver\": \"webhook\",\n  \"status\": \"firing\",\n  \"alerts\": [\n    {\n      \"status\": \"firing\",\n      \"labels\": {\n        \"alertgroup\": \"general.rules\",\n        \"alertname\": \"Watchdog\",\n        \"severity\": \"none\",\n        \"source\": \"promxy\"\n      },\n      \"annotations\": {\n        \"description\": \"This is an alert meant to ensure that the entire alerting pipeline is functional...\",\n        \"runbook_url\": \"https://runbooks.prometheus-operator.dev/runbooks/general/watchdog\",\n        \"summary\": \"An alert that should always be firing to certify that Alertmanager is working properly.\"\n      },\n      \"startsAt\": \"2025-06-02T10:27:29.14Z\",\n      \"endsAt\": \"0001-01-01T00:00:00Z\",\n      \"generatorURL\": \"http://127.0.0.1:8082/...\",\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-alerts/#advanced-routing","title":"Advanced Routing","text":"<p>The configuration of the Alertmanager Demo is very basic.</p> <p>Please use these guides to apply advanced routing options:</p> <ul> <li> <p>Prometheus Alertmanager configuration reference - all possible options.</p> </li> <li> <p>VMAlertManager Slack example -     a multichannel notification system to ensure that critical alerts     are promptly delivered to the responsible teams.</p> </li> <li> <p>Matchers -     configurable routing rules that determine where and how alerts are directed     (for example, email, Slack, PagerDuty) based on severity, source, or other attributes.</p> </li> <li> <p>Grouping and     example from Prometheus     with <code>group_by: [cluster, alertname]</code> -     you may want to use <code>group_by: [alertgroup, alertname]</code> instead     for alert correlation across clusters to identify systemic issues and reduce noise     when the same alert fires in multiple clusters.</p> </li> </ul>"},{"location":"admin/kof/kof-alerts/#alertmanager-ui","title":"Alertmanager UI","text":"<p>To access the Alertmanager UI:</p> <ol> <li> <p>In the management cluster, forward the alertmanager port:     <pre><code>kubectl port-forward -n kof svc/vmalertmanager-cluster 9093:9093\n</code></pre></p> </li> <li> <p>Open http://127.0.0.1:9093/     and check tabs such as \"Alerts\" and \"Silences\".</p> </li> </ol> <p>See the demo in the Grafana Alerting UI section where Alertmanager UI shows the same data.</p>"},{"location":"admin/kof/kof-alerts/#grafana-alerting-ui","title":"Grafana Alerting UI","text":"<p>To access the Grafana Alerting UI:</p> <ol> <li> <p>Apply the Access to Grafana step.</p> </li> <li> <p>Open: Grafana - Alerting - and then \"Alert rules\" or \"Silences\", like this:</p> </li> </ol> <p></p>"},{"location":"admin/kof/kof-alerts/#prometheus-ui","title":"Prometheus UI","text":"<p>There are few places where you can find the graph of the firing alert:</p> <ol> <li> <p>Grafana - Alerting - Alert rules - rule - See graph.</p> <p>This shows the graph in Grafana UI, as in the Grafana Alerting UI demo above.</p> </li> <li> <p>Grafana - Alerting - Groups - group - alert - See source - Graph.</p> <p>This shows the graph in Prometheus UI.</p> </li> <li> <p>The same Prometheus UI link is sent to receiver like Slack in <code>generatorURL</code> field,     as shown in the Alertmanager Demo.</p> </li> </ol> <p>The Prometheus UI looks like this:</p> <p></p> <p>To enable Promxy Prometheus UI, please run this command in the management cluster:</p> <pre><code>kubectl port-forward -n kof svc/kof-mothership-promxy 8082:8082\n</code></pre> <p>If you expose the Prometheus UI with some external domain, please set <code>promxy.extraArgs.\"web.external-url\"</code> in <code>mothership-values.yaml</code> file and reapply it as described in the Management Cluster section.</p>"},{"location":"admin/kof/kof-alerts/#custom-rules","title":"Custom rules","text":"<p>You can update or create rules for all or specific clusters in a centralized way, passing values to the <code>kof-mothership</code> chart installed in the management cluster.</p> <p>For example, let's update the <code>CPUThrottlingHigh</code> alert in the <code>kubernetes-resources</code> group:</p> <ol> <li> <p>Note the original alert     in the <code>PrometheusRule</code> has the threshold <code>&gt; ( 25 / 100 )</code>.</p> </li> <li> <p>Add this cluster-specific patch to the <code>mothership-values.yaml</code> file:     <pre><code>clusterAlertRules:\n  cluster1:\n    kubernetes-resources:\n      CPUThrottlingHigh:\n        expr: |-\n          sum(increase(container_cpu_cfs_throttled_periods_total{cluster=\"cluster1\", container!=\"\"}[5m])) without (id, metrics_path, name, image, endpoint, job, node)\n            / on (cluster, namespace, pod, container, instance) group_left\n          sum(increase(container_cpu_cfs_periods_total{cluster=\"cluster1\"}[5m])) without (id, metrics_path, name, image, endpoint, job, node)\n            &gt; ( 42 / 100 )\n</code></pre>     Note the <code>cluster=\"cluster1\"</code> filters and the <code>&gt; ( 42 / 100 )</code> threshold.</p> </li> <li> <p>Add a similar patch for <code>cluster10</code> to the same <code>clusterAlertRules</code>.</p> </li> <li> <p>Now that we have special <code>CPUThrottlingHigh</code> alerts for <code>cluster1</code> and <code>cluster10</code>,     we want to exclude these clusters from the default <code>CPUThrottlingHigh</code> alert     to avoid the ambiguity of which threshold fires this alert in each cluster.</p> <p>Add this patch to the same file: <pre><code>defaultAlertRules:\n  kubernetes-resources:\n    CPUThrottlingHigh:\n      expr: |-\n        sum(increase(container_cpu_cfs_throttled_periods_total{cluster!~\"^cluster1$|^cluster10$\", container!=\"\"}[5m])) without (id, metrics_path, name, image, endpoint, job, node)\n          / on (cluster, namespace, pod, container, instance) group_left\n        sum(increase(container_cpu_cfs_periods_total{cluster!~\"^cluster1$|^cluster10$\"}[5m])) without (id, metrics_path, name, image, endpoint, job, node)\n          &gt; ( 25 / 100 )\n</code></pre> Note the <code>cluster!~\"^cluster1$|^cluster10$\"</code> filters and the default threshold.</p> </li> <li> <p>You can also update or create recording rules in the same way,     but the whole rule group should be redefined, because the <code>record</code> field is not unique.</p> </li> <li> <p>You may update or create more rules,     like the <code>ContainerHighMemoryUsage</code> alert that was added on demand     from the awesome-prometheus-alerts collection.</p> </li> <li> <p>Apply the <code>mothership-values.yaml</code> file as described in the Management Cluster section.</p> </li> </ol>"},{"location":"admin/kof/kof-alerts/#generation-of-rules","title":"Generation of rules","text":"<p>The next steps are automated:</p> <pre><code>graph TB\n  KPSRF[rules files and values&lt;br&gt;copied from kube-prometheus-stack&lt;br&gt;to kof-mothership] --&gt;\n  PR[PrometheusRules]\n\n  ARV[kof-mothership values:&lt;br&gt;defaultAlertRules,&lt;br&gt;clusterAlertRules] --&gt;\n  ARCM[input ConfigMaps:&lt;br&gt;k-m-promxy-rules-default,&lt;br&gt;k-m-promxy-rules-cluster-*] --kof-operator:&lt;br&gt;configmap_controller&lt;br&gt;updates--&gt;\n  KMPR[output ConfigMap:&lt;br&gt;k-m-promxy-rules]\n  PR --&gt; KMPR ---&gt;\n  EPR[Alerting /etc/promxy/rules]\n\n  RRV[kof-mothership values:&lt;br&gt;defaultRecordRules&lt;br&gt;clusterRecordRules] --&gt;\n  RRCM[input ConfigMaps:&lt;br&gt;kof-record-rules-default,&lt;br&gt;kof-record-rules-cluster-*]\n  PR --&gt; KRVM[output ConfigMap:&lt;br&gt;kof-record-vmrules-*]\n  RRCM --&gt; KRVM\n\n  KRVM --\"Management special case:&lt;br&gt;helm upgrade -i kof-storage&lt;br&gt;-f vmrules.yaml\"--&gt; KSV\n  KRVM --Regional MCS/ClusterProfile&lt;br&gt;valuesFrom: ConfigMap--&gt;\n  KSV[kof-storage values:&lt;br&gt;vmrules: groups: ...] --&gt;\n  VMR[Recording VMRules]\n\n  RCD[Regional ClusterDeployment] --\"kof-operator:&lt;br&gt;clusterdeployment_controller&lt;br&gt;creates empty\"--&gt; KRVM</code></pre> <ul> <li> <p>Rules patches (empty by default) are rendered from <code>kof-mothership</code> values     to the input <code>ConfigMaps</code>, which are merged with upstream <code>PrometheusRules</code>,     generating     the output <code>ConfigMaps</code>.</p> <ul> <li>If you want to protect some output <code>ConfigMap</code> from automatic changes,     set its label <code>k0rdent.mirantis.com/kof-generated: \"false\"</code></li> </ul> </li> <li> <p>Alerting rules are mounted to Promxy in the management cluster as <code>/etc/promxy/rules</code>.</p> </li> <li> <p>Recording rules are passed via <code>MultiClusterService</code> (or <code>ClusterProfile</code> for <code>istio</code> case)     to each regional cluster where <code>kof-storage</code> chart renders them to <code>VMRules</code>.</p> </li> </ul>"},{"location":"admin/kof/kof-alerts/#mothership-recording-rules","title":"Mothership recording rules","text":"<p>If you've selected to store KOF data of the management cluster in the same management cluster, then:</p> <ol> <li> <p>Copy the generated mothership recording rules from the output <code>ConfigMap</code> to a YAML file:     <pre><code>kubectl get cm -n kof kof-record-vmrules-mothership -o yaml \\\n| yq -r .data.values &gt; vmrules.yaml\n</code></pre></p> </li> <li> <p>Add <code>-f vmrules.yaml</code> to the <code>helm upgrade ... kof-storage</code> command     described in the From Management to Management section     and apply it.</p> </li> </ol>"},{"location":"admin/kof/kof-alerts/#execution-of-rules","title":"Execution of rules","text":"<p>Details of where and how the recording and alerting rules are executed:</p> <pre><code>sequenceDiagram\n    box rgba(0, 0, 255, 0.2) Regional kof-storage\n        participant VMR as Recording VMRules\n        participant VMA as VMAlert\n        participant VMS as VMStorage\n    end\n\n    box rgba(255, 0, 0, 0.2) Management kof-mothership\n        participant MP as Promxy\n        participant MVMS as VMStorage\n        participant VMAM as VMAlertManager\n    end\n\n    VMA-&gt;&gt;VMR: execute\n    VMA-&gt;&gt;VMS: read \"expr\" metrics\n    VMA-&gt;&gt;VMS: write \"record\" metrics\n\n    note over MP: execute&lt;br&gt;Alerting /etc/promxy/rules\n    MP-&gt;&gt;VMS: read \"expr\" metrics\n    MP-&gt;&gt;MVMS: write \"ALERTS\" metrics\n    MP-&gt;&gt;VMAM: Notify about alert</code></pre> <ul> <li> <p>Recording <code>VMRules</code> are executed by <code>VMAlert</code>, reading and writing to <code>VMStorage</code> -     all this happens in <code>kof-storage</code> in each regional cluster.</p> <p>The From Management to Management case is special: <code>VMRules</code> are provided by <code>kof-storage</code> chart in the management cluster, while <code>VMAlert</code> and <code>VMStorage</code> are provided by <code>kof-mothership</code> - to avoid having two VictoriaMetrics engines in the same cluster.</p> </li> <li> <p>Alerting rules are executed by Promxy in <code>kof-mothership</code> in the management cluster,     reading metrics from all regional <code>VMStorages</code>,     writing to the management <code>VMStorage</code>,     and notifying <code>VMAlertManager</code> in the management cluster.</p> </li> </ul>"},{"location":"admin/kof/kof-architecture/","title":"Architecture","text":""},{"location":"admin/kof/kof-architecture/#high-level","title":"High-level","text":"<p>From a high-level perspective, KOF consists of three layers:</p> <ul> <li>the Collection layer, where the statistics and events are gathered,</li> <li>the Regional layer, which includes storage to keep track of those statistics and events,</li> <li>and the Management layer, where you interact through the UI.</li> </ul> <pre><code>flowchart TD\n    A[Management UI, promxy] \n    A --&gt; C[Storage Region 1]\n    A --&gt; D[Storage Region 2]\n    C --&gt; E[Collect Child 1]\n    C --&gt; F[Collect Child 2]\n    D ==&gt; G[...]</code></pre>"},{"location":"admin/kof/kof-architecture/#mid-level","title":"Mid-level","text":"<p>Getting a little bit more detailed, it's important to undrestand that data flows upwards, from observed objects to centralized Grafana on the Management layer:</p> Management Cluster      kof-mothership chart            grafana-operator             victoria-metrics-operator             cluster-api-visualizer             sveltos-dashboard             kof-operator             Mirantis k0rdent Enterprise service templates             promxy           kof-operators chart            opentelemetry-operator             prometheus-operator-crds           kof-collectors chart            opencost             kube-state-metrics             prometheus-node-exporter           Either kof-istio            Certificates             ClusterProfiles           Or kof-regional and kof-child            MultiClusterServices         Cloud 1..N        Region 1..M      Regional Cluster          kof-storage chart                    grafana-operator                     victoria-metrics-operator                     victoria-logs-single                     jaeger-operator                     external-dns                   kof-operators chart                    opentelemetry-operator                     prometheus-operator-crds                   kof-collectors chart                    opencost                     kube-state-metrics                     prometheus-node-exporter                   cert-manager                 ingress-nginx or kof-istio        Child Cluster 1          cert-manager                 Optional kof-istio                 kof-operators chart                    opentelemetry-operator                     prometheus-operator-crds                   kof-collectors chart                    opencost                     kube-state-metrics                     prometheus-node-exporter                   observed objects"},{"location":"admin/kof/kof-architecture/#low-level","title":"Low-level","text":"<p>At a low level, you can see how metrics, logs, and traces work their way around the system.</p> <p></p>"},{"location":"admin/kof/kof-architecture/#helm-charts","title":"Helm Charts","text":"<p>KOF is deployed as a series of Helm charts at various levels.</p>"},{"location":"admin/kof/kof-architecture/#kof-mothership","title":"kof-mothership","text":"<ul> <li>Centralized Grafana dashboard, managed by grafana-operator</li> <li>Local VictoriaMetrics storage for alerting rules only, managed by victoria-metrics-operator</li> <li>cluster-api-visualizer for insight into multicluster configuration</li> <li>Sveltos dashboard, automatic secret distribution</li> <li>kof-operator (don't confuse it with the <code>kof-operators</code> chart) for auto-configuration</li> <li>Mirantis k0rdent Enterprise service templates used by <code>kof-regional</code> and <code>kof-child</code> charts</li> <li>Promxy for aggregating Prometheus metrics from regional clusters</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-regional","title":"kof-regional","text":"<ul> <li>MultiClusterService   which configures and installs <code>kof-storage</code> and other charts to regional clusters</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-child","title":"kof-child","text":"<ul> <li>MultiClusterService   which configures and installs <code>kof-collectors</code> and other charts to child clusters</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-storage","title":"kof-storage","text":"<ul> <li>Regional Grafana dashboard, managed by grafana-operator</li> <li>Regional VictoriaMetrics storage with main data, managed by victoria-metrics-operator</li> <li>vmauth entrypoint proxy for VictoriaMetrics components</li> <li>vmcluster for high-available fault-tolerant version of VictoriaMetrics database</li> <li>victoria-logs-single for high-performance, cost-effective, scalable logs storage</li> <li>Regional Jaeger tracing platform, managed by jaeger-operator</li> <li>external-dns to communicate with other clusters</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-istio","title":"kof-istio","text":"<ul> <li>Optional Istio support for secure connectivity between clusters without external DNS</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-operators","title":"kof-operators","text":"<ul> <li>prometheus-operator-crds required to create OpenTelemetry collectors, also required to monitor <code>kof-mothership</code> itself</li> <li>OpenTelemetry collectors below, managed by opentelemetry-operator</li> </ul>"},{"location":"admin/kof/kof-architecture/#kof-collectors","title":"kof-collectors","text":"<ul> <li>prometheus-node-exporter for hardware and OS metrics</li> <li>kube-state-metrics for metrics about the state of Kubernetes objects</li> <li>OpenCost \"shines a light into the black box of Kubernetes spend\"</li> </ul>"},{"location":"admin/kof/kof-install/","title":"Installing Mirantis k0rdent Enterprise Observability and FinOps","text":""},{"location":"admin/kof/kof-install/#prerequisites","title":"Prerequisites","text":"<p>Before beginning KOF installation, you should have the following components in place:</p> <ul> <li>A Mirantis k0rdent Enterprise management cluster - You can get instructions to create one in the quickstart guide.</li> <li>You will also need your infrastructure provider credentials, such as those shown in the guide for AWS.<ul> <li>Note that you should skip the \"Create your <code>ClusterDeployment</code>\" and later sections.</li> </ul> </li> <li>Finally, select one of the options:<ul> <li>DNS auto-config to automate the process for all regional clusters.</li> <li>Manual DNS config is applied later   for each regional cluster separately and manually.</li> <li>Istio service mesh for secure connectivity between clusters.   This is the only option which does not need access to create external DNS records   for service endpoints such as <code>kof.example.com</code>.</li> </ul> </li> </ul>"},{"location":"admin/kof/kof-install/#image-registry","title":"Image Registry","text":"<p>Let's configure the registry where the Docker images will be pulled from.</p> <p>Create the <code>global-values.yaml</code> file:</p> <pre><code>global:\n  registry: registry.mirantis.com/k0rdent-enterprise\n  imageRegistry: registry.mirantis.com/k0rdent-enterprise\n  image:\n    registry: registry.mirantis.com/k0rdent-enterprise\n  hub: registry.mirantis.com/k0rdent-enterprise/istio\n  helmChartsRepo: oci://registry.mirantis.com/k0rdent-enterprise/charts\ncert-manager-istio-csr:\n  image:\n    repository: registry.mirantis.com/k0rdent-enterprise/jetstack/cert-manager-istio-csr\ncert-manager-service-template:\n  skipVerifyJob: true\n  repo:\n    type: oci\n    url: oci://registry.mirantis.com/k0rdent-enterprise/charts\ncluster-api-visualizer:\n  image:\n    repository: registry.mirantis.com/k0rdent-enterprise/k0rdent\n    tag: 1.4.1\nexternal-dns:\n  image:\n    repository: registry.mirantis.com/k0rdent-enterprise/external-dns/external-dns\ngrafana-operator:\n  image:\n    repository: registry.mirantis.com/k0rdent-enterprise/grafana/grafana-operator\ningress-nginx-service-template:\n  skipVerifyJob: true\n  repo:\n    type: oci\n    url: oci://registry.mirantis.com/k0rdent-enterprise/charts\njaeger-operator:\n  image:\n    repository: registry.mirantis.com/k0rdent-enterprise/jaegertracing/jaeger-operator\nkcm:\n  kof:\n    repo:\n      spec:\n        url: oci://registry.mirantis.com/k0rdent-enterprise/charts\nopencost:\n  opencost:\n    exporter:\n      image:\n        registry: registry.mirantis.com/k0rdent-enterprise\n    ui:\n      image:\n        registry: registry.mirantis.com/k0rdent-enterprise\nopentelemetry-operator:\n  manager:\n    image:\n      repository: registry.mirantis.com/k0rdent-enterprise/opentelemetry-operator/opentelemetry-operator\n    collectorImage:\n      repository: registry.mirantis.com/k0rdent-enterprise/otel/opentelemetry-collector-contrib\n  kubeRBACProxy:\n    image:\n      repository: registry.mirantis.com/k0rdent-enterprise/brancz/kube-rbac-proxy\n</code></pre> <p>This file will be used in the next sections.</p>"},{"location":"admin/kof/kof-install/#air-gap","title":"Air Gap","text":"<p>For an air-gapped environment please apply additional steps below:</p> <ol> <li> <p>Replace all mentions of the <code>registry.mirantis.com/k0rdent-enterprise</code>     in the <code>global-values.yaml</code> file and other examples     with your own registry, for example, <code>registry.local</code></p> <p>Notice</p> <p> Custom registry using HTTP redirect is not supported yet due to certificate verification issue. We plan to fix it in the next release.</p> </li> <li> <p>Update <code>global-values.yaml</code> file using the secrets created on the     Configuring a Custom OCI Registry for KCM components     step. For example:     <pre><code>kcm:\n  kof:\n    repo:\n      spec:\n        secretRef:\n          name: my-private-oci-registry-creds\n        certSecretRef:\n          name: registry-cert\ncert-manager-service-template:\n  repo:\n    secretRef:\n      name: my-private-oci-registry-creds\n    certSecretRef:\n      name: registry-cert\ningress-nginx-service-template:\n  repo:\n    secretRef:\n      name: my-private-oci-registry-creds\n    certSecretRef:\n      name: registry-cert\n</code></pre></p> </li> <li> <p>Either apply the Istio section or one of the options below,     required for DNS auto-config     and Manual DNS config cases only.</p> </li> <li> <p>To use self-signed certificates:</p> <ul> <li>Add to <code>global-values.yaml</code> file:     <pre><code>storage:\n  cert-manager:\n    cluster-issuer:\n      provider: self-signed\n</code></pre></li> <li>On step 9 in the Regional Cluster section apply:     <pre><code>metadata:\n  annotations:\n    k0rdent.mirantis.com/kof-http-config: '{\"tls_config\": {\"insecure_skip_verify\": true}}'\n</code></pre></li> <li>On step 7 in the Child Cluster section you may apply:     <pre><code>spec:\n  config:\n    clusterAnnotations:\n      k0rdent.mirantis.com/kof-collectors-values: |\n        kof:\n          logs:\n            tls_options:\n              insecure_skip_verify: true\n          metrics:\n            tls_options:\n              insecure_skip_verify: true\n          traces:\n            tls_options:\n              insecure_skip_verify: true\n</code></pre>     Note: <code>insecure_skip_verify</code> is a temporary workaround auto-enabled by default     until we implement a chain of trust in the next releases.</li> <li> <p>On step 1 in the Storing KOF data - From Management to Regional section you may apply:     <pre><code>cat &gt;collectors-values.yaml &lt;&lt;EOF\nkof:\n  logs:\n    endpoint: https://vmauth.$REGIONAL_DOMAIN/vli/insert/opentelemetry/v1/logs\n    tls_options:\n      insecure_skip_verify: true\n  metrics:\n    endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/insert/0/prometheus/api/v1/write\n    tls_options:\n      insecure_skip_verify: true\n  traces:\n    endpoint: https://jaeger.$REGIONAL_DOMAIN/collector\n    tls_options:\n      insecure_skip_verify: true\nopencost:\n  opencost:\n    prometheus:\n      external:\n        url: https://vmauth.$REGIONAL_DOMAIN/vm/select/0/prometheus\nEOF\n</code></pre>     Note: <code>insecure_skip_verify</code> is a temporary workaround auto-enabled by default     until we implement a chain of trust in the next releases.</p> <p>Note: <code>opencost.prometheus.external.url</code> and related values of the opencost chart do not support <code>insecure_skip_verify</code> for now.</p> </li> </ul> </li> <li> <p>To use your own <code>cert-manager</code>, add to <code>global-values.yaml</code> file:       <pre><code>storage:\n  cert-manager:\n    enabled: false\n</code></pre></p> </li> <li> <p>To use your own certificates:</p> <ul> <li>Wait until you apply the Regional Cluster section     and get the <code>regional-kubeconfig</code> in the Verifying step.</li> <li>Create the next secrets there in <code>kof</code> namespace     as documented in Ingress - TLS:<ul> <li><code>vmauth-tls</code></li> <li><code>jaeger-ingress-tls-secret</code></li> <li><code>grafana-cluster-tls</code></li> <li><code>dex-ingress-tls-secret</code></li> </ul> </li> <li>Or you can create these secrets in the management cluster     and auto-distribute them to regional clusters using the <code>clusterProfiles</code>     as documented in step 4 of the Management Cluster section.</li> </ul> </li> </ol>"},{"location":"admin/kof/kof-install/#dns-auto-config","title":"DNS auto-config","text":"<p>To avoid manual configuration of DNS records for service endpoints later, you can automate the process now using external-dns.</p>"},{"location":"admin/kof/kof-install/#aws","title":"AWS","text":"<p>For AWS in production, use the Node IAM Role or IRSA methods in production.</p> <p>Just for the sake of this demo based on the <code>aws-standalone</code> template, however, you can use the most straightforward (though less secure) static credentials method:</p> <ol> <li>Create an <code>external-dns</code> IAM user with this policy.</li> <li>Create an access key and <code>external-dns-aws-credentials</code> file, as in:     <pre><code>[default]\naws_access_key_id = &lt;EXAMPLE_ACCESS_KEY_ID&gt;\naws_secret_access_key = &lt;EXAMPLE_SECRET_ACCESS_KEY&gt;\n</code></pre></li> <li>Create the <code>external-dns-aws-credentials</code> secret in the <code>kof</code> namespace:     <pre><code>kubectl create namespace kof\nkubectl create secret generic \\\n  -n kof external-dns-aws-credentials \\\n  --from-file external-dns-aws-credentials\n</code></pre></li> </ol>"},{"location":"admin/kof/kof-install/#azure","title":"Azure","text":"<p>To enable DNS auto-config on Azure, use DNS Zone Contributor.</p> <ol> <li> <p>Create an Azure service principal with the DNS Zone Contributor permissions. You can find an example here.</p> </li> <li> <p>Create the <code>azure.json</code> text file containing the service principal configuration data:     <pre><code>{\n  \"tenantId\": \"&lt;EXAMPLE_TENANT_ID&gt;\",\n  \"subscriptionId\": \"&lt;EXAMPLE_SUBSCRIPTION_ID&gt;\",\n  \"resourceGroup\": \"&lt;EXAMPLE_RESOURCE_GROUP&gt;\",\n  \"aadClientId\": \"&lt;EXAMPLE_SP_APP_ID&gt;\",\n  \"aadClientSecret\": \"&lt;EXAMPLE_SP_PASSWORD&gt;\"\n}\n</code></pre></p> </li> <li> <p>Create the <code>external-dns-azure-credentials</code> secret in the <code>kof</code> namespace:     <pre><code>kubectl create namespace kof\nkubectl create secret generic \\\n  -n kof external-dns-azure-credentials \\\n  --from-file azure.json\n</code></pre> See external-dns Azure documentation for more details.</p> </li> </ol>"},{"location":"admin/kof/kof-install/#istio","title":"Istio","text":"<p>If you've selected to skip both DNS auto-config now and Manual DNS config later, you can apply these steps to enable the Istio service mesh:</p> <ol> <li> <p>Check the overview in the kof/docs/istio.md     and this video:</p> <p> </p> </li> <li> <p>Create and label the <code>kof</code> namespace to allow Istio to inject its sidecars:     <pre><code>kubectl create namespace kof\nkubectl label namespace kof istio-injection=enabled\n</code></pre></p> </li> <li> <p>Install the <code>kof-istio</code> chart to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait \\\n  --create-namespace -n istio-system kof-istio \\\n  -f global-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-istio --version 1.1.0\n</code></pre>     You may want to customize the collectors     by passing values to the kof-istio-child     profile for all child clusters at once now,     or for each child cluster later. You can also opt for using the default values.</p> </li> </ol> <p>Notice</p> <p> Currently the <code>kof-istio</code> chart uses <code>ClusterProfiles</code> kof-istio-regional and kof-istio-child. In the future, however, the KOF team plans to replace them with <code>MultiClusterService</code> objects in the kof-regional and kof-child charts.</p> <p>If you selected the Istio option, use <code>kof-istio</code>.  Otherwise use <code>kof-regional</code> with <code>kof-child</code>.  Do not use all 3 charts at the same time.</p>"},{"location":"admin/kof/kof-install/#management-cluster","title":"Management Cluster","text":"<p>To install KOF on the management cluster, look through the default values of the kof-mothership and kof-operators charts, and apply this example, or use it as a reference:</p> <ol> <li> <p>Install <code>kof-operators</code> as required by <code>kof-mothership</code>:     <pre><code>helm upgrade -i --reset-values --wait \\\n  --create-namespace -n kof kof-operators \\\n  -f global-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-operators --version 1.1.0\n</code></pre></p> </li> <li> <p>Create the <code>mothership-values.yaml</code> file:     <pre><code>kcm:\n  installTemplates: true\n</code></pre>     This enables installation of <code>ServiceTemplates</code> such as <code>cert-manager</code> and <code>kof-storage</code>,     making it possible to reference them from the regional and child <code>MultiClusterService</code> objects.</p> </li> <li> <p>If you want to use a default storage class,     but <code>kubectl get sc</code> shows no <code>(default)</code>, create it.     Otherwise you can use a non-default storage class in the <code>mothership-values.yaml</code> file:     <pre><code>global:\n  storageClass: &lt;EXAMPLE_STORAGE_CLASS&gt;\n</code></pre></p> <p>If <code>kubectl get sc</code> shows nothing or just <code>kubernetes.io/no-provisioner</code> in the <code>PROVISIONER</code> column, apply OpenEBS or similar.</p> </li> <li> <p>If you've applied the DNS auto-config section,     add its information to the <code>kcm:</code> object in the <code>mothership-values.yaml</code> file.</p> <p>For AWS, add:</p> <pre><code>  kof:\n    clusterProfiles:\n      kof-aws-dns-secrets:\n        matchLabels:\n          k0rdent.mirantis.com/kof-aws-dns-secrets: \"true\"\n        secrets:\n          - external-dns-aws-credentials\n</code></pre> <p>For Azure, add:</p> <pre><code>  kof:\n    clusterProfiles:\n      kof-azure-dns-secrets:\n        matchLabels:\n          k0rdent.mirantis.com/kof-azure-dns-secrets: \"true\"\n        secrets:\n          - external-dns-azure-credentials\n</code></pre> <p>This enables Sveltos to auto-distribute the DNS secret to regional clusters.</p> </li> <li> <p>Two secrets are auto-created by default:</p> <ul> <li><code>storage-vmuser-credentials</code> is a secret used by VictoriaMetrics.     You don't need to use it directly.     It is auto-distributed to other clusters by <code>clusterProfiles</code> here.</li> <li><code>grafana-admin-credentials</code> is a secret that we will use in the Grafana section.     It is auto-created here.</li> </ul> </li> <li> <p>Install <code>kof-mothership</code>:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-mothership \\\n  -f global-values.yaml \\\n  -f mothership-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-mothership --version 1.1.0\n</code></pre></p> <p>If you're upgrading from KOF version less than <code>1.1.0</code>, after upgrade please run: <pre><code>kubectl apply --server-side --force-conflicts \\\n-f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre> This is required by grafana-operator release notes.</p> <p>There is a similar step for each regional cluster on verification step 2.</p> </li> <li> <p>Wait until the value of <code>VALID</code> changes to <code>true</code> for all <code>ServiceTemplate</code> objects:     <pre><code>kubectl get svctmpl -A\n</code></pre></p> </li> <li> <p>If you have not applied the Istio section:</p> <p>Notice</p> <p> Currently the <code>kof-istio</code> chart uses <code>ClusterProfiles</code> kof-istio-regional and kof-istio-child. In the future, however, the KOF team plans to replace them with <code>MultiClusterService</code> objects in the kof-regional and kof-child charts.</p> <p>If you selected the Istio option, use <code>kof-istio</code>.  Otherwise use <code>kof-regional</code> with <code>kof-child</code>.  Do not use all 3 charts at the same time.</p> <ul> <li>Look through the <code>MultiClusterService</code> objects in the kof-regional     and kof-child charts.</li> <li>If your regional clusters already have <code>ingress-nginx</code> and <code>cert-manager</code> services,     you can ask the <code>kof-regional</code> chart to not install them by setting Helm values     <code>ingress-nginx.enabled</code> and <code>cert-manager.enabled</code> to <code>false</code>.</li> <li>You may want to customize collectors     for all child clusters at once now, or for each child cluster later, or just use the default values.</li> <li>Install these charts into the management cluster with default or custom values:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-regional \\\n  -f global-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-regional --version 1.1.0\n\nhelm upgrade -i --reset-values --wait -n kof kof-child \\\n  -f global-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-child --version 1.1.0\n</code></pre></li> </ul> </li> <li> <p>Wait for all pods to show that they're <code>Running</code>:     <pre><code>kubectl get pod -n kof\n</code></pre></p> </li> <li>Check options to store metrics of the management cluster in the Storing KOF data guide.</li> </ol>"},{"location":"admin/kof/kof-install/#regional-cluster","title":"Regional Cluster","text":"<p>To install KOF on the regional cluster, look through the default values of the kof-storage chart, and apply this example for AWS, or use it as a reference:</p> <ol> <li> <p>Set your KOF variables using your own values:     <pre><code>REGION=us-east-2\nREGIONAL_CLUSTER_NAME=aws-$REGION\nREGIONAL_DOMAIN=$REGIONAL_CLUSTER_NAME.kof.example.com\nADMIN_EMAIL=$(git config user.email)\necho \"$REGIONAL_CLUSTER_NAME, $REGIONAL_DOMAIN, $ADMIN_EMAIL\"\n</code></pre></p> </li> <li> <p>Use the up-to-date <code>ClusterTemplate</code>, as in:     <pre><code>kubectl get clustertemplate -n kcm-system | grep aws\nTEMPLATE=aws-standalone-cp-1-0-12\n</code></pre></p> </li> <li> <p>Compose the regional <code>ClusterDeployment</code>:</p> <p>For AWS:</p> <pre><code>cat &gt;regional-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $REGIONAL_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n    k0rdent.mirantis.com/kof-cluster-role: regional\nspec:\n  template: $TEMPLATE\n  credential: aws-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    clusterAnnotations:\n      k0rdent.mirantis.com/kof-regional-domain: $REGIONAL_DOMAIN\n      k0rdent.mirantis.com/kof-cert-email: $ADMIN_EMAIL\n    region: $REGION\n    controlPlaneNumber: 1\n    controlPlane:\n      instanceType: t3.large\n    workersNumber: 3\n    worker:\n      instanceType: t3.xlarge\nEOF\n</code></pre> <p>For Azure:</p> <pre><code>AZURE_SUBSCRIPTION_ID=$(az account show --query id -o tsv)\necho \"AZURE_SUBSCRIPTION_ID=$AZURE_SUBSCRIPTION_ID\"\n\ncat &gt;regional-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $REGIONAL_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n    k0rdent.mirantis.com/kof-cluster-role: regional\nspec:\n  template: $TEMPLATE\n  credential: azure-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: azure-cluster-identity\n      namespace: kcm-system\n    clusterAnnotations:\n      k0rdent.mirantis.com/kof-regional-domain: $REGIONAL_DOMAIN\n      k0rdent.mirantis.com/kof-cert-email: $ADMIN_EMAIL\n    subscriptionID: $AZURE_SUBSCRIPTION_ID\n    location: $REGION\n    controlPlaneNumber: 1\n    controlPlane:\n      vmSize: Standard_A4_v2\n    workersNumber: 3\n    worker:\n      vmSize: Standard_A4_v2\nEOF\n</code></pre> </li> <li> <p>If you've applied the DNS auto-config section,     add it to the <code>.metadata.labels</code> in the <code>regional-cluster.yaml</code> file.</p> <p>For AWS, add:</p> <pre><code>k0rdent.mirantis.com/kof-aws-dns-secrets: \"true\"\n</code></pre> <p>For Azure, add:</p> <pre><code>k0rdent.mirantis.com/kof-azure-dns-secrets: \"true\"\n</code></pre> </li> <li> <p>If you've applied the Istio section, update the <code>regional-cluster.yaml</code> file:</p> <ul> <li> <p>Replace this line:   <pre><code>k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n</code></pre>   with this line:   <pre><code>k0rdent.mirantis.com/istio-role: child\n</code></pre>   Note that <code>child</code> is not a typo   and the <code>kof-</code> prefix is not missing in <code>istio-role</code>.   These values make your configuration compatible with possible migration of Istio support   from KOF to the upstream k0rdent,   where <code>child</code> is a generic term described in the Cluster Deployments concept.</p> </li> <li> <p>Delete these lines:   <pre><code>k0rdent.mirantis.com/kof-regional-domain: $REGIONAL_DOMAIN\nk0rdent.mirantis.com/kof-cert-email: $ADMIN_EMAIL\n</code></pre></p> </li> </ul> </li> <li> <p>This <code>ClusterDeployment</code> uses propagation of its <code>.metadata.labels</code>     to the resulting <code>Cluster</code> because there are no <code>.spec.config.clusterLabels</code> here.     If you add them, please copy <code>.metadata.labels</code> as well.</p> </li> <li> <p>The <code>aws-standalone-cp</code> template provides the default storage class <code>ebs-csi-default-sc</code> for AWS. The k0rdent quickstart guide     provides the default storage class <code>managed-csi</code> for Azure.     If you want to use a non-default storage class,     add it to the <code>regional-cluster.yaml</code> file in the <code>.spec.config.clusterAnnotations</code>:     <pre><code>k0rdent.mirantis.com/kof-storage-class: &lt;EXAMPLE_STORAGE_CLASS&gt;\n</code></pre></p> </li> <li> <p>The <code>kof-operator</code> creates and configures <code>PromxyServerGroup</code> and <code>GrafanaDatasource</code> automatically.     It uses the endpoints listed below by default.     If you want to disable the built-in metrics, logs, and traces to use your own existing instances instead,     add custom endpoints to the <code>regional-cluster.yaml</code> file in the <code>.spec.config.clusterAnnotations</code>:     <pre><code>k0rdent.mirantis.com/kof-write-metrics-endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/insert/0/prometheus/api/v1/write\nk0rdent.mirantis.com/kof-read-metrics-endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/select/0/prometheus\nk0rdent.mirantis.com/kof-write-logs-endpoint: https://vmauth.$REGIONAL_DOMAIN/vls/insert/opentelemetry/v1/logs\nk0rdent.mirantis.com/kof-read-logs-endpoint: https://vmauth.$REGIONAL_DOMAIN/vls\nk0rdent.mirantis.com/kof-write-traces-endpoint: https://jaeger.$REGIONAL_DOMAIN/collector\n</code></pre></p> </li> <li> <p>If you need a custom http client configuration for <code>PromxyServerGroup</code> and <code>GrafanaDatasource</code>,     add it to the <code>regional-cluster.yaml</code> file in the <code>.metadata.annotations</code>. For example:     <pre><code>k0rdent.mirantis.com/kof-http-config: '{\"dial_timeout\": \"10s\", \"tls_config\": {\"insecure_skip_verify\": true}}'\n</code></pre></p> </li> <li> <p>The <code>MultiClusterService</code> named kof-regional-cluster     configures and installs <code>cert-manager</code>, <code>ingress-nginx</code>, <code>kof-storage</code>, <code>kof-operators</code>, and <code>kof-collectors</code> charts automatically.     To pass any custom values to the <code>kof-storage</code> chart     or its subcharts, such as victoria-logs-cluster,     add them to the <code>regional-cluster.yaml</code> file in the <code>.spec.config.clusterAnnotations</code>. For example:     <pre><code>k0rdent.mirantis.com/kof-storage-values: |\n  victoria-logs-cluster:\n    vlinsert:\n      replicaCount: 2\n</code></pre></p> </li> <li> <p>Verify and apply the Regional <code>ClusterDeployment</code>:     <pre><code>cat regional-cluster.yaml\n\nkubectl apply -f regional-cluster.yaml\n</code></pre></p> </li> <li> <p>Watch how the cluster is deployed until all values of <code>READY</code> are <code>True</code>:     <pre><code>clusterctl describe cluster -n kcm-system $REGIONAL_CLUSTER_NAME \\\n  --show-conditions all\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-install/#child-cluster","title":"Child Cluster","text":"<p>To install KOF on the actual cluster to be monitored, look through the default values of the kof-operators and kof-collectors charts, and apply this example for AWS, or use it as a reference:</p> <ol> <li> <p>Set your own value below, verifing the variables:     <pre><code>CHILD_CLUSTER_NAME=$REGIONAL_CLUSTER_NAME-child1\necho \"$CHILD_CLUSTER_NAME, $REGIONAL_DOMAIN\"\n</code></pre></p> </li> <li> <p>Use the up-to-date <code>ClusterTemplate</code>, as in:     <pre><code>kubectl get clustertemplate -n kcm-system | grep aws\nTEMPLATE=aws-standalone-cp-1-0-12\n</code></pre></p> </li> <li> <p>Compose the child <code>ClusterDeployment</code>:</p> <p>For AWS:</p> <pre><code>cat &gt;child-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $CHILD_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n    k0rdent.mirantis.com/kof-cluster-role: child\nspec:\n  template: $TEMPLATE\n  credential: aws-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    region: $REGION\n    controlPlaneNumber: 1\n    controlPlane:\n      instanceType: t3.large\n    workersNumber: 3\n    worker:\n      instanceType: t3.medium\nEOF\n</code></pre> <p>For Azure:</p> <pre><code>AZURE_SUBSCRIPTION_ID=$(az account show --query id -o tsv)\necho \"AZURE_SUBSCRIPTION_ID=$AZURE_SUBSCRIPTION_ID\"\n\ncat &gt;child-cluster.yaml &lt;&lt;EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $CHILD_CLUSTER_NAME\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n    k0rdent.mirantis.com/kof-cluster-role: child\nspec:\n  template: $TEMPLATE\n  credential: azure-cluster-identity-cred\n  config:\n    clusterIdentity:\n      name: azure-cluster-identity\n      namespace: kcm-system\n    subscriptionID: $AZURE_SUBSCRIPTION_ID\n    location: $REGION\n    controlPlaneNumber: 1\n    controlPlane:\n      vmSize: Standard_A4_v2\n    workersNumber: 3\n    worker:\n      vmSize: Standard_A4_v2\nEOF\n</code></pre> </li> <li> <p>If you've applied the Istio section, update the <code>child-cluster.yaml</code> file:</p> <p>replace this line: <pre><code>k0rdent.mirantis.com/kof-storage-secrets: \"true\"\n</code></pre> with this line: <pre><code>k0rdent.mirantis.com/istio-role: child\n</code></pre></p> </li> <li> <p>This <code>ClusterDeployment</code> uses propagation of its <code>.metadata.labels</code>     to the resulting <code>Cluster</code> because there are no <code>.spec.config.clusterLabels</code> here.     If you add them, please copy <code>.metadata.labels</code> as well.</p> </li> <li> <p>The <code>kof-operator</code> discovers     the regional cluster by the location of the child cluster.     If you have more than one regional cluster in the same AWS region / Azure location / etc.,     and you want to connect the child cluster to specific regional cluster,     add this regional cluster name to the <code>child-cluster.yaml</code> file in the <code>.metadata.labels</code>:     <pre><code>k0rdent.mirantis.com/kof-regional-cluster-name: $REGIONAL_CLUSTER_NAME\n</code></pre></p> </li> <li> <p>The <code>MultiClusterService</code> named kof-child-cluster     configures and installs <code>cert-manager</code>, <code>kof-operators</code>, and <code>kof-collectors</code> charts automatically.     To pass any custom values to the <code>kof-collectors</code> chart     or its subcharts, such as opencost,     add them to the <code>child-cluster.yaml</code> file in the <code>.spec.config</code>. For example:     <pre><code>clusterAnnotations:\n  k0rdent.mirantis.com/kof-collectors-values: |\n    opencost:\n      opencost:\n        exporter:\n          replicas: 2\n</code></pre>     Note: the first <code>opencost</code> key is to reference the subchart,     and the second <code>opencost</code> key is part of its values.</p> </li> <li> <p>Verify and apply the <code>ClusterDeployment</code>:     <pre><code>cat child-cluster.yaml\n\nkubectl apply -f child-cluster.yaml\n</code></pre></p> </li> <li> <p>Watch while the cluster is deployed until all values of <code>READY</code> are <code>True</code>:     <pre><code>clusterctl describe cluster -n kcm-system $CHILD_CLUSTER_NAME \\\n  --show-conditions all\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-limits/","title":"Resource Limits","text":"<p>See also: System Requirements.</p>"},{"location":"admin/kof/kof-limits/#resources-of-management-cluster","title":"Resources of Management Cluster","text":"<ul> <li> <p>promxy:   <pre><code>resources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    cpu: 100m\n    memory: 128Mi\n</code></pre></p> </li> <li> <p>promxy-deployment:   <pre><code>resources:\n  requests:\n    cpu: 0.02\n    memory: 20Mi\n  limits:\n    cpu: 0.02\n    memory: 20Mi\n</code></pre></p> </li> <li> <p>promxy-operator:   <pre><code>resources:\n  limits:\n    cpu: 500m\n    memory: 128Mi\n  requests:\n    cpu: 10m\n    memory: 64Mi\n</code></pre></p> </li> </ul>"},{"location":"admin/kof/kof-limits/#resources-of-a-child-cluster","title":"Resources of a Child Cluster","text":"<ul> <li>opentelemetry:   <pre><code>resourceRequirements:\n  limits:\n    memory: 128Mi\n  requests:\n    memory: 128Mi\n</code></pre></li> </ul>"},{"location":"admin/kof/kof-maintainence/","title":"Maintaining KOF","text":""},{"location":"admin/kof/kof-maintainence/#backup-requirements","title":"Backup Requirements","text":"<p>Backing up KOF requires backing up the following:</p> <ul> <li>Grafana configurations</li> <li>Alert definitions</li> <li>Custom dashboards</li> <li>Retention policies</li> </ul>"},{"location":"admin/kof/kof-maintainence/#health-monitoring","title":"Health Monitoring","text":"<p>To implement health monitoring:</p> <ol> <li>Apply the steps in the Verification section</li> <li>Apply the steps in the Sveltos section</li> </ol>"},{"location":"admin/kof/kof-maintainence/#uninstallation","title":"Uninstallation","text":"<p>To remove the demo clusters created in this section:</p> <p>Warning</p> <p> Make sure these are just your demo clusters and do not contain important data.</p> <pre><code>kubectl delete --wait --cascade=foreground -f child-cluster.yaml\nkubectl delete --wait --cascade=foreground -f regional-cluster.yaml\nkubectl delete --wait promxyservergroup \\\n  -n kof -l app.kubernetes.io/managed-by=kof-operator\nkubectl delete --wait grafanadatasource \\\n  -n kof -l app.kubernetes.io/managed-by=kof-operator\n</code></pre> <p>To remove KOF from the management cluster:</p> <pre><code>helm uninstall --wait --cascade foreground -n istio-system kof-istio\nhelm uninstall --wait --cascade foreground -n kof kof-child\nhelm uninstall --wait --cascade foreground -n kof kof-regional\nhelm uninstall --wait --cascade foreground -n kof kof-collectors\nhelm uninstall --wait --cascade foreground -n kof kof-storage\nhelm uninstall --wait --cascade foreground -n kof kof-mothership\nhelm uninstall --wait --cascade foreground -n kof kof-operators\nkubectl delete namespace kof --wait --cascade=foreground\n</code></pre>"},{"location":"admin/kof/kof-scaling/","title":"Scaling Guidelines","text":"<p>The method for scaling KOF depends on the type of expansion:</p>"},{"location":"admin/kof/kof-scaling/#regional-expansion","title":"Regional Expansion","text":"<ol> <li>Deploy a regional cluster in the new region.</li> <li>Configure child clusters in this region to point to this regional cluster.</li> </ol>"},{"location":"admin/kof/kof-scaling/#adding-a-new-child-cluster","title":"Adding a New Child Cluster","text":"<ol> <li>Apply templates, as in the child cluster section.</li> <li>Verify the data flow.</li> <li>Configure any custom dashboards.</li> </ol>"},{"location":"admin/kof/kof-scaling/#you-must-construct-additional-pylons","title":"You Must Construct Additional Pylons","text":"<ol> <li>Change the <code>replicaCount</code> of components like <code>victoria-logs-cluster</code>     as documented in the regional cluster section.</li> <li>Change the <code>replicas</code> number of components like <code>opencost</code>     as documented in the child cluster section.</li> </ol>"},{"location":"admin/kof/kof-storing/","title":"Storing KOF data","text":""},{"location":"admin/kof/kof-storing/#overview","title":"Overview","text":"<p>KOF data (metrics, logs, traces) can be collected from each cluster and stored in specific places:</p> <pre><code>sequenceDiagram\n    Child cluster-&gt;&gt;Regional cluster: KOF data of the&lt;br&gt;child cluster&lt;br&gt;is stored in the&lt;br&gt;regional cluster.\n    Regional cluster-&gt;&gt;Regional cluster: KOF data of the&lt;br&gt;regional cluster&lt;br&gt;is stored in the same&lt;br&gt;regional cluster.\n    Management cluster-&gt;&gt;Management cluster: KOF data of the&lt;br&gt;management cluster&lt;br&gt;can be stored in:&lt;br&gt;&lt;br&gt;the same management cluster,\n    Management cluster-&gt;&gt;Regional cluster: the regional cluster,\n    Management cluster-&gt;&gt;Third-party storage: a third-party storage,&lt;br&gt;e.g. AWS CloudWatch.</code></pre>"},{"location":"admin/kof/kof-storing/#from-child-and-regional","title":"From Child and Regional","text":"<p>KOF data collected from the child and regional clusters is routed out-of-the box. No additional steps are required here.</p>"},{"location":"admin/kof/kof-storing/#from-management-to-management","title":"From Management to Management","text":"<p>This option stores KOF data of the management cluster in the same management cluster.</p> <ul> <li>Grafana and VictoriaMetrics are provided by the <code>kof-mothership</code> chart,   hence disabled in the <code>kof-storage</code> chart.</li> <li>PromxyServerGroup, VictoriaLogs, and Jaeger are provided by the <code>kof-storage</code> chart.</li> </ul> <p>To apply this option:</p> <ol> <li> <p>Create the <code>storage-values.yaml</code> file:     <pre><code>grafana:\n  enabled: false\n  security:\n    create_secret: false\nvictoria-metrics-operator:\n  enabled: false\nvictoriametrics:\n  enabled: false\npromxy:\n  enabled: true\n</code></pre></p> <p>If you want to use a non-default storage class, add to the <code>storage-values.yaml</code> file: <pre><code>victoria-logs-cluster:\n  vlstorage:\n    persistentVolume:\n      storageClassName: &lt;EXAMPLE_STORAGE_CLASS&gt;\n</code></pre></p> </li> <li> <p>Create the <code>collectors-values.yaml</code> file:     <pre><code>kcm:\n  monitoring: true\n</code></pre></p> </li> <li> <p>Install the <code>kof-storage</code> and <code>kof-collectors</code> charts to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-storage \\\n  -f global-values.yaml \\\n  -f storage-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-storage --version 1.1.0\n\nhelm upgrade -i --reset-values --wait -n kof kof-collectors \\\n  -f global-values.yaml \\\n  -f collectors-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-collectors --version 1.1.0\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-storing/#from-management-to-regional","title":"From Management to Regional","text":"<p>This option stores KOF data of the management cluster in the regional cluster.</p> <p>It assumes that:</p> <ul> <li>You did not enable Istio.</li> <li>You have a regional cluster with the <code>REGIONAL_DOMAIN</code> configured here.</li> </ul> <p>To apply this option:</p> <ol> <li> <p>Create the <code>collectors-values.yaml</code> file:     <pre><code>cat &gt;collectors-values.yaml &lt;&lt;EOF\nkcm:\n  monitoring: true\nkof:\n  logs:\n    endpoint: https://vmauth.$REGIONAL_DOMAIN/vli/insert/opentelemetry/v1/logs\n  metrics:\n    endpoint: https://vmauth.$REGIONAL_DOMAIN/vm/insert/0/prometheus/api/v1/write\n  traces:\n    endpoint: https://jaeger.$REGIONAL_DOMAIN/collector\nopencost:\n  opencost:\n    prometheus:\n      external:\n        url: https://vmauth.$REGIONAL_DOMAIN/vm/select/0/prometheus\nEOF\n</code></pre></p> </li> <li> <p>Install the <code>kof-collectors</code> chart to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-collectors \\\n  -f global-values.yaml \\\n  -f collectors-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-collectors --version 1.1.0\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-storing/#from-management-to-regional-with-istio","title":"From Management to Regional with Istio","text":"<p>This option stores KOF data of the management cluster in the regional cluster using Istio.</p> <p>It assumes that:</p> <ul> <li>You have Istio enabled.</li> <li>You have a regional cluster with the <code>REGIONAL_CLUSTER_NAME</code> configured here.</li> </ul> <p>To apply this option:</p> <ol> <li> <p>Create the <code>collectors-values.yaml</code> file:     <pre><code>cat &gt;collectors-values.yaml &lt;&lt;EOF\nkcm:\n  monitoring: true\nkof:\n  basic_auth: false\n  logs:\n    endpoint: http://$REGIONAL_CLUSTER_NAME-logs-insert:9481/insert/opentelemetry/v1/logs\n  metrics:\n    endpoint: http://$REGIONAL_CLUSTER_NAME-vminsert:8480/insert/0/prometheus/api/v1/write\n  traces:\n    endpoint: http://$REGIONAL_CLUSTER_NAME-jaeger-collector:4318\nopencost:\n  opencost:\n    prometheus:\n      existingSecretName: \"\"\n      external:\n        url: http://$REGIONAL_CLUSTER_NAME-vmselect:8481/select/0/prometheus\nEOF\n</code></pre></p> </li> <li> <p>Install the <code>kof-collectors</code> chart to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-collectors \\\n  -f global-values.yaml \\\n  -f collectors-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-collectors --version 1.1.0\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-storing/#from-management-to-third-party","title":"From Management to Third-party","text":"<p>This option stores KOF data of the management cluster in a third-party storage, using the AWS CloudWatch Logs Exporter as an example.</p> <p>Use the most secure option to specify AWS credentials in production.</p> <p>For now, however, just for the sake of this demo, you can use the most straightforward (though less secure) static credentials method:</p> <ol> <li> <p>Create AWS IAM user with access to CloudWatch Logs,     for example, with <code>\"Action\": \"logs:*\"</code> allowed in the inline policy.</p> </li> <li> <p>Create access key and save it to the <code>cloudwatch-credentials</code> file:     <pre><code>AWS_ACCESS_KEY_ID=REDACTED\nAWS_SECRET_ACCESS_KEY=REDACTED\n</code></pre></p> </li> <li> <p>Create the <code>cloudwatch-credentials</code> secret:     <pre><code>kubectl create secret generic -n kof cloudwatch-credentials \\\n  --from-env-file=cloudwatch-credentials\n</code></pre></p> </li> <li> <p>Create the <code>collectors-values.yaml</code> file:     <pre><code>COLLECTOR_CONFIG=\"\n    env:\n      - name: AWS_ACCESS_KEY_ID\n        valueFrom:\n          secretKeyRef:\n            name: cloudwatch-credentials\n            key: AWS_ACCESS_KEY_ID\n      - name: AWS_SECRET_ACCESS_KEY\n        valueFrom:\n          secretKeyRef:\n            name: cloudwatch-credentials\n            key: AWS_SECRET_ACCESS_KEY\n    exporters:\n      awscloudwatchlogs:\n        region: us-east-2\n        log_group_name: management\n        log_stream_name: logs\"\n\ncat &gt;collectors-values.yaml &lt;&lt;EOF\nkcm:\n  monitoring: true\nkof:\n  logs:\n    endpoint: \"\"\n  metrics:\n    endpoint: \"\"\n  traces:\n    endpoint: \"\"\ncollectors:\n  k8scluster:$COLLECTOR_CONFIG\n    service:\n      pipelines:\n        logs:\n          exporters:\n            - awscloudwatchlogs\n            - debug\n        metrics:\n          exporters:\n            - debug\n  node:$COLLECTOR_CONFIG\n    service:\n      pipelines:\n        logs:\n          exporters:\n            - awscloudwatchlogs\n            - debug\n        metrics:\n          exporters:\n            - debug\n        traces:\n          exporters:\n            - debug\nEOF\n</code></pre></p> </li> <li> <p>Install the <code>kof-collectors</code> chart to the management cluster:     <pre><code>helm upgrade -i --reset-values --wait -n kof kof-collectors \\\n  -f global-values.yaml \\\n  -f collectors-values.yaml \\\n  oci://registry.mirantis.com/k0rdent-enterprise/charts/kof-collectors --version 1.1.0\n</code></pre></p> </li> <li> <p>Configure AWS CLI with the same access key, for verification:     <pre><code>aws configure\n</code></pre></p> </li> <li> <p>Verify that the management cluster logs are stored in the CloudWatch:     <pre><code>aws logs get-log-events \\\n  --region us-east-2 \\\n  --log-group-name management \\\n  --log-stream-name logs \\\n  --limit 1\n</code></pre>     Example of the output:     <pre><code>{\"events\": [{\n  \"timestamp\": 1744305535107,\n  \"message\": \"{\\\"body\\\":\\\"10.244.0.1 - - [10/Apr/2025 17:18:55] \\\\\\\"GET /-/ready HTTP/1.1 200 ...\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-using/","title":"Using KOF","text":"<p>Most of the time, you'll access KOF's data through Grafana.</p>"},{"location":"admin/kof/kof-using/#access-to-grafana","title":"Access to Grafana","text":"<p>To make Grafana available, start with these steps:</p> <ol> <li> <p>Get the Grafana username and password:     <pre><code>kubectl get secret -n kof grafana-admin-credentials -o yaml | yq '{\n  \"user\": .data.GF_SECURITY_ADMIN_USER | @base64d,\n  \"pass\": .data.GF_SECURITY_ADMIN_PASSWORD | @base64d\n}'\n</code></pre></p> </li> <li> <p>Forward a port to the Grafana dashboard:     <pre><code>kubectl port-forward -n kof svc/grafana-vm-service 3000:3000\n</code></pre></p> </li> <li> <p>Login to http://127.0.0.1:3000/dashboards with the username/password printed above.</p> </li> <li> <p>Open a dashboard and select any cluster:</p> </li> </ol> <p></p> <p></p>"},{"location":"admin/kof/kof-using/#single-sign-on","title":"Single Sign-On","text":"<p>Port forwarding, as described above, is a quick solution.</p> <p>Single Single-On provides better experience. If you want to enable it, please apply this advanced guide: SSO for Grafana.</p>"},{"location":"admin/kof/kof-using/#cluster-overview","title":"Cluster Overview","text":"<p>From here you can get an overview of the cluster, including:</p> <ul> <li>Health metrics</li> <li>Resource utilization</li> <li>Performance trends</li> <li>Cost analysis</li> </ul>"},{"location":"admin/kof/kof-using/#logging-interface","title":"Logging Interface","text":"<p>The logging interface will also be available, including:</p> <ul> <li>Real-time log streaming</li> <li>Full-text search</li> <li>Log aggregation</li> <li>Alert correlation</li> </ul>"},{"location":"admin/kof/kof-using/#cost-management","title":"Cost Management","text":"<p>Finally there are the cost management features, including:</p> <ul> <li>Resource cost tracking</li> <li>Usage analysis</li> <li>Budget monitoring</li> <li>Optimization recommendations</li> </ul>"},{"location":"admin/kof/kof-using/#access-to-jaeger","title":"Access to Jaeger","text":"<p>Jaeger UI of each regional cluster can be accessed by following these steps:</p> <ol> <li> <p>Ensure you have the <code>regional-kubeconfig</code> file created on the verification step.</p> </li> <li> <p>If you've applied the Istio section:</p> <ul> <li> <p>Forward a port to the Jaeger UI:     <pre><code>KUBECONFIG=regional-kubeconfig kubectl port-forward \\\n  -n kof svc/kof-storage-jaeger-query 16686:16686\n</code></pre></p> </li> <li> <p>Open the link http://127.0.0.1:16686/search   and explore the Jaeger UI.</p> </li> </ul> </li> <li> <p>If you have not applied the Istio section:</p> <ul> <li> <p>Ensure you have the <code>REGIONAL_DOMAIN</code> variable set on the installation step.</p> </li> <li> <p>Get the regional Jaeger username and password:     <pre><code>KUBECONFIG=regional-kubeconfig kubectl get secret \\\n  -n kof jaeger-credentials -o yaml | yq '{\n  \"user\": .data.username | @base64d,\n  \"pass\": .data.password | @base64d\n}'\n</code></pre></p> </li> <li> <p>Get the the Jaeger UI URL, open it,     and login with the username/password printed above:     <pre><code>echo https://jaeger.$REGIONAL_DOMAIN\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"admin/kof/kof-using/#access-to-the-kof-ui","title":"Access to the KOF UI","text":"<p>When the TargetAllocator is in use, the configuration of OpenTelemetryCollectors Prometheus receivers is distributed across the cluster.</p> <p>The KOF UI collects metrics metadata from the same endpoints that are scraped by the Prometheus server:</p> <pre><code>graph TB\n    KOF_UI[KOF UI] --&gt; C1OTC11\n    KOF_UI --&gt; C1OTC1N\n    KOF_UI --&gt; C1OTC21\n    KOF_UI --&gt; C1OTC2N\n    KOF_UI --&gt; C2OTC11\n    KOF_UI --&gt; C2OTC1N\n    KOF_UI --&gt; C2OTC21\n    KOF_UI --&gt; C2OTC2N\n    subgraph Cluster1\n    subgraph C1Node1[Node 1]\n        C1OTC11[OTel Collector]\n        C1OTC1N[OTel Collector]\n    end\n    subgraph C1NodeN[Node N]\n        C1OTC21[OTel Collector]\n        C1OTC2N[OTel Collector]\n    end\n\n    C1OTC11 --PrometheusReceiver--&gt; C1TA[TargetAllocator]\n    C1OTC1N --PrometheusReceiver--&gt; C1TA\n    C1OTC21 --PrometheusReceiver--&gt; C1TA\n    C1OTC2N --PrometheusReceiver--&gt; C1TA\n    end\n    subgraph Cluster2\n    subgraph C2Node1[Node 1]\n        C2OTC11[OTel Collector]\n        C2OTC1N[OTel Collector]\n    end\n    subgraph C2NodeN[Node N]\n        C2OTC21[OTel Collector]\n        C2OTC2N[OTel Collector]\n    end\n\n    C2OTC11 --PrometheusReceiver--&gt; C2TA[TargetAllocator]\n    C2OTC1N --PrometheusReceiver--&gt; C2TA\n    C2OTC21 --PrometheusReceiver--&gt; C2TA\n    C2OTC2N --PrometheusReceiver--&gt; C2TA\n    end</code></pre> <p>You can access the KOF UI by following these steps:</p> <ol> <li> <p>Forward a port to the KOF UI:</p> <pre><code>kubectl port-forward -n kof deploy/kof-mothership-kof-operator 9090:9090\n</code></pre> </li> <li> <p>Open the link http://127.0.0.1:9090</p> </li> <li> <p>Check the state of the endpoints:</p> </li> </ol> <p></p>"},{"location":"admin/kof/kof-verification/","title":"KOF Verification","text":"<p>Finally, verify that KOF installed properly.</p>"},{"location":"admin/kof/kof-verification/#verification-steps","title":"Verification steps","text":"<ol> <li> <p>Wait until the value of <code>HELMCHARTS</code> and <code>POLICYREFS</code>     changes from <code>Provisioning</code> to <code>Provisioned</code>:     <pre><code>kubectl get clustersummaries -A -o wide\n</code></pre>     If you see the <code>Failed/Provisioning</code> loop, check status and logs:     <pre><code>kubectl get clustersummaries -A -o yaml \\\n  | yq '.items[].status.featureSummaries[]\n  | select(.status != \"Provisioned\")'\n\nkubectl logs -n kof deploy/kof-mothership-kof-operator\n</code></pre></p> </li> <li> <p>Wait for all pods in the regional and child clusters to show as <code>Running</code>     in the namespaces <code>kof, kube-system, projectsveltos</code>:     <pre><code>kubectl get secret -n kcm-system $REGIONAL_CLUSTER_NAME-kubeconfig \\\n  -o=jsonpath={.data.value} | base64 -d &gt; regional-kubeconfig\n\nkubectl get secret -n kcm-system $CHILD_CLUSTER_NAME-kubeconfig \\\n  -o=jsonpath={.data.value} | base64 -d &gt; child-kubeconfig\n\nKUBECONFIG=regional-kubeconfig kubectl get pod -A\nKUBECONFIG=child-kubeconfig kubectl get pod -A\n</code></pre>     If you're upgrading from a KOF version less than <code>1.1.0</code>, after upgrade please run the following:     <pre><code>KUBECONFIG=regional-kubeconfig kubectl apply --server-side --force-conflicts \\\n-f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre>     This is noted as required in the grafana-operator release notes.</p> </li> <li> <p>Wait until the value of <code>READY</code> changes to <code>True</code>     for all certificates in the regional cluster:     <pre><code>KUBECONFIG=regional-kubeconfig kubectl get cert -n kof\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-verification/#manual-dns-config","title":"Manual DNS config","text":"<p>If you've opted out of DNS auto-config and Istio, you will need to do the following:</p> <ol> <li> <p>Get the <code>EXTERNAL-IP</code> of <code>ingress-nginx</code>:     <pre><code>KUBECONFIG=regional-kubeconfig kubectl get svc \\\n  -n kof ingress-nginx-controller\n</code></pre>     It should look like <code>REDACTED.us-east-2.elb.amazonaws.com</code></p> </li> <li> <p>Create these DNS records of type <code>A</code>, all pointing to that <code>EXTERNAL-IP</code>:     <pre><code>echo grafana.$REGIONAL_DOMAIN\necho jaeger.$REGIONAL_DOMAIN\necho vmauth.$REGIONAL_DOMAIN\n</code></pre></p> </li> </ol>"},{"location":"admin/kof/kof-verification/#sveltos","title":"Sveltos","text":"<p>Use the Sveltos dashboard to verify secrets have been auto-distributed to the required clusters:</p> <ol> <li> <p>Start by preparing the system:</p> <pre><code>kubectl create sa platform-admin\nkubectl create clusterrolebinding platform-admin-access \\\n  --clusterrole cluster-admin --serviceaccount default:platform-admin\n\nkubectl create token platform-admin --duration=24h\nkubectl port-forward -n kof svc/dashboard 8081:80\n</code></pre> </li> <li> <p>Now open http://127.0.0.1:8081/login and paste the token output in step 1 above.</p> </li> <li>Open the <code>ClusterAPI</code> tab: http://127.0.0.1:8081/sveltos/clusters/ClusterAPI/1</li> <li>Check both regional and child clusters:<ul> <li>Cluster profiles should be <code>Provisioned</code>.</li> <li>Secrets should be distributed.</li> </ul> </li> </ol> <p></p>"},{"location":"admin/kof/kof-version-compat/","title":"Version Compatibility","text":"Component Version Notes Mirantis k0rdent Enterprise \u2265 0.0.7 Required for template support Kubernetes \u2265 1.32 Earlier versions untested OpenTelemetry \u2265 0.75 Recommended minimum VictoriaMetrics \u2265 0.40 Required for clustering <p>Detailed:</p> <ul> <li>kof-mothership</li> <li>kof-storage</li> <li>kof-operators</li> <li>kof-collectors</li> </ul>"},{"location":"admin/services/","title":"Working With Services","text":"<p>Mirantis k0rdent Enterprise considers applications to be \"services\".  These services may be traditional services, such as Nginx Ingress, or they can be user-created applications. To deploy these applications, create a  <code>ServiceTemplate</code> and deploy an instance of it.</p> <ul> <li>Using and creating service templates</li> <li>Creating multi-cluster services</li> </ul>"},{"location":"admin/services/admin-create-multiclusterservice/","title":"Deploy services using MultiClusterService","text":"<p>The <code>MultiClusterService</code> object is used to deploy services on multiple matching clusters.</p>"},{"location":"admin/services/admin-create-multiclusterservice/#creation","title":"Creation","text":"<p>You can create the <code>MultiClusterService</code> object with the following YAML:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\nmetadata:\n  name: &lt;name&gt;\nspec:\n  clusterSelector:\n    matchLabels:\n      &lt;key1&gt;: &lt;value1&gt;\n      &lt;key2&gt;: &lt;value2&gt;\n      . . .\n  serviceSpec:\n    services:\n    - template: &lt;servicetemplate-1-name&gt;\n      name: &lt;release-name&gt;\n      namespace: &lt;release-namespace&gt;\n    priority: 100\n</code></pre>"},{"location":"admin/services/admin-create-multiclusterservice/#matching-multiple-clusters","title":"Matching Multiple Clusters","text":"<p>Consider the following example where two clusters have been deployed using <code>ClusterDeployment</code> objects:</p> <p>Command:</p> <p><pre><code>kubectl get clusterdeployments.k0rdent.mirantis.com -n kcm-system\n</code></pre> <pre><code>NAME             READY   STATUS\ndev-cluster-1   True    ClusterDeployment is ready\ndev-cluster-2   True    ClusterDeployment is ready\n</code></pre></p> <p>Command: <pre><code> kubectl get cluster -n kcm-system --show-labels\n</code></pre> <pre><code>NAME           CLUSTERCLASS     PHASE         AGE     VERSION   LABELS\ndev-cluster-1                  Provisioned   2h41m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-1,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\ndev-cluster-2                  Provisioned   3h10m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-2,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\n</code></pre></p> <p>The <code>dev-cluster-1</code> <code>ClusterDeployment</code> services are specified as: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: dev-cluster-1\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 100\n  . . .\n</code></pre></p> <p>The <code>dev-cluster-2</code> <code>ClusterDeployment</code> beach-head services are specified as: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: dev-cluster-2\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 500\n  . . .\n</code></pre></p> <p>Note</p> <p> See Deploy beach-head Services using Cluster Deployment for how to use beach-head services with ClusterDeployment.</p> <p>Now create the following <code>global-ingress</code> <code>MultiClusterService</code> object: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\nmetadata:\n  name: global-ingress\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    priority: 300\n</code></pre></p> <p>This MultiClusterService will match any CAPI cluster with the label <code>app.kubernetes.io/managed-by: Helm</code> and deploy chart version 4.11.3 of ingress-nginx service on it.</p>"},{"location":"admin/services/admin-create-multiclusterservice/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Refer to \"Configuring Custom Values\" in Deploy beach-head Services using Cluster Deployment for more information on using custom values.</p>"},{"location":"admin/services/admin-create-multiclusterservice/#templating-custom-values","title":"Templating Custom Values","text":"<p>Refer to \"Templating Custom Values\" in Deploy beach-head Services using Cluster Deployment for more information dynamic custom values.</p>"},{"location":"admin/services/admin-create-multiclusterservice/#services-priority-and-conflict","title":"Services Priority and Conflict","text":"<p>The <code>.spec.serviceSpec.priority</code> field specifies the priority for the services managed by a ClusterDeployment or MultiClusterService object.</p> <p>Considering the example above:</p> <ol> <li>ClusterDeployment <code>dev-cluster-1</code> manages deployment of kyverno (v3.2.6) and ingress-nginx (v4.11.0) with <code>priority=100</code> on its cluster.</li> <li>ClusterDeployment <code>dev-cluster-2</code> manages deployment of ingress-nginx (v4.11.0) with <code>priority=500</code> on its cluster.</li> <li>MultiClusterService <code>global-ingress</code> manages deployment of ingress-nginx (v4.11.3) with <code>priority=300</code> on both clusters. This scenario presents a conflict on both the clusters as the MultiClusterService is attempting to deploy v4.11.3 of ingress-nginx on both whereas the ClusterDeployment for each is attempting to deploy v4.11.0 of ingress-nginx.</li> </ol> <p>This is where <code>.spec.serviceSpec.priority</code> can be used to specify who gets the priority. Higher number means higer priority and vice versa. In this example:</p> <ol> <li>MultiClusterService \"global-ingress\" will take precedence over ClusterDeployment \"dev-cluster-1\" and ingress-nginx (v4.11.3) defined in MultiClusterService object will be deployed on the cluster.</li> <li>ClusterDeployment \"dev-cluster-2\" will take precedence over MultiClusterService \"global-ingress\" and ingress-nginx (v4.11.0) defined in ClusterDeployment object will be deployed on the cluster.</li> </ol> <p>Note</p> <p>If <code>priority</code> values are equal, the first one to reach the cluster wins and deploys its beach-head services.</p>"},{"location":"admin/services/admin-create-multiclusterservice/#checking-status","title":"Checking Status","text":"<p>The status for the <code>MultiClusterService</code> object shows the deployment status for the beach-head services managed by it on each of the CAPI target clusters that it matches. Consider the same example where 2 ClusterDeployments and 1 MultiClusterService is deployed. The status for the <code>global-ingress</code> <code>MultiClusterService</code> appears as:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: MultiClusterService\nmetadata:\n  . . .\n  name: global-ingress\n  resourceVersion: \"38146\"\n  . . .\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    . . .\n  . . .\nstatus:\n  conditions:\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: \"\"\n    reason: Succeeded\n    status: \"True\"\n    type: SveltosClusterProfileReady\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: MultiClusterService is ready\n    reason: Succeeded\n    status: \"True\"\n    type: Ready\n  observedGeneration: 1\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary p--dev-cluster-2-capi-dev-cluster-2 managing it.\n      reason: Failed\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary p--dev-cluster-2-capi-dev-cluster-2\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>The status under <code>.status.services</code> shows a conflict for <code>dev-cluster-2</code> as expected because the <code>MultiClusterService</code> has a lower priority. On the other hand, it shows provisioned for <code>dev-cluster-1</code> because the <code>MultiClusterService</code> has a higher priority.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . . \n  name: dev-cluster-1\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 100\n    . . .\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary global-ingress-capi-dev-cluster-1 managing it.\n      reason: Provisioning\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T07:44:43Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary global-ingress-capi-dev-cluster-1\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>The status under <code>.status.services</code> for the <code>ClusterDeployment</code> <code>dev-cluster-1</code> shows that it is managing Kyverno but unable to manage ingress-nginx because another object with higher priority is managing it, so it shows a conflict instead.</p> <p>On the otherhand, the <code>dev-cluster-2</code> <code>ClusterDeployment</code> has a higher priority: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  name: dev-cluster-2\n  namespace: kcm-system\n  resourceVersion: \"30889\"\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-0\n    priority: 500\n    . . .\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>The status under <code>.status.services</code> for <code>ClusterDeployment</code> <code>dev-cluster-2</code> shows that it is managing ingress-nginx, as expected because it has a higher priority.</p>"},{"location":"admin/services/admin-create-multiclusterservice/#parameter-list","title":"Parameter List","text":"<p>Refer to \"Parameter List\" in Deploy beach-head Services using Cluster Deployment for more information.</p>"},{"location":"admin/services/admin-service-templates/","title":"Using and Creating ServiceTemplates: Creating and Deploying Applications","text":"<p>Deploying an application, like deploying a cluster, involves applying a template to the management cluster. Rather than a <code>ClusterTemplate</code>, however, applications are deployed using a <code>ServiceTemplate</code>.</p> <p>You can find more information on Bringing Your Own Templates in the Template Guide, but this section gives you an idea of how to create a <code>ServiceTemplate</code> and use it to deploy an application to a Mirantis k0rdent Enterprise child cluster.</p> <p><code>ServiceTemplate</code> supports the following types as a source:</p> <ul> <li><code>HelmChart</code></li> <li><code>GitRepository</code></li> <li><code>Bucket</code></li> <li><code>OCIRepository</code></li> <li><code>Secret</code></li> <li><code>ConfigMap</code></li> </ul>"},{"location":"admin/services/admin-service-templates/#creating-helm-based-servicetemplate","title":"Creating Helm-based ServiceTemplate","text":"<p>Note</p> <p> Only <code>HelmRepository</code> support as a source for <code>HelmChart</code>. Define the source of the Helm chart that defines the service. The source object must have the label <code>k0rdent.mirantis.com/managed: \"true\"</code>. For example, this YAML describes a FluxCD source object of <code>kind</code> <code>HelmRepository</code>:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: custom-templates-repo\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/external-templates-repo/charts\n</code></pre>"},{"location":"admin/services/admin-service-templates/#create-the-servicetemplate","title":"Create the <code>ServiceTemplate</code>","text":"<p>A template can either define a Helm chart directly using the template's <code>spec.helm.chartSpec</code> field or reference its location using the <code>spec.helm.chartRef</code> field.</p> <p>For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  helm:\n    chartSpec:\n      chart: ingress-nginx\n      version: 4.11.3\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n</code></pre> <p>In this case, we're creating a <code>ServiceTemplate</code> called <code>ingress-nginx-4.11.3</code> in the <code>my-target-namespace</code> namespace.  It references version 4.11.3 of the <code>ingress-nginx</code> chart located in the <code>k0rdent-catalog</code> Helm repository.</p> <p>For more information on creating templates, see the Template Guide.</p>"},{"location":"admin/services/admin-service-templates/#creating-kustomization-based-servicetemplate","title":"Creating Kustomization-based ServiceTemplate","text":"<p>Define the source of the Kustomization that defines the service. If the source object is one of Flux source - <code>GitRepository</code>, <code>Bucket</code> or <code>OCIRepository</code> - it must have the label <code>k0rdent.mirantis.com/managed: \"true\"</code>. Source object can be already created. In this case <code>.spec.kustomization.localSourceRef</code> should be used:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kustomization-app\n  namespace: kcm-system\nspec:\n  kustomize:\n    localSourceRef:\n      kind: GitRepository\n      name: project-x\n    deploymentType: Remote\n    path: \"./base\"\n</code></pre> <p>Aside from flux sources, local <code>ConfigMap</code> or <code>Secret</code> object can be used as a source of the kustomization manifests. The <code>ServiceTemplate</code> definition will not differ from one in above, except the <code>.spec.kustomize.localSourceRef.kind</code> which should be set to respective object type. However, to use <code>ConfigMap</code> or <code>Secret</code> as a source, they must embed archive with kustomization. After the archive was created, this can be done by executing the following command:</p> <pre><code>kubectl create configmap kustomization-source --from-file=/path/to/kustomization.tar.gz\n</code></pre> <p>Another option is to let the controller to create the remote source object. In this case <code>.spec.kustomization.remoteSourceSpec</code> should be used:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kustomization-app\n  namespace: kcm-system\nspec:\n  kustomize:\n    remoteSourceSpec:\n      oci:\n        url: oci://ghcr.io/org/project-x\n        reference:\n          tag: latest\n        interval: 10m\n    deploymentType: Remote\n    path: \"./overlays\"\n</code></pre> <p>When <code>.spec.kustomize.remoteSourceSpec</code> is defined, the controller will create corresponding object.</p>"},{"location":"admin/services/admin-service-templates/#creating-raw-resources-based-servicetemplate","title":"Creating Raw-Resources-based ServiceTemplate","text":"<p>This type of source is quite similar to the kustomization sources, with the only exception:  when using <code>ConfigMap</code> or <code>Secret</code> as a source, the field <code>.spec.resources.localSourceRef.path</code> will be ignored  and the resources' manifests to be deployed must be inlined in the source's <code>data</code>.</p> <p>Example of the <code>ConfigMap</code> and corresponding <code>ServiceTemplate</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: project-cm\n  namespace: project-x\ndata:\n  namespace.yaml: |\n    apiVersion: v1\n    kind: Namespace\n    metadata:\n      name: managed-ns\n</code></pre> <p>To create such <code>ConfigMap</code> the following command can be used:</p> <pre><code>kubectl --namespace project-x create configmap project-cm --from-file=/path/to/namespace.yaml\n</code></pre> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: managed-ns\n  namespace: project-x\nspec:\n  resources:\n    localSourceRef:\n      kind: ConfigMap\n      name: project-cm\n    deploymentType: Remote\n    path: \"\"  # will be ignored\n</code></pre> <p>Using the remote source for <code>ServiceTemplate</code> based on raw resources is similar to the kustomization-based template:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kustomization-app\n  namespace: kcm-system\nspec:\n  resources:\n    remoteSourceSpec:\n      git:\n        url: https://github.com/org/project-x.git\n        reference:\n          branch: main\n        interval: 10m\n    deploymentType: Remote\n    path: \"./overlays\"\n</code></pre>"},{"location":"admin/services/admin-service-templates/#deploying-application-with-servicetemplate","title":"Deploying Application with ServiceTemplate","text":""},{"location":"admin/services/admin-service-templates/#create-a-servicetemplatechain","title":"Create a <code>ServiceTemplateChain</code>","text":"<p>To let Mirantis k0rdent Enterprise know where this <code>ServiceTemplate</code> can and can't be used, create a <code>ServiceTemplateChain</code> object, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplateChain\nmetadata:\n  name: project-ingress-nginx-4.11.3\n  namespace: my-target-namespace\nspec:\n  supportedTemplates:\n    - name: project-ingress-nginx-4.11.3\n    - name: project-ingress-nginx-4.10.0\n      availableUpgrades:\n        - name: project-ingress-nginx-4.11.3\n</code></pre> <p>Here you see a template called <code>project-ingress-nginx-4.11.3</code> that is meant to be deployed in the <code>my-target-namespace</code> namespace. The <code>.spec.helm.chartSpec</code> specifies the name of the Helm chart and where to find it, as well as the version and other important information. The <code>ServiceTempateChain</code> shows that this template is also an upgrade path from version 4.10.0.</p> <p>If you wanted to deploy this as an application, you would first go ahead and add it to the cluster in which you were working, so if you were to save this YAML to a file called <code>project-ingress.yaml</code> you could run this command on the management cluster:</p> <pre><code>kubectl apply -f project-ingress.yaml -n my-target-namespace\n</code></pre>"},{"location":"admin/services/admin-service-templates/#adding-a-service-to-a-clusterdeployment","title":"Adding a <code>Service</code> to a <code>ClusterDeployment</code>","text":"<p>To add the service defined by this template to a cluster, you would simply add it to the <code>ClusterDeployment</code> object when you create it, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-1-0-12\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: project-ingress-nginx-4.11.3\n        name: ingress-nginx\n        namespace: my-target-namespace\n    priority: 100\n</code></pre> As you can see, you're simply referencing the template in the <code>.spec.serviceSpec.services[].template</code> field of the <code>ClusterDeployment</code> to tell Mirantis k0rdent Enterprise that you want this service to be part of this cluster.</p> <p>If you wanted to add this service to an existing cluster, you would simply patch the definition of the <code>ClusterDeployment</code>, as in:</p> <p><pre><code>kubectl patch clusterdeployment my-cluster-deployment -n my-target-namespace --type='merge' -p '{\"spec\":{\"serviceSpec\":{\"services\":[{\"template\":\"project-ingress-nginx-4.11.3\",\"name\":\"ingress-nginx\",\"namespace\":\"my-target-namespace\"}]}}}'\n</code></pre> For more information on creating and using <code>ServiceTemplate</code> objects, see the User Guide.</p>"},{"location":"admin/upgrade/","title":"Upgrading Mirantis k0rdent Enterprise","text":"<p>Upgrading Mirantis k0rdent Enterprise involves making upgrades to the <code>Management</code> object. To do that, you must have the <code>Global Admin</code> role. (For detailed information about Mirantis k0rdent Enterprise RBAC roles and permissions, refer to the RBAC documentation.) Follow these steps to upgrade Mirantis k0rdent Enterprise:</p> <ol> <li> <p>Create a new <code>Release</code> object</p> <p>Start by creating a <code>Release</code> object in the management cluster that points to the desired version. You can get <code>Release</code> object for the current version from the following URL: <code>https://get.mirantis.com/k0rdent-enterprise/1.1.0/release.yaml</code></p> <p>For example, this <code>Release</code> object looks like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Release\nmetadata:\n  name: k0rdent-enterprise-1-1-0\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  version: 1.1.0\n  kcm:\n    template: k0rdent-enterprise-1-1-0\n  capi:\n    template: cluster-api-1-1-0\n  providers:\n    - name: cluster-api-provider-k0sproject-k0smotron\n      template: cluster-api-provider-k0sproject-k0smotron-1-0-6\n    - name: cluster-api-provider-azure\n      template: cluster-api-provider-azure-1-0-3\n    - name: cluster-api-provider-vsphere\n      template: cluster-api-provider-vsphere-1-0-2\n    - name: cluster-api-provider-aws\n      template: cluster-api-provider-aws-1-0-3\n    - name: cluster-api-provider-openstack\n      template: cluster-api-provider-openstack-1-0-3\n    - name: cluster-api-provider-docker\n      template: cluster-api-provider-docker-1-0-2\n    - name: cluster-api-provider-gcp\n      template: cluster-api-provider-gcp-1-0-3\n    - name: cluster-api-provider-ipam\n      template: cluster-api-provider-ipam-1-0-2\n    - name: cluster-api-provider-infoblox\n      template: cluster-api-provider-infoblox-1-0-1\n    - name: projectsveltos\n      template: projectsveltos-0-57-2\n</code></pre> <p>Thankfully, you don't have to build these YAML files yourself. Once you've chosen a release, you can go ahead and create the release object by referencing the YAML file online, as in:</p> <p><pre><code>kubectl create -f https://get.mirantis.com/k0rdent-enterprise/1.1.0/release.yaml\n</code></pre> <pre><code>release.k0rdent.mirantis.com/k0rdent-enterprise-1-1-0 created\n</code></pre></p> </li> <li> <p>List Available <code>Releases</code></p> <p>Once you've created the new <code>Release</code> you need to update the <code>Management</code> object to use it. Start by viewing all available <code>Release</code>s:</p> <pre><code>kubectl get releases\n</code></pre> <pre><code>NAME        AGE\nkcm-0-0-6   71m\nkcm-0-0-7   65m\nkcm-0-1-0   6d9h\nk0rdent-enterprise-1-1-0   12m\n</code></pre> </li> <li> <p>Patch the <code>Management</code> object with the new <code>Release</code></p> <p>Update the <code>spec.release</code> field in the <code>Management</code> object to point to the new release. Replace <code>&lt;release-name&gt;</code> with the name of your desired release:</p> <pre><code>RELEASE_NAME=k0rdent-enterprise-1-1-0\nkubectl patch managements.k0rdent.mirantis.com kcm --patch \"{\\\"spec\\\":{\\\"release\\\":\\\"${RELEASE_NAME}\\\"}}\" --type=merge\n</code></pre> </li> <li> <p>Verify the Upgrade</p> <p>Although the change will be made immediately, it will take some time for Mirantis k0rdent Enterprise to update the components it should be using. Monitor the readiness of the <code>Management</code> object to ensure the upgrade was successful. For example:</p> <p><pre><code>kubectl get managements.k0rdent.mirantis.com kcm\n</code></pre> <pre><code>NAME   READY   RELEASE     AGE\nkcm    True    k0rdent-enterprise-1-1-0   4m34s\n</code></pre></p> </li> </ol>"},{"location":"appendix/","title":"Appendix","text":"<ul> <li>Extended management configuration</li> <li>Using a Private Secure Registry to deploy k0rdent</li> <li>Understanding the dry run</li> <li>Cloud provider credentials management in CAPI</li> <li>Glossary</li> </ul>"},{"location":"appendix/appendix-dryrun/","title":"Understanding Dry Run mode","text":"<p>The <code>ClusterDeployment</code> process includes a \"dry run\" mode, which enables you to validate your configuration without actually provisioning resources. By default, <code>.spec.dryRun</code> is set to <code>false</code>, but enabling it can help identify potential issues early.</p> <p>Note that if no configuration (<code>.spec.config</code>) is provided, default values from the selected template will populate the object, and <code>.spec.dryRun</code> will automatically be enabled.</p> <p>Example: Dry Run with default configuration:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  dryRun: true\n</code></pre> <p>After validation (this is, you see <code>TemplateReady</code> as a condition in <code>.status.conditions</code>), remove or disable <code>.spec.dryRun</code> to proceed with deployment.</p> <p>Example: Validated <code>ClusterDeployment</code> object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    publicIP: true\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n  status:\n    conditions:\n    - type: TemplateReady\n      status: \"True\"\n      reason: Succeeded\n      message: Template is valid\n    - type: Ready\n      status: \"True\"\n      reason: Succeeded\n      message: ClusterDeployment is ready\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/","title":"Extended management configuration","text":""},{"location":"appendix/appendix-extend-mgmt/#extended-management-configuration","title":"Extended Management Configuration","text":"<p>Mirantis k0rdent Enterprise is deployed with the following default configuration, which may vary depending on the release version:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm: {}\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: cluster-api-provider-azure\n  - name: cluster-api-provider-openstack\n  - name: cluster-api-provider-vsphere\n  - name: projectsveltos\n  release: kcm-0-0-7\n</code></pre> <p>As you can see, the <code>Management</code> object defines the providers that are available from within Mirantis k0rdent Enterprise. Some of these are providers directly used by the user, such as aws, azure, and so on, and others are used internally by Mirantis k0rdent Enterprise, such as Sveltos.</p> <p>To see what is included in a specific release, look at the <code>release.yaml</code> file in the tagged release. For example, here is the v0.0.7 release.yaml.</p> <p>Mirantis k0rdent Enterprise allows you to customize its default configuration by modifying the spec of the <code>Management</code> object. This enables you to manage the list of providers to deploy and adjust the default settings for core components.</p> <p>For detailed examples and use cases, refer to Examples and Use Cases</p>"},{"location":"appendix/appendix-extend-mgmt/#configuration-guide","title":"Configuration Guide","text":"<p>There are two options to override the default management configuration of Mirantis k0rdent Enterprise:</p> <ol> <li> <p>Update the <code>Management</code> object after the Mirantis k0rdent Enterprise installation using <code>kubectl</code>:</p> <p><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; edit management</code></p> </li> <li> <p>Deploy Mirantis k0rdent Enterprise skipping the default <code>Management</code> object creation and provide your    own <code>Management</code> configuration:</p> <ul> <li> <p>Create <code>management.yaml</code> file and configure core components and providers.   For example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm:\n      config:\n        controller:\n          templatesRepoURL: \"oci://ghcr.io/my-oci-registry-name/kcm/charts\"\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: projectsveltos\n  release: kcm-0-0-7\n</code></pre>       In the example above, the <code>Management</code> object is configured with custom registry settings for the KCM controller       and a reduced list of providers.</p> </li> <li> <p>Specify <code>--create-management=false</code> controller argument and install Mirantis k0rdent Enterprise:   If installing using <code>helm</code> add the following parameter to the <code>helm   install</code> command:</p> <pre><code>--set=\"controller.createManagement=false\"\n</code></pre> </li> <li> <p>Create <code>kcm</code> <code>Management</code> object after Mirantis k0rdent Enterprise installation:</p> <pre><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; create -f management.yaml\n</code></pre> </li> </ul> </li> </ol> <p>You can customize the default configuration options for core components by updating the <code>.spec.core.&lt;core-component-name&gt;.config</code> section in the <code>Management</code> object. For example, to override the default settings for the KCM component, modify the <code>spec.core.kcm.config</code> section. To view the complete list of configuration options available for kcm, refer to: KCM Configuration Options for Mirantis k0rdent Enterprise v0.0.7 (Replace v0.0.7 with the relevant release tag for other Mirantis k0rdent Enterprise versions).</p> <p>To customize the list of providers to deploy, update the <code>.spec.providers</code> section. You can add or remove providers and configure custom templates for each provider. Each provider in the list must include the <code>name</code> field and may include the <code>template</code> and <code>config</code> fields:</p> <pre><code>- name: &lt;provider-name&gt; \n  template: &lt;provider-template&gt; # optional. If omitted, the default template from the `Release` object will be used\n  config: {} # optional provider configuration containing provider Helm Chart values in YAML format\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#examples-and-use-cases","title":"Examples and Use Cases","text":""},{"location":"appendix/appendix-extend-mgmt/#configuring-a-custom-oci-registry-for-kcm-components","title":"Configuring a Custom OCI Registry for KCM components","text":"<p>You can override the default registry settings in Mirantis k0rdent Enterprise by specifying the <code>templatesRepoURL</code>, <code>insecureRegistry</code>, and <code>registryCredsSecret</code> parameters under <code>spec.core.kcm.config.controller</code>.</p> <ul> <li><code>templatesRepoURL</code>: Specifies the registry URL for downloading Helm charts representing templates. Use the <code>oci://</code> prefix for OCI registries. Default: <code>oci://ghcr.io/k0rdent/kcm/charts</code>.</li> <li><code>globalRegistry</code>: Specifies the global registry. This value will be propagated to all <code>ClusterDeployment</code> objects configuration as <code>global.registry</code> (for example, it is used for pulling cluster Helm extensions, such as the Cloud Controller Manager and to download required images, such as <code>etcd</code> or <code>kube-proxy</code>).</li> <li><code>insecureRegistry</code>: Allows connecting to an HTTP registry. Default: <code>false</code>.</li> <li><code>registryCredsSecret</code>: Specifies the name of a Kubernetes <code>Secret</code> containing authentication credentials for the  registry (optional). This <code>Secret</code> should exist in the system namespace (default: <code>kcm-system</code>).</li> </ul> <p>Additionally, if your templates repository (<code>templatesRepoURL</code>) and/or registry (<code>globalRegistry</code>) is private and uses a certificate signed by an unknown authority, you can make them \"trusted\" within the K0rdent system by configuring the <code>registryCertSecret</code> parameter. This parameter should reference the name of a <code>Secret</code> in the system (default: <code>kcm-system</code>) namespace that contains the root CA certificate(s) (<code>ca.crt</code>) used to verify the server certificates of the registry and/or templates repository. If the <code>templatesRepoURL</code> and <code>globalRegistry</code> refer to different endpoints, and each uses a different certificate authority, you can include both certificates concatenated in the same <code>ca.crt</code> key of the <code>Secret</code>, like this:</p> <pre><code>-----BEGIN CERTIFICATE-----\n&lt;templatesRepo CA cert&gt;\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n&lt;registry CA cert&gt;\n-----END CERTIFICATE-----\n</code></pre> <p>Note</p> <p> This is used for server certificate verification only - mutual TLS (mTLS) is not supported yet.</p> <p>Note</p> <p> If you\u2019re using a private registry signed by an unknown certificate authority, refer to Private Secure Registry Usage for the required prerequisites.</p> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        controller:\n          templatesRepoURL: \"oci://ghcr.io/my-private-oci-registry-name/kcm/charts\"\n          globalRegistry: ghcr.io/my-private-oci-registry-name\n          insecureRegistry: false\n          registryCredsSecret: my-private-oci-registry-creds\n          registryCertSecret: registry-cert\n</code></pre> <p>Note</p> <p> Prior to K0rdent v0.3.0, the <code>templatesRepoURL</code> parameter was named <code>defaultRegistryURL</code>. (See: K0rdent v0.3.0 Release Notes).</p> <p>Example of a <code>Secret</code> with Registry Credentials:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-private-oci-registry-creds\n  namespace: kcm-system\nstringData:\n  username: \"my-user-123\"\n  password: \"my-password-123\"\n</code></pre> <p>Example of a <code>Secret</code> with Registry Certificate:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: registry-cert\n  namespace: kcm-system\nstringData:\n  ca.crt: |\n    -----BEGIN CERTIFICATE-----\n    MIIDfjCCAmagAwIBAgIUV/Ykpp7jzkOdfsZs0wwNZOS9X04wDQYJKoZIhvcNAQEL\n    ...\n    2eVUGBCoHgFcUrkjcZlxvjjdaV5L/Y6mEt6u9mIhsb1M8w==\n    -----END CERTIFICATE-----\n</code></pre> <p>The KCM controller will create the default HelmRepository using the provided configuration and fetch KCM components from this repository. For the example above, the following <code>HelmRepository</code> will be created:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\n  name: my-private-oci-registry-name\n  namespace: kcm-system\nspec:\n  insecure: false\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/my-private-oci-registry-name/kcm/charts\n  secretRef:\n    name: my-private-oci-registry-creds\n  certSecretRef:\n    name: registry-cert\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#configuring-a-global-k0s-url","title":"Configuring a global K0s URL","text":"<p>You can override the default URL from which to download the k0s binary in Mirantis k0rdent Enterprise by specifying the <code>globalK0sURL</code>, and optionally <code>k0sURLCertSecret</code> (if the k0s download URL is private and uses a certificate signed by an unknown authority), under <code>spec.core.kcm.config.controller</code>. This is optional and is only needed when the environment does not have access to the default upstream k0s binaries endpoint. This is required for airgapped environments.</p> <ul> <li><code>globalK0sURL</code>: Specifies the prefix of the k0s URL from which to download the k0s binary. This value will be propagated to all <code>ClusterDeployment</code> objects configuration as <code>global.k0sURL</code>.</li> <li><code>k0sURLCertSecret</code>: The name of the secret in the system (default: <code>kcm-system</code>) namespace containing the root CA certificate (<code>ca.crt</code>) for the k0s download URL.</li> </ul> <p>Note</p> <p> This is used for server certificate verification only - mutual TLS (mTLS) is not supported yet.</p> <p>Note</p> <p> If you\u2019re using a private registry signed by an unknown certificate authority, refer to Private Secure Registry Usage for the required prerequisites.</p> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        controller:\n          globalK0sURL: https://172.19.123.4:8443\n          k0sURLCertSecret: k0s-url-cert\n</code></pre> <p>Example of a <code>Secret</code> with K0s URL Certificate:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: k0s-url-cert\n  namespace: kcm-system\nstringData:\n  ca.crt: |\n    -----BEGIN CERTIFICATE-----\n    MIIDfjCCAmagAwIBAgIUV/Ykpp7jzkOdfsZs0wwNZOS9X04wDQYJKoZIhvcNAQEL\n    ...\n    2eVUGBCoHgFcUrkjcZlxvjjdaV5L/Y6mEt6u9mIhsb1M8w==\n    -----END CERTIFICATE-----\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#configuring-a-custom-image-for-kcm-controllers","title":"Configuring a Custom Image for KCM controllers","text":"<p>You can override the default image for the KCM controllers by specifying the <code>repository</code>, <code>tag</code> and <code>pullPolicy</code> parameters under <code>spec.core.kcm.config.image</code>: </p> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        image:\n          repository: ghcr.io/my-custom-repo/kcm/controller\n          tag: v0.0.7\n          pullPolicy: IfNotPresent\n</code></pre>"},{"location":"appendix/appendix-extend-mgmt/#configuring-manager-settings-for-capi-providers","title":"Configuring manager settings for CAPI providers","text":"<p>Starting from <code>v0.3.0</code>, Mirantis k0rdent Enterprise supports configuring manager settings for CAPI providers. You can override these settings by defining the <code>spec.providers[*].config.manager</code> section. The values under the <code>manager</code> section should follow the format described here: https://github.com/kubernetes-sigs/cluster-api-operator/blob/v0.18.1/api/v1alpha2/provider_types.go#L126.</p> <p>Warning</p> <p>This is not supported for the <code>k0sproject-k0smotron</code> provider due to a bug in the CAPI Operator: CAPI operator incorrectly finds the manager container if the number of containers is &gt;1.</p> <p>For example, to override feature gates for the Cluster API Provider AWS, configure the following:</p> <pre><code>spec:\n  providers:\n  - name: cluster-api-provider-aws\n    config:\n      manager:\n        featureGates:\n          MachinePool: true\n          EKSEnableIAM: true\n          EKSAllowAddRoles: true\n</code></pre>"},{"location":"appendix/appendix-providers/","title":"Cloud Provider Credentials Management in CAPI","text":"<p>Cloud provider credentials in Cluster API (CAPI) environments are managed through objects in the management cluster. <code>Credential</code>, <code>ClusterIdentity</code>, and <code>Secret</code> (related to <code>ClusterIdentity</code>) objects handle credential storage and management, while a dedicated <code>ConfigMap</code> object is used as a template to render configuration into child clusters.</p>"},{"location":"appendix/appendix-providers/#configuration-patterns","title":"Configuration Patterns","text":"<p>The configuration follows two patterns:</p> <ol> <li>ClusterIdentity Pattern</li> <li>Uses a <code>ClusterIdentity</code> resource that defines provider identity configuration</li> <li>References a <code>Secret</code> with credentials</li> <li> <p>Used by <code>Azure</code> and <code>vSphere</code> in-tree providers</p> </li> <li> <p>Source Secret Pattern</p> </li> <li>Uses only a <code>Secret</code> without <code>ClusterIdentity</code></li> <li><code>Secret</code> contains all cloud configuration data</li> <li>Used by <code>OpenStack</code> in-tree provider</li> </ol> <p>In both cases <code>ConfigMap</code> with template code is used to render configuration into child clusters.</p>"},{"location":"appendix/appendix-providers/#credential-resource","title":"Credential Resource","text":"<p>The <code>Credential</code> resource provides an abstraction layer by either: - Referencing a <code>ClusterIdentity</code> through <code>identityRef</code> - Directly referencing a <code>Secret</code>, depending on the pattern used</p>"},{"location":"appendix/appendix-providers/#template-configmap","title":"Template ConfigMap","text":"<ul> <li>Marked with <code>projectsveltos.io/template: \"true\"</code> annotation</li> <li>Contains Go template code for generating child cluster resources via the Sveltos templating system</li> <li>Template processing accesses cluster objects through:</li> <li>Built-in Sveltos variables (<code>Cluster</code>, <code>InfrastructureProvider</code>)</li> <li><code>getResource</code> function for additionally exposed objects (<code>InfrastructureProviderIdentity</code>, <code>InfrastructureProviderIdentitySecret</code>)</li> <li>The object name needs to follow a predictable naming pattern, such as the <code>ClusterIdentity</code> object (referenced via <code>identityRef</code> in the <code>Credential</code> resource) name + <code>-resource-template</code> suffix. It must also be placed in same Namespace as the <code>ClusterIdentity</code> object it references</li> </ul>"},{"location":"appendix/appendix-providers/#templating-system","title":"Templating System","text":"<p>The templating system leverages: - Golang templating - Sprig functions - Sveltos resource manipulation functions</p>"},{"location":"appendix/appendix-providers/#examples","title":"Examples","text":"<p>Provider-specific examples are available in <code>*.credentials.yaml</code> files here.</p> <p>Let's take <code>Azure</code> provider as an example azure-credentials.yaml</p> <ul> <li><code>ClusterIdentity</code></li> <li><code>Secret</code> (related to <code>ClusterIdentity</code>)</li> <li><code>Credential</code></li> <li><code>ConfigMap</code></li> </ul> <p>Note</p> <p> These examples assume the <code>ConfigMap</code> is included in a template. To use these examples directly via <code>kubectl</code>, please replace <code>$$</code> (double dollar signs) with <code>$</code> (single dollar sign)</p>"},{"location":"appendix/appendix-providers/#provider-registration","title":"Provider Registration","text":"<p>Providers are registered through <code>ProviderInterface</code> CR, each provider Helm chart ships with corresponding <code>ProviderInterface</code> object.</p> <p>Modifications to the <code>Management</code> Spec are needed to enable newly added provider.</p> <p>For detailed information, refer to Extended Management Configuration</p>"},{"location":"appendix/appendix-providers/#examples_1","title":"Examples","text":"<p>As mentioned previously provider configuration examples can be found here, look for <code>ProviderInterface</code> object inside CAPI Helm charts.</p> <p>Let's take the <code>Azure</code> provider as an example interface.yaml, as seen, the CR definitions are straightforward.</p>"},{"location":"appendix/glossary/","title":"Mirantis k0rdent Enterprise Glossary","text":"<p>This glossary is a collection of terms related to Mirantis k0rdent Enterprise. It clarifies some of the unique terms and concepts we use or explains more common ones that may need a little clarity in the way we use them.</p>"},{"location":"appendix/glossary/#beach-head-services","title":"Beach-head Services","text":"<p>We use the term to refer to those Kubernetes services that need to be installed on a Kubernetes cluster to make it actually useful, for example: an ingress controller, CNI, and/or CSI. While from the perspective of how they are deployed they are no different from other Kubernetes services, we define them as distinct from the apps and services deployed as part of the applications.</p>"},{"location":"appendix/glossary/#accessmanagement-crd","title":"AccessManagement (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise used to define and manage access controls.  It typically includes specifications for <code>AccessRule</code> and <code>TargetNamespace</code> objects to control the  distribution of resources such as <code>ClusterTemplate</code>, <code>ServiceTemplate</code>, and <code>Credential</code> objects to  specific namespaces within managed clusters.</p>"},{"location":"appendix/glossary/#accessrules","title":"AccessRules","text":"<p>A component within the <code>AccessManagement</code> CRD that specifies which Mirantis k0rdent Enterprise  resources (such as <code>ClusterTemplateChain</code>, <code>Credential</code>, and <code>ServiceTemplateChain</code> objects) are to be  distributed to a defined set of <code>TargetNamespaces</code>.</p>"},{"location":"appendix/glossary/#cluster-api-capi","title":"Cluster API (CAPI)","text":"<p>CAPI is a Kubernetes project that provides a declarative way to manage the lifecycle of  Kubernetes clusters. It abstracts the underlying infrastructure, allowing users to  create, scale, upgrade, and delete clusters using a consistent API. CAPI is extensible  via providers that offer infrastructure-specific functionality, such as AWS, Azure, and  vSphere.</p>"},{"location":"appendix/glossary/#capi-provider-see-also-infrastructure-provider","title":"CAPI provider (see also Infrastructure provider)","text":"<p>A CAPI provider is a Kubernetes CAPI extension that allows Mirantis k0rdent Enterprise to manage and drive  the creation of clusters on a specific infrastructure via API calls.</p>"},{"location":"appendix/glossary/#capa","title":"CAPA","text":"<p>CAPA stands for Cluster API Provider for AWS.</p>"},{"location":"appendix/glossary/#capg","title":"CAPG","text":"<p>CAPG stands for Cluster API Provider for Google Cloud.</p>"},{"location":"appendix/glossary/#capo","title":"CAPO","text":"<p>CAPO stands for Cluster API Provider for OpenStack.</p>"},{"location":"appendix/glossary/#capv","title":"CAPV","text":"<p>CAPV stands for Cluster API Provider for vSphere.</p>"},{"location":"appendix/glossary/#capz","title":"CAPZ","text":"<p>CAPZ stands for Cluster API Provider for Azure.</p>"},{"location":"appendix/glossary/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<p>Cloud Controller Manager (CCM) is a Kubernetes component that embeds logic to manage a  specific infrastructure provider.</p>"},{"location":"appendix/glossary/#cluster-deployment","title":"Cluster Deployment","text":"<p>A Kubernetes cluster created and managed by Mirantis k0rdent Enterprise.</p>"},{"location":"appendix/glossary/#clusterdeployment-crd","title":"ClusterDeployment (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise that represents  the desired state and configuration of a Kubernetes cluster to be created and managed by  Mirantis k0rdent Enterprise. It typically includes details about the infrastructure  provider, cluster topology, version, and references to other configurations such as  <code>ClusterTemplate</code> or <code>ServiceTemplate</code> objects.</p>"},{"location":"appendix/glossary/#clusteridentity","title":"ClusterIdentity","text":"<p>ClusterIdentity is a Kubernetes object that references a Secret object containing  credentials for a specific infrastructure provider.</p>"},{"location":"appendix/glossary/#clusteripam-crd","title":"ClusterIPAM (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise responsible  for defining and managing IP address pools at a broad, cluster-aware level. It serves as  the central authority or source from which IP address ranges (CIDRs) or specific blocks  can be allocated for use by various Kubernetes clusters or their internal networks.</p>"},{"location":"appendix/glossary/#clusteripamclaim-crd","title":"ClusterIPAMClaim (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise that enables  users or automated processes to request or \"claim\" specific IP address resources (such as subnets or blocks of IPs) from a <code>ClusterIPAM</code> instance. This mechanism ensures orderly  allocation of IP addresses to individual Kubernetes clusters or other network-dependent  components.</p>"},{"location":"appendix/glossary/#clustertemplate-crd","title":"ClusterTemplate (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise that provides  a reusable blueprint for defining the configuration, components, and characteristics of a  Kubernetes cluster. <code>ClusterTemplate</code> objects are used by Mirantis k0rdent Enterprise to ensure consistency when provisioning or managing multiple clusters.</p>"},{"location":"appendix/glossary/#clustertemplatechain-crd","title":"ClusterTemplateChain (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise that defines an ordered sequence or a collection of <code>ClusterTemplate</code> objects. This allows for the  application of multiple layers of cluster configurations in a structured and repeatable  manner, often used to build up complex cluster environments.</p>"},{"location":"appendix/glossary/#clusteridentity_1","title":"ClusterIdentity","text":"<p>ClusterIdentity is a Kubernetes object that references a Secret object containing  credentials for a specific infrastructure provider.</p>"},{"location":"appendix/glossary/#credential","title":"Credential","text":"<p>A <code>Credential</code> is a custom resource (CR) in kcm that supplies Mirantis k0rdent Enterprise with the necessary  credentials to manage a specific infrastructure. The credential object references other  CRs with infrastructure-specific credentials such as access keys, passwords,  certificates, etc. This means that a credential is specific to the CAPI provider that  uses it.</p>"},{"location":"appendix/glossary/#declarative-approach","title":"Declarative approach","text":"<p>We define the declarative approach to cluster management using the Kubernetes principles  as the process where you define the state you want within custom resource objects and the  controllers or customer operators ensure that the system moves toward that desired state.</p>"},{"location":"appendix/glossary/#distributed-container-management-environment-dcme","title":"Distributed Container Management Environment (DCME)","text":"<p>An infrastructure setup focused on managing containerized applications across various,  often geographically dispersed, locations and platforms. Mirantis k0rdent Enterprise  is designed to provide platform engineers with the tools to build and operate a DCME.</p>"},{"location":"appendix/glossary/#dry-run","title":"Dry Run","text":"<p>A feature or operational mode within Mirantis k0rdent Enterprise that enables users to  simulate the outcome of an action, such as deploying or modifying a cluster or service,  without making any actual changes to the infrastructure. Running a command in <code>--dry-run</code> mode  helps in validating configurations and understanding potential impacts.</p>"},{"location":"appendix/glossary/#eks-eks-cluster","title":"EKS / EKS cluster","text":"<p>Refers to Amazon Elastic Kubernetes Service (EKS), a managed Kubernetes service by AWS.  Mirantis k0rdent Enterprise supports creating and managing EKS clusters, as well as using EKS to host the management cluster.</p>"},{"location":"appendix/glossary/#entra-id","title":"Entra-ID","text":"<p>Microsoft Entra ID (formerly Azure Active Directory) is a cloud-based identity and access  management service. Mirantis k0rdent Enterprise can integrate with Entra-ID for  authentication purposes.</p>"},{"location":"appendix/glossary/#internal-developer-platform-idp","title":"Internal Developer Platform (IDP)","text":"<p>A platform built by an organization to provide its developers with self-service capabilities  for accessing tools, services, and infrastructure required for software development and  deployment. Mirantis k0rdent Enterprise aims to enable platform engineers to compose  and deliver IDPs.</p>"},{"location":"appendix/glossary/#ip-address-management-ipam","title":"IP Address Management (IPAM)","text":"<p>The general concept and practice of planning, tracking, allocating, and managing IP address  space within a network. In Mirantis k0rdent Enterprise, this refers to the systems or  features, such as <code>ClusterIPAM</code> and <code>ClusterIPAMClaim</code>, used for managing IP addresses for  clusters and services.</p>"},{"location":"appendix/glossary/#k0rdent-cluster-manager-kcm","title":"k0rdent Cluster Manager (KCM)","text":"<p>Deployment and life-cycle management of Kubernetes clusters, including configuration,  updates, and other CRUD operations.</p>"},{"location":"appendix/glossary/#k0rdent-crds","title":"k0rdent CRDs","text":"<p>The collective term for the set of Custom Resource Definitions (CRDs) that are specific  to Mirantis k0rdent Enterprise. These CRDs extend the Kubernetes API to define  and manage k0rdent-specific resources, configurations, and functionalities, forming the  core of its declarative management capabilities.</p>"},{"location":"appendix/glossary/#k0rdent-observability-and-finops-kof","title":"k0rdent Observability and FinOps (KOF)","text":"<p>Cluster and beach-head services monitoring, events and log management.</p>"},{"location":"appendix/glossary/#k0rdent-state-manager-ksm","title":"k0rdent State Manager (KSM)","text":"<p>Installation and life-cycle management of deployed services.</p>"},{"location":"appendix/glossary/#k0s-k0s-cluster","title":"k0s / k0s cluster","text":"<p>Refers to k0s, an open-source, lightweight, and certified Kubernetes distribution.  Mirantis k0rdent Enterprise supports creating and managing k0s clusters, as well as running the Management Cluster on k0s.</p>"},{"location":"appendix/glossary/#hosted-control-plane-hcp","title":"Hosted Control Plane (HCP)","text":"<p>An HCP is a Kubernetes control plane that runs outside of the clusters it manages.  Instead of running the control plane components (like the API server, controller  manager, and etcd) within the same cluster as the worker nodes, the control plane is  hosted on a separate, often centralized, infrastructure. This approach can provide  benefits such as easier management, improved security, and better resource utilization,  as the control plane can be scaled independently of the worker nodes.</p>"},{"location":"appendix/glossary/#infrastructure-provider-see-also-capi-provider","title":"Infrastructure provider (see also CAPI provider)","text":"<p>An infrastructure provider (aka <code>InfrastructureProvider</code>) is a Kubernetes custom  resource (CR) that defines the infrastructure-specific configuration needed for managing  Kubernetes clusters. It enables Cluster API (CAPI) to provision and manage clusters on  a specific infrastructure platform (for example, AWS, Azure, VMware, OpenStack, and so on.).</p>"},{"location":"appendix/glossary/#localsourceref","title":"LocalSourceRef","text":"<p>A defined structure within Mirantis k0rdent Enterprise Custom Resource Definitions  that specifies a local source for configuration data, such as a kustomize manifest stored  within the management cluster (for example, in a <code>ConfigMap</code> or <code>Secret</code>).</p>"},{"location":"appendix/glossary/#management-crd","title":"Management (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise. While  \"Management Cluster\" refers to the Kubernetes cluster where Mirantis k0rdent Enterprise  itself is installed and operates, the <code>Management</code> CRD represents a specific  Mirantis k0rdent Enterprise resource type for a core management-related  configuration or entity within the ecosystem.</p>"},{"location":"appendix/glossary/#managementbackup-crd","title":"ManagementBackup (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise specifically  designed for configuring, triggering, and managing backup operations related to the management  plane components or the configuration of the management cluster itself.</p>"},{"location":"appendix/glossary/#multi-cluster-service","title":"Multi-Cluster Service","text":"<p>The <code>MultiClusterService</code> is a custom resource used to manage services' deployment across multiple clusters.</p>"},{"location":"appendix/glossary/#management-cluster","title":"Management Cluster","text":"<p>The Kubernetes cluster where Mirantis k0rdent Enterprise is installed and from which all other managed  clusters are managed.</p>"},{"location":"appendix/glossary/#okta","title":"Okta","text":"<p>An identity and access management (IAM) service. Mirantis k0rdent Enterprise can integrate  with Okta to handle user authentication and authorization for accessing Mirantis k0rdent Enterprise  functionality.</p>"},{"location":"appendix/glossary/#pluggableprovider-crd","title":"PluggableProvider (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise that represents a configured  instance of an external provider (e.g., infrastructure, services). It enables the integration of such  providers into the Mirantis k0rdent Enterprise system in a modular or \"pluggable\" fashion, specifically  by adhering to a defined <code>ProviderInterface</code>.</p>"},{"location":"appendix/glossary/#project-sveltos","title":"Project Sveltos","text":"<p>An open-source Kubernetes add-on controller that Mirantis k0rdent Enterprise, particularly its  k0rdent State Manager (KSM) component, leverages for managing the deployment and lifecycle of add-ons and  applications across managed Kubernetes clusters.</p>"},{"location":"appendix/glossary/#providerinterface-crd","title":"ProviderInterface (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise that defines a standardized  contract or API structure for how Mirantis k0rdent Enterprise interacts with various external  infrastructure or service providers (for example, cloud platforms or storage systems). This abstraction  layer enables consistent provider integration and management.</p>"},{"location":"appendix/glossary/#providertemplate-crd","title":"ProviderTemplate (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise used for creating reusable  and parameterized templates for the configuration of specific infrastructure providers (such as AWS,  Azure, GCP). These templates abstract provider-specific details and promote consistency in cluster  provisioning.</p>"},{"location":"appendix/glossary/#release-crd","title":"Release (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise primarily used to define and  manage aspects of Mirantis k0rdent Enterprise's own internal software lifecycle. <code>Release</code> plays  a role in tracking the versions of different components, coordinating updates or rollbacks of the  Mirantis k0rdent Enterprise platform itself on the management cluster, or specifying the  collection of software artifacts and configurations that constitute a particular internal k0rdent release.</p>"},{"location":"appendix/glossary/#remotesourcespec","title":"RemoteSourceSpec","text":"<p>A defined structure within Mirantis k0rdent Enterprise Custom Resource Definitions that specifies  a remote source for configuration data. This can include sources such as  a Git repository or an  S3-compatible object storage bucket (defined by <code>bucketName</code>, <code>endpoint</code>, etc.), often used for kustomize  manifests or Helm charts.</p>"},{"location":"appendix/glossary/#role-based-access-control-rbac","title":"Role Based Access Control (RBAC)","text":"<p>Role-Based Access Control (RBAC) defines roles, permissions, and rules governing user and system access to  Mirantis k0rdent Enterprise resources and the clusters it manages, ensuring secure and controlled operations.</p>"},{"location":"appendix/glossary/#service-definition-for-deployment","title":"Service (definition for deployment)","text":"<p>Within Mirantis k0rdent Enterprise's CRDs (for example, as part of <code>MultiClusterService</code> or templates),  this refers to a specific schema or object structure that defines a service to be deployed onto a cluster.  It typically includes attributes such as the service's name (often the chart release name), the template to  use, target namespace, Helm values, and <code>valuesFrom</code> for sourcing configuration from <code>ConfigMap</code> or <code>Secret</code> objects.</p>"},{"location":"appendix/glossary/#servicetemplate-crd","title":"ServiceTemplate (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise that provides a reusable template  for defining how a specific service, application, or set of Kubernetes resources (such as a database, monitoring  agent, or custom workload) is deployed and configured on managed Kubernetes clusters.</p>"},{"location":"appendix/glossary/#servicetemplatechain-crd","title":"ServiceTemplateChain (CRD)","text":"<p>A Custom Resource Definition (CRD) in Mirantis k0rdent Enterprise that defines an ordered sequence  or a collection of <code>ServiceTemplate</code> objects. This enables the orchestrated deployment of multiple services  or applications as a cohesive logical unit onto managed clusters. The <code>ServiceTemplateChain</code> also defines potential upgrade paths.</p>"},{"location":"appendix/glossary/#targetnamespaces","title":"TargetNamespaces","text":"<p>A component within the <code>AccessManagement</code> CRD that defines the specific Kubernetes namespaces within  managed clusters where selected k0rdent resources (such as <code>ClusterTemplate</code>, <code>Credential</code>, and <code>ServiceTemplate</code>  objects defined in an <code>AccessRule</code>) will be distributed or made available.</p>"},{"location":"appendix/glossary/#templateresourcerefs","title":"templateResourceRefs","text":"<p>A structure commonly found within Mirantis k0rdent Enterprise CRDs (Custom Resource Definitions) that  enables a template to reference existing Kubernetes resources (such as <code>Secret</code> or <code>ConfigMap</code> objects)  residing in the management cluster. These referenced resources can then be fetched and their data  injected or used during the instantiation of the k0rdent template for a target cluster or service.</p>"},{"location":"appendix/private-secure-registry/","title":"Using a Private Secure Registry to deploy Mirantis k0rdent Enterprise","text":""},{"location":"appendix/private-secure-registry/#prerequisites","title":"Prerequisites","text":"<p>If you are deploying Mirantis k0rdent Enterprise with registry overrides (see Configuring a Custom OCI Registry for KCM components), and your registry endpoint is secured with a certificate signed by an unknown Certificate Authority (CA), you must ensure that the CA certificate is trusted by your management cluster nodes before deploying Mirantis k0rdent Enterprise.</p> <p>To do this, add the CA certificate to the system\u2019s trust store on each management cluster node.</p>"},{"location":"appendix/private-secure-registry/#for-management-clusters-running-on-k0s","title":"For Management Clusters Running on k0s","text":"<p>Before starting the k0s controller, do the following:</p> <ol> <li> <p>Copy the CA certificate file (e.g., <code>&lt;PATH_TO_CA_CERT&gt;</code>) to the system\u2019s trusted certificate directory:</p> <pre><code>sudo cp &lt;PATH_TO_CA_CERT&gt; /usr/local/share/ca-certificates/\n</code></pre> </li> <li> <p>Update the system\u2019s trusted certificates:</p> <pre><code>sudo update-ca-certificates\n</code></pre> </li> <li> <p>Proceed to install k0s as usual.</p> </li> </ol>"},{"location":"appendix/private-secure-registry/#for-management-clusters-running-on-kind","title":"For Management Clusters Running on Kind","text":"<ol> <li> <p>Create a Kind configuration file that mounts the CA certificate into the container nodes. Suppose your CA certificate is located at <code>&lt;PATH_TO_CA_CERT&gt;</code> on your host.</p> <pre><code># kind-config.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n  - role: control-plane\n    extraMounts:\n      - hostPath: &lt;PATH_TO_CA_CERT&gt;\n        containerPath: /usr/local/share/ca-certificates/registry-ca.crt\n</code></pre> </li> <li> <p>Create the Kind cluster using the configuration:</p> <pre><code>kind create cluster --config kind-config.yaml --name &lt;KIND_CLUSTER_NAME&gt;\n</code></pre> </li> <li> <p>Run the following command to update the trust store inside the container. Repeat this command for each node in the Kind cluster, replacing <code>&lt;KIND_CLUSTER_NAME&gt;-control-plane</code> with the name of the Docker container representing that node:</p> <pre><code>docker exec &lt;KIND_CLUSTER_NAME&gt;-control-plane update-ca-certificates\n</code></pre> </li> </ol>"},{"location":"catalog/","title":"k0rdent Catalog","text":"<p>Mirantis k0rdent Enterprise is composable by design. The open source community around k0rdent has created a host of composable components that can be leveraged by Mirantis k0rdent Enterprise users -- available through the k0rdent Catalog. Mirantis and partners jointly support a subset of these components for key use-cases.</p> <p>Some of these solutions will be documented in this section, starting with:</p> <ul> <li>Ceph Unified Storage is an open\u2010source, distributed storage platform that offers unified block, file, and object storage. When run under Kubernetes, it provides scalable, resilient, and self-healing data management using commodity hardware.</li> </ul> <p>For more information about fully-supported components for k0rdent, please contact us</p>"},{"location":"concepts/","title":"Mirantis k0rdent Enterprise Concepts","text":"<p>Mirantis k0rdent Enterprise addresses the complexities of modern infrastructure by helping platform  engineers and developers through a declarative, Kubernetes-native approach to  cluster management. Mirantis k0rdent Enterprise aims to close the divide between rapidly evolving  application workloads, especially those powered by AI/ML, and the frequently slower  infrastructure that supports them. It achieves this goal by utilizing the  well-established open-source ecosystem, primarily Kubernetes, to offer a modular,  extensible, and repeatable template-based solution. Mirantis k0rdent Enterprise simplifies infrastructure,  boosting efficiency, developer speed, and control, so operators can focus on delivering  applications.</p> <p>This section of the documentation includes:</p> <ul> <li>Why Mirantis k0rdent Enterprise?</li> <li>Mirantis k0rdent Enterprise architecture</li> </ul>"},{"location":"concepts/k0rdent-architecture/","title":"Mirantis k0rdent Enterprise architecture","text":"<p>The Mirantis k0rdent Enterprise architecture follows a declarative approach to cluster management using Kubernetes principles. The modular extensible architecture provides for a repeatable template-driven solution to interact with sub components such as the Cluster API (CAPI) and other Kubernetes components.</p> <p>The key principles of the architecture include:</p> <ul> <li>Leveraging Kubernetes' core principles</li> <li>A highly aligned but loosely coupled architecture</li> <li>A pluggable and extensible architecture</li> <li>A template-driven approach for repeatability</li> <li>A standards-driven API</li> <li>The leveraging of unmodified upstream components (for example, the Kubernetes Cluster API)</li> <li>Support for integration with custom components downstream</li> </ul> <p>Note</p> <p> This document is a ongoing work in progress, and we would welcome suggestions and questions. </p>"},{"location":"concepts/k0rdent-architecture/#overview","title":"Overview","text":"<p>The Mirantis k0rdent Enterprise Management Cluster orchestrates the provisioning and lifecycle of multiple child clusters on multiple clouds and infrastructures, keeping you from having to directly interact with individual infrastructure providers. By abstracting the infrastructure in this way, Mirantis k0rdent Enterprise promotes reusability (reducing, for example, the effort required to implement an IDP on a particular cloud), encourages standardization where practical, and lets you use the clouds and technologies you want, while also minimizing the cost of switching components such as open source subsystems, cloud substrates, and so on.</p> <p>The Mirantis k0rdent Enterprise architecture comprises the following high level components:</p> <ul> <li>Cluster Management: Tools and controllers for defining, provisioning, and managing clusters.</li> <li>State Management: Controllers and systems for monitoring, updating, and managing the state of child clusters and their workloads.</li> <li>Infrastructure Providers: Services and APIs responsible for the under-the-hood provisioning resources such as virtual machines, networking, and storage for clusters.</li> <li>Templates: Templates define managed child clusters or the workloads that run on them. Instantiating those templates creates the corresponding resources.</li> <li>User Interface: A comprehensive UI lets users create and manage clusters, services, and templates, simplifying the process of performing common operations.</li> </ul> <p></p>"},{"location":"concepts/k0rdent-architecture/#management-cluster","title":"Management cluster","text":"<p>The management cluster is the core of the Mirantis k0rdent Enterprise architecture. It hosts all of the controllers needed to make Mirantis k0rdent Enterprise work. This includes:</p> <ul> <li>k0rdent Cluster Manager (KCM) Controller:  KCM provides a wrapper for Mirantis k0rdent Enterprise\u2019s CAPI-related capabilities. It orchestrates:<ul> <li>Cluster API (CAPI) Controllers: CAPI controllers are designed to work with specific infrastructure providers. For example, one CAPI controller manages the creation and lifecycle of Kubernetes clusters running on Amazon Web Services, while another manages those on Azure. It\u2019s also possible to create custom CAPI controllers to integrate with internal systems.</li> <li>k0smotron Controller: k0smotron extends CAPI with additional functionality, including control plane and worker node bootstrap providers for k0s Kubernetes and a control plane provider that supports the creation of a Hosted Control Plane, or a k0s control plane represented by pods on a host Kubernetes cluster, which can be the same cluster that hosts Mirantis k0rdent Enterprise. The k0smotron project has also provided a so-called \u2018RemoteMachine\u2019 infrastructure provider for CAPI, enabling deployment and cluster operations via SSH on arbitrary remote Linux servers (including small-scale edge devices).</li> </ul> </li> <li>k0rdent State Manager (KSM) Controller: KSM is responsible for lifecycle managing (deploy, scale, update, upgrade, teardown) services and applications on clusters, and for doing continuous state management of these services and applications. This is currently part of the KCM code base; we may split it out in the future. It orchestrates:<ul> <li>Services Controller: Responsible for coordinating Kubernetes services, such as combinations of services and infrastructure provisioning dependencies that add capabilities to the platform. For example, Nginx, with its dependencies, can be packaged as a service. Artifacts for services are stored locally or in an OCI repository, and are referenced as kubernetes CRD objects.</li> </ul> </li> <li>k0rdent Observability &amp; FinOps (KOF) Controller (not depicted in above diagram): Mirantis k0rdent Enterprise Observability and FinOps provides enterprise-grade observability and FinOps capabilities for k0rdent-managed Kubernetes clusters. It enables centralized metrics, logging, and cost management through a unified OpenTelemetry-based architecture.</li> </ul> <p>We\u2019ll take a closer look at these pieces under Roles and Responsibilities.</p>"},{"location":"concepts/k0rdent-architecture/#cluster-deployments","title":"Cluster Deployments","text":"<p>A cluster deployment is also known as a child cluster, or a workload cluster. It\u2019s a Kubernetes cluster provisioned and managed by the management cluster, and it\u2019s where developers run their applications and workloads. These are \u201cregular\u201d Kubernetes clusters, and don\u2019t host any management components. Clusters are fully isolated from the management cluster via namespaces, and also from each other, making it possible to create multi-tenant environments. </p> <p>You can tailor a child cluster to specific use cases, with customized addons such as ingress controllers, monitoring tools, and logging solutions. You can also define specific Kubernetes configurations (for example, network policies, storage classes, and security policies) so they work for you and your applications or environments.</p> <p>Simply put, child clusters are where applications and workloads run.</p>"},{"location":"concepts/k0rdent-architecture/#templates","title":"Templates","text":"<p>One of the important tenets of the platform engineering philosophy is the use of Infrastructure as Code, but Mirantis k0rdent Enterprise takes that one step further through the use of templates. Templates are re-usable text definitions of components that can be used to create and manage clusters. Templates provide a declarative way for users and developers to deploy and manage complex clusters or components while massively reducing the number of parameters they need to configure. Considered generally, Mirantis k0rdent Enterprise templates are:</p> <ul> <li>Formatted using YAML: Templates use YAML as an abstraction to represent the target state, so they\u2019re human-readable and editable.</li> <li>Designed to be used in multiple contexts using runtime parameterization: Through the use of placeholders, you can customize templates at runtime without having to directly edit the template.</li> <li>Used for both cluster creation and addon management: Users can define a cluster using YAML, or they can define addons, such as an ingress operator or monitoring tools, to be added to those clusters.</li> <li>Capable of limited scope: Mirantis k0rdent Enterprise lets you set restrictions over what templates can be deployed by whom. For example, as the platform manager (see Roles and Responsibilities), you can specify that non-admin users can only execute templates that deploy a particular set of controllers.</li> </ul> <p>Major template types used in Mirantis k0rdent Enterprise include:</p> <ul> <li>Cluster Templates: <code>ClusterTemplate</code> objects define clusters in coordination with the clouds and infrastructures on which they run. They're designed to be immutable; they get invoked by Mirantis k0rdent Enterprise objects such as <code>ClusterDeployment</code> objects to create and manage individual clusters and groups of clusters.</li> <li>Service Templates: <code>ServiceTemplate</code> objects define services, addons, and workloads that run on clusters. They're also designed to be immutable, and get invoked by <code>ClusterDeployment</code> objects and other Mirantis k0rdent Enterprise objects so that IDPs/platforms can be declared and managed as units. </li> </ul>"},{"location":"concepts/k0rdent-architecture/#roles-and-responsibilities","title":"Roles and responsibilities","text":"<p>Mirantis k0rdent Enterprise was designed to be used by several groups of people, with hierarchical and complementary roles and responsibilities. You may have your own names for them, but we\u2019ll refer to them as:</p> <ul> <li>Platform Architect: This person or team has global responsibility to the business and technical stakeholders for designing IDPs/platforms for later adaptation to particular clouds and infrastructures, workloads, performance and cost objectives, security and regulatory regimes, and operational requirements. Mirantis k0rdent Enterprise enables Platform Architects to create sets of reusable <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, closely defining IDPs/platforms in the abstract.</li> <li>Platform Lead: This person or team (sometimes referred to as 'CloudOps') is primarily responsible for actions corresponding to Mirantis k0rdent Enterprise Cluster Manager (KCM). They adapt <code>ClusterTemplate</code> objects to the correct cloud, and they make sure that everything is working properly. They\u2019re also responsible for limiting the Project Team\u2019s access to the <code>Cluster</code> and <code>Service</code> templates necessary to do their jobs. For example, they might limit the templates that can be deployed to an approved set, or provide CAPI operators for only the clouds on which the company wants applications to run, helping to eliminate shadow IT. </li> <li>Platform Engineer: This person or team is responsible for the day-to-day management of the environment. They use <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects provided by the Platform Lead (as authorized to do so) and may create additional <code>ServiceTemplate</code> objects to customize their own Kubernetes cluster so that it's appropriate for their application.</li> </ul>"},{"location":"concepts/k0rdent-architecture/#credentials","title":"Credentials","text":"<p>Creating and managing Kubernetes clusters requires having the proper permissions on the target infrastructure, but you certainly wouldn't want to give out your AWS account information to every single one of your developers.</p> <p>To solve this problem, Mirantis k0rdent Enterprise lets you create a <code>Credential</code> object that provides the access your developers need. It works like this:</p> <ol> <li>The platform lead creates a provider-specific <code>ClusterIdentity</code> and <code>Secret</code> that include all of the information necessary to perform various actions.</li> <li>The platform lead then creates a <code>Credential</code> object that references the <code>ClusterIdentity</code>.</li> <li>Developers reference the <code>Credential</code> object, which gives the cluster the ability to access these credentials (little \u201cc\u201d) without having to expose them to developers directly.</li> </ol> <p>Note</p> <p> To be sure credentials are not visible to developers, make sure to limit access to the <code>kcm-system</code> namespace.</p>"},{"location":"concepts/k0rdent-architecture/#tldr-conclusion","title":"TL;DR - Conclusion","text":"<p>Mirantis k0rdent Enterprise provides a comprehensive Kubernetes lifecycle management framework through its three core components:</p> <ul> <li>KCM: Cluster provisioning and management</li> <li>KSM: Application and runtime state management</li> <li>KOF: Observability, logging, and cost optimization</li> </ul> <p>With multi-provider support, templated deployments, and strong security controls, Mirantis k0rdent Enterprise is being built to enable scalable, efficient, and consistent Kubernetes operations.</p>"},{"location":"concepts/why-k0rdent/","title":"Why Mirantis k0rdent Enterprise?","text":"<p>Mirantis k0rdent Enterprise was developed to provide for the needs of platform engineers and the developers that they serve, as well as the application workloads that they support. </p>"},{"location":"concepts/why-k0rdent/#applications-and-workloads","title":"Applications and Workloads","text":"<p>Workloads evolve and grow, often faster then the infrastructure needed to support them. Infrastructure is almost always the lagging factor in getting new or existing applications into the hands of users and scaling them to meet user expectations. This challenge has grown exponentially with the rise of AI and ML workloads. Specifically:</p> <ul> <li>Workload complexity is increasing</li> <li>Modern workloads depend on specialized infrastructure</li> <li>Developers have high expectations of time to value</li> </ul>"},{"location":"concepts/why-k0rdent/#platform-engineering","title":"Platform Engineering","text":"<p>Modern infrastructure systems are increasingly complex, and administrators need to manage that complexity while still responding quickly to developers' needs efficiently as possible. This has led to development of internal developer platforms (IDP) and platform engineering. These environments provide the frameworks and tools for increasing developer productivity when developing, deploying, and managing applications and services, enabling developers to focus on their specific tasks or goals and not the underlying complexities. Overall:</p> <ul> <li>Developer platforms increase developer productivity</li> <li>Platform engineers need to implement and grow the platforms</li> <li>Infrastructure needs to support the required complexity</li> </ul>"},{"location":"concepts/why-k0rdent/#modern-infrastructure-systems","title":"Modern Infrastructure Systems","text":"<p>The increasingly distributed nature of modern infrastructure systems and the demands of modern workloads is leading to increasing complexity. Solutions need to solve a diverse set of challenges and provide consistency, repeatability, and prevention of lock-in, all without increasing the burden on operators. Modern platform engineers and operators are increasingly time constrained with the vast number of challenges they need to overcome, including security and compliance, cost management, resilience, and scale to name but a few. Keep in mind that:</p> <ul> <li>Distributed deployments are the new normal</li> <li>Infrastructure management is not just a technical problem</li> <li>Operators need to focus on building value chains</li> </ul>"},{"location":"concepts/why-k0rdent/#open-source","title":"Open Source","text":"<p>The open source ecosystem, and especially Kubernetes, is mature and offers an increasing number of tools that solve real problems. The open source ecosystem, if leveraged correctly, also supports building unique architectures to support a business' needs while helping to avoid lock-in and architectural dead ends. All of these tools need to be selected, deployed, and lifecycle-managed in a way that is repeatable and traceable. Open source:</p> <ul> <li>Prevents lock-in and supports architectural self determination</li> <li>Solutions allow for solving of problems in unique ways</li> <li>Can help solve the problem of managing the complexity of modern infrastructure</li> </ul>"},{"location":"concepts/why-k0rdent/#only-in-mirantis-k0rdent-enterprise","title":"Only in Mirantis k0rdent Enterprise","text":"<p>Mirantis k0rdent Enterprise builds on this strong open source foundation to deliver additional capabilities that address the most demanding enterprise requirements. These exclusive components extend the platform's reach into environments and use cases that require specialized tooling and support. They include:</p> <ul> <li>Bare Metal Cloud Deployment: Deploy complete cloud infrastructures directly to bare metal servers, providing maximum performance and control over the underlying hardware while maintaining cloud-native operational models</li> <li>Airgapped Environment Support: Enable cloud deployment and management in completely isolated networks, meeting the strictest security and compliance requirements without sacrificing functionality or operational efficiency</li> <li>Ceph Storage Provider Integration: Leverage Ceph as a native storage backend, providing distributed, scalable storage solutions that integrate seamlessly with the platform's lifecycle management capabilities</li> <li>Mirantis k0rdent Virtualization: Deliver enterprise-grade virtualization capabilities, enabling organizations to run virtual machines alongside containerized workloads within the same Kubernetes infrastructure</li> <li>Software Bills of Materials (SBOM): Provide comprehensive supply chain transparency through detailed software composition analysis, enabling users to verify the integrity and composition of all software components for enhanced security and compliance</li> </ul> <p>These enterprise capabilities ensure that organizations can adopt cloud-native infrastructure patterns regardless of their specific operational constraints, security requirements, or performance needs. Mirantis k0rdent Enterprise maintains the architectural flexibility and vendor neutrality that platform engineers value while providing the specialized tooling that enterprise environments demand.</p>"},{"location":"quickstarts/","title":"Guide to QuickStarts","text":"<p>The following QuickStart chapters provide a recipe for quickly installing and trying Mirantis k0rdent Enterprise. Setting up Mirantis k0rdent Enterprise for production is detailed in the Administrator Guide.</p>"},{"location":"quickstarts/#what-the-quickstart-covers","title":"What the QuickStart covers","text":"<p>The QuickStart section explains:</p> <ul> <li>Getting a working environment set up for managing Mirantis k0rdent Enterprise.</li> <li>Setting up a minimal Kubernetes cluster (management cluster) plus requirements to host Mirantis k0rdent Enterprise itself.</li> <li>Selecting a cloud environment (AWS or Azure) and configuring Mirantis k0rdent Enterprise to lifecycle manage clusters on this substrate.</li> <li>Using Mirantis k0rdent Enterprise to deploy a managed cluster.</li> <li>(Optional stretch goal) Setting up the Mirantis k0rdent Enterprise management cluster to simultaneously lifecycle manage clusters on both cloud environments.</li> </ul>"},{"location":"quickstarts/#quickstart-prerequisites","title":"QuickStart Prerequisites","text":"<p>QuickStart prerequisites are simple \u2014 you'll need:</p> <ul> <li>A desktop or virtual machine running a supported version of linux. You'll use this machine to install a basic Kubernetes working environment, and to host a single-node k0s Kubernetes management cluster to host Mirantis k0rdent Enterprise components. For simplest setup, configure this machine as follows:<ul> <li>A minimum of 8GB RAM, 4 vCPUs, 100GB SSD (for example, AWS <code>t2.xlarge</code> or equivalent)</li> <li>Set up for SSH access using keys (standard for cloud VMs)</li> <li>Set up for passwordless sudo (that is, edit <code>/etc/sudoers</code> to configure your user to issue <code>sudo</code> commands without a password challenge)</li> <li>Inbound traffic: SSH (port 22) and ping from your laptop's IP address</li> <li>Outbound traffic: All to any IP address</li> <li>Apply all recent updates and upgrade local applications (<code>sudo apt update</code>/<code>sudo apt upgrade</code>)</li> <li>(Optional) snapshot the machine in its virgin state</li> </ul> </li> <li>Administrative-level access to an AWS or Azure cloud account, depending on which cloud environment you prefer. Mirantis k0rdent Enterprise will leverage this cloud to provide infrastructure for hosting child clusters.</li> </ul>"},{"location":"quickstarts/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Any linux based OS that supports deploying k0s will work, though you may need to adjust the suggested commands.</p> OS Package Manager Link Ubuntu Server <code>apt</code> 22.04.5 LTS, Jammy Jellyfish <p>Note</p> <p>Other recent versions of 'enterprise' Linux should work with the following instructions as well, though you will need to adapt for different package managers and perhaps use slightly-different provider-recommended methods for installing required dependencies (for example, Helm). Once you've installed Mirantis k0rdent Enterprise in the management cluster and have kubectl, Helm, and other resources connected, you'll mostly be dealing with Kubernetes, and everything should work the same way on any host OS.</p>"},{"location":"quickstarts/#limitations","title":"Limitations","text":"<p>This QuickStart guides you in quickly creating a minimal Mirantis k0rdent Enterprise working environment. As mentioned earlier, setting up Mirantis k0rdent Enterprise for production is detailed in the Administrator Guide.</p> <p>The current QuickStart focuses on AWS and Azure cloud environments, and guides in creating 'standalone' clusters. In Mirantis k0rdent Enterprise parlance, that means 'CNCF-certified Kubernetes clusters with control planes and workers hosted on cloud virtual machines.' The 'CNCF-certified Kubernetes cluster' (in this case) is the k0s Kubernetes distro.</p> <p>Next you'll want to learn how to:</p> <ul> <li>Set up the Management Node and Cluster</li> <li>Configure and Deploy to AWS</li> <li>Configure and Deploy to Azure</li> <li>Configure and Deploy on any SSH-accessible Linux hosts</li> <li>Configure and Deploy to GCP</li> </ul>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/","title":"QuickStart 1 - Setup Management Cluster","text":"<p>Please review the Guide to QuickStarts for preliminaries. This QuickStart unit details setting up a single-VM environment for managing and interacting with Mirantis k0rdent Enterprise, and for hosting its components on a single-node local Kubernetes management cluster. Once Mirantis k0rdent Enterprise is installed on the management cluster, you can drive it by SSHing into the management node (<code>kubectl</code> is there and will be provisioned with the appropriate <code>kubeconfig</code>) or remotely by various means. For example, you can install the management cluster <code>kubeconfig</code> in Lens or another Kubernetes dashboard on your laptop, tunnel across from your own local <code>kubectl</code>, and so on.</p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#install-a-single-node-k0s-cluster-locally-as-the-management-cluster","title":"Install a single-node k0s cluster locally as the management cluster","text":"<p>k0s Kubernetes is a CNCF-certified minimal single-binary Kubernetes that installs with one command and brings along its own CLI. We're using it to quickly set up a single-node management cluster on our manager node. However, Mirantis k0rdent Enterprise works on any CNCF-certified Kubernetes. If you choose to use something else, we would love to hear how you set things up to work for you.</p> <p>Download and install k0s:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --enable-worker --no-taints\nsudo k0s start\n</code></pre> <p>You can check to see if the cluster is working by leveraging <code>kubectl</code> (installed and configured automatically by k0s) via the k0s CLI:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#install-kubectl","title":"Install kubectl","text":"<p>k0s installs a compatible <code>kubectl</code> and makes it accessible via its own client. But to make your environment easier to configure, we advise installing <code>kubectl</code> the normal way on the manager node and using it to control the local k0s management cluster:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#get-the-local-k0s-clusters-kubeconfig-for-kubectl","title":"Get the local k0s cluster's kubeconfig for kubectl","text":"<p>On startup, k0s stores the administrator's <code>kubeconfig</code> in a local directory, making it easy to access:</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>At this point, your newly-installed <code>kubectl</code> should be able to interoperate with the k0s management cluster with administrative privileges. Test to see that the cluster is ready (this usually takes about one minute):</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#install-helm","title":"Install Helm","text":"<p>The Helm Kubernetes package manager is used to install Mirantis k0rdent Enterprise services. We'll install Helm as follows:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Issuing these commands should produce something very much like the following output:</p> <pre><code>Downloading https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#install-mirantis-k0rdent-enterprise","title":"Install Mirantis k0rdent Enterprise","text":"<p>Now we'll install Mirantis k0rdent Enterprise itself into the k0s management cluster:</p> <pre><code>helm install kcm oci://registry.mirantis.com/k0rdent-enterprise/charts/k0rdent-enterprise --version 1.1.0 -n kcm-system --create-namespace --set kordent-ui.enabled=false\n</code></pre> <p>You'll see something like the following. Ignore the warnings, since this is an ephemeral, non-production, non-shared environment:</p> <pre><code>Pulled: registry.mirantis.com/k0rdent-enterprise/charts/k0rdent-enterprise:1.1.0\nDigest: sha256:b51693108d167fe0de851ae04ec1caf7865745e8aa27049bc1fab47693ab9f7c\nNAME: kcm\nLAST DEPLOYED: Wed Aug 13 2025 08:42:36 2025\nNAMESPACE: kcm-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nk0rdent enterprise chart has been installed.\nPlease make sure to change the auth method or set a new password for basic auth by\ncreating a secret and referencing it in `k0rdent-ui.auth.basic.secretKeyRef`\n</code></pre> <p>Note</p> <p> This command installs Mirantis k0rdent Enterprise with the UI disabled. To enable it, see the instructions for configuring the UI.</p> <p>Mirantis k0rdent Enterprise startup takes several minutes.</p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#check-that-mirantis-k0rdent-enterprise-cluster-management-pods-are-running","title":"Check that Mirantis k0rdent Enterprise cluster management pods are running","text":"<p>One fundamental Mirantis k0rdent Enterprise subsystem, k0rdent Cluster Manager (KCM), handles cluster lifecycle management on clouds and infrastructures. For example, it helps you configure and compose clusters and manages infrastructure via Cluster API (CAPI). Before continuing, check that the KCM pods are ready:</p> <pre><code>kubectl get pods -n kcm-system   # check pods in the kcm-system namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre> <p>Pods reported in states other than Running should become ready momentarily.</p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#check-that-the-projectsveltos-pods-are-running","title":"Check that the projectsveltos pods are running","text":"<p>The other fundamental Mirantis k0rdent Enterprise subsystem, k0rdent State Manager (KSM), handles services configuration and lifecycle management on clusters. This utilizes the projectsveltos Kubernetes Add-On Controller and other open source projects. Before continuing, check that the KSM pods are ready:</p> <pre><code>kubectl get pods -n projectsveltos   # check pods in the projectsveltos namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre> <p>If you have fewer pods than shown above, just wait 5 minutes or so for all the pods to reconcile and start running.</p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#verify-that-mirantis-k0rdent-enterprise-itself-is-ready","title":"Verify that Mirantis k0rdent Enterprise itself is ready","text":"<p>The actual measure of whether Mirantis k0rdent Enterprise is ready is the state of the <code>Management</code> object. To check, issue this command:</p> <p><pre><code>kubectl get Management -n kcm-system\n</code></pre> <pre><code>NAME   READY   RELEASE     AGE\nkcm    True    kcm-1-1-0   9m\n</code></pre></p>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-provider-and-related-templates-are-available","title":"Verify that KCM provider and related templates are available","text":"<p>Mirantis k0rdent Enterprise KCM leverages CAPI to manage Kubernetes cluster assembly and host infrastructure. CAPI requires infrastructure providers for different clouds and infrastructure types. These are delivered and referenced within Mirantis k0rdent Enterprise using templates, instantiated in the management cluster as objects. Before continuing, verify that the default provider template objects are installed and verified. Other templates are also stored as provider templates in this namespace (for example, the templates that determine setup of KCM itself and other parts of the Mirantis k0rdent Enterprise system, such as projectsveltos, which is a component of Mirantis k0rdent Enterprise Service Manager (KSM, see below)) as well as the k0smotron subsystem, which enables creation and lifecycle management of managed clusters that use Kubernetes-hosted control planes (such as control planes as pods):</p> <pre><code>kubectl get providertemplate -n kcm-system   # list providertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME                                   VALID\ncluster-api-1-0-4                                 true\ncluster-api-provider-aws-1-0-3                    true\ncluster-api-provider-azure-1-0-3                  true\ncluster-api-provider-docker-1-0-2                 true\ncluster-api-provider-gcp-1-0-3                    true\ncluster-api-provider-infoblox-1-0-1               true\ncluster-api-provider-ipam-1-0-2                   true\ncluster-api-provider-k0sproject-k0smotron-1-0-6   true\ncluster-api-provider-openstack-1-0-3              true\ncluster-api-provider-vsphere-1-0-2                true\nk0smotron-1-0-6                                   true\nkcm-1-1-0                                         true\nprojectsveltos-0-57-2                             true\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-clustertemplate-objects-are-available","title":"Verify that KCM ClusterTemplate objects are available","text":"<p>CAPI also requires control plane and bootstrap (worker node) providers to construct and/or manage different Kubernetes cluster distros and variants. Again, these providers are delivered and referenced within Mirantis k0rdent Enterprise using templates, instantiated in the management cluster as <code>ClusterTemplate</code> objects. Before continuing, verify that default <code>ClusterTemplate</code> objects are installed and valid:</p> <pre><code>kubectl get clustertemplate -n kcm-system   # list clustertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME                            VALID\nadopted-cluster-1-0-1           true\naws-eks-1-0-2                   true\naws-hosted-cp-1-0-11             true\naws-standalone-cp-1-0-12         true\nazure-aks-1-0-1                 true\nazure-hosted-cp-1-0-13           true\nazure-standalone-cp-1-0-13       true\ndocker-hosted-cp-1-0-2          true\ngcp-gke-1-0-3                   true\ngcp-hosted-cp-1-0-12             true\ngcp-standalone-cp-1-0-12         true\nopenstack-standalone-cp-1-0-11   true\nremote-cluster-1-0-9            true\nvsphere-hosted-cp-1-0-10         true\nvsphere-standalone-cp-1-0-11     true\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#verify-that-ksm-servicetemplate-objects-are-available","title":"Verify that KSM ServiceTemplate objects are available","text":"<p>Mirantis k0rdent Enterprise State Manager (KSM) uses Service Templates to lifecycle manage services and applications installed on clusters. These, too, are represented as declarative templates, instantiated as <code>ServiceTemplate</code> objects. Check that default <code>ServiceTemplate</code> objects have been created and validated:</p> <pre><code>kubectl get servicetemplate -n kcm-system   # list servicetemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre>"},{"location":"quickstarts/quickstart-1-mgmt-node-and-cluster/#next-steps","title":"Next steps","text":"<p>Your QuickStart management node is now complete, and Mirantis k0rdent Enterprise is installed and operational. Next, it's time to select AWS or Azure as an environment for hosting managed clusters.</p>"},{"location":"quickstarts/quickstart-2-aws/","title":"QuickStart 2 - AWS target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable Mirantis k0rdent Enterprise (running on your management node) to manage clusters on Amazon Web Services (AWS), and we'll deploy our first child cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an AWS account to complete this step. If you haven't yet created a management node and installed Mirantis k0rdent Enterprise, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done one of the other quickstarts, such as our Azure QuickStart (QuickStart 2 - Azure target environment), you can  use the same management cluster, continuing here with steps to add the ability to manage clusters on AWS. The Mirantis k0rdent Enterprise management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to AWS (for example, it could be on an Azure virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of Mirantis k0rdent Enterprise is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Note</p> <p> CLOUD SECURITY 101: Mirantis k0rdent Enterprise requires some but not all permissions to manage AWS via the CAPA (ClusterAPI for AWS) provider.</p> <p>Because Mirantis k0rdent Enterprise doesn't require all permissions, a best practice for using Mirantis k0rdent Enterprise with AWS (this pattern is repeated with other clouds and infrastructures) is to create a new user on your account with the particular permissions it and CAPA require. In this section, we'll create and configure IAM for that user, and perform other steps to make that Mirantis k0rdent Enterprise user's credentials accessible to it in the management node.</p> <p>Note</p> <p> If you're working on a shared AWS account, please ensure that the Mirantis k0rdent Enterprise user is not already set up before creating a new one.</p> <p>Creating a Mirantis k0rdent Enterprise user with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with Kubernetes at progressively greater scales. For more on Mirantis k0rdent Enterprise security best practices, please see the Administrator Guide.</p>"},{"location":"quickstarts/quickstart-2-aws/#install-the-aws-cli","title":"Install the AWS CLI","text":"<p>We'll use the AWS CLI to create and set IAM permissions for the Mirantis k0rdent Enterprise user, so we'll install it on our management node:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#install-clusterawsadm","title":"Install clusterawsadm","text":"<p>Mirantis k0rdent Enterprise uses Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. This QuickStart leverages <code>clusterawsadm</code>, a CLI tool created by the CAPA project that helps with AWS-specific tasks like IAM role, policy, and credential configuration.</p> <p>To install <code>clusterawsadm</code> on Ubuntu on x86 hardware:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#export-your-administrative-credentials","title":"Export your administrative credentials","text":"<p>You should have these already, preserved somewhere safe. If not, you can visit the AWS webUI (Access Management &gt; Users) and generate new credentials (Access Key ID, Secret Access Key, and Session Token (if using multi-factor authentication)).</p> <p>Export the credentials to the management node environment:</p> <pre><code>export AWS_REGION=EXAMPLE_AWS_REGION\nexport AWS_ACCESS_KEY_ID=EXAMPLE_ACCESS_KEY_ID\nexport AWS_SECRET_ACCESS_KEY=EXAMPLE_SECRET_ACCESS_KEY\nexport AWS_SESSION_TOKEN=EXAMPLE_SESSION_TOKEN # Optional. If you are using Multi-Factor or Single Sign On Auth.\n</code></pre> <p>These credentials will be used both by the AWS CLI (to create your Mirantis k0rdent Enterprise user) and by <code>clusterawsadm</code> (to create a CloudFormation template used by CAPA within Mirantis k0rdent Enterprise).</p>"},{"location":"quickstarts/quickstart-2-aws/#check-for-available-ips","title":"Check for available IPs","text":"<p>Because Mirantis k0rdent Enterprise has 3 availablilty zone NAT gateways, each cluster needs 3 public IPs. Unfortunately, the default <code>EC2-VPC Elastic IPs</code> quota per region is 5, so while you likely won't have issues with a first cluster, if you try to deplay a  second to the same region, you are likely to run into issues.  </p> <p>You can determine how many elastic IPs are available from the command line:</p> <p><pre><code>LIMIT=$(aws ec2 describe-account-attributes --attribute-names vpc-max-elastic-ips --query 'AccountAttributes[0].AttributeValues[0].AttributeValue' --output text)\nUSED=$(aws ec2 describe-addresses --query 'Addresses[*].PublicIp' --output text | wc -w)\nAVAILABLE=$((LIMIT - USED))\necho \"Available Public IPs: $AVAILABLE\"\n</code></pre> <pre><code>Available Public IPs: 5\n</code></pre></p> <p>If you have less than 3 available public IPs, you can request an increase in your quota:</p> <pre><code>aws service-quotas request-service-quota-increase \\\n    --service-code ec2 \\\n    --quota-code L-0263D0A3 \\\n    --desired-value 20\n</code></pre> <p>You can check on the status of your request:</p> <p><pre><code>aws service-quotas list-requested-service-quota-change-history \\\n    --service-code ec2\n</code></pre> <pre><code>{\n    \"RequestedQuotas\": [\n        {\n            \"Id\": \"EXAMPLE_ACCESS_KEY_ID\",\n            \"ServiceCode\": \"ec2\",\n            \"ServiceName\": \"Amazon Elastic Compute Cloud (Amazon EC2)\",\n            \"QuotaCode\": \"L-0263D0A3\",\n            \"QuotaName\": \"EC2-VPC Elastic IPs\",\n            \"DesiredValue\": 20.0,\n            \"Status\": \"PENDING\",\n            \"Created\": \"2025-02-09T02:27:01.573000-05:00\",\n            \"LastUpdated\": \"2025-02-09T02:27:01.956000-05:00\",\n            \"Requester\": \"{\\\"accountId\\\":\\\"EXAMPLE_ACCESS_KEY_ID\\\",\\\"callerArn\\\":\\\"arn:aws:iam::EXAMPLE_ACCESS_KEY_ID:user/nchase\\\"}\",\n            \"QuotaArn\": \"arn:aws:servicequotas:EXAMPLE_AWS_REGION:EXAMPLE_ACCESS_KEY_ID:ec2/L-0263D0A3\",\n            \"GlobalQuota\": false,\n            \"Unit\": \"None\",\n            \"QuotaRequestedAtLevel\": \"ACCOUNT\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-aws/#create-the-mirantis-k0rdent-enterprise-aws-user","title":"Create the Mirantis k0rdent Enterprise AWS user","text":"<p>Now we can use the AWS CLI to create a new Mirantis k0rdent Enterprise user:</p> <p><pre><code> aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <pre><code>{\n    \"User\": {\n        \"Path\": \"/\",\n        \"UserName\": \"k0rdentQuickstart\",\n        \"UserId\": \"EXAMPLE_USER_ID\",\n        \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentQuickstart\",\n        \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n    }\n}\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-aws/#configure-aws-iam-for-mirantis-k0rdent-enterprise","title":"Configure AWS IAM for Mirantis k0rdent Enterprise","text":"<p>Before Mirantis k0rdent Enterprise CAPI can manage resources on AWS, you need to use <code>clusterawsadm</code> to create a bootstrap CloudFormation stack with additional IAM policies and a service account. You do this under the administrative account credentials you earlier exported to the management node environment:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#attach-iam-policies-to-the-mirantis-k0rdent-enterprise-user","title":"Attach IAM policies to the Mirantis k0rdent Enterprise user","text":"<p>Next, we'll attach appropriate policies to the Mirantis k0rdent Enterprise user. These are:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers-eks.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> <p>We use the AWS CLI to attach them. To do this, you will need to extract the Amazon Resource Name (ARN) for the newly-created user:</p> <pre><code>AWS_ARN_ID=$(aws iam get-user --user-name k0rdentQuickstart --query 'User.Arn' --output text | grep -oP '\\d{12}')\necho $AWS_ARN_ID\n</code></pre> <p>Assemble and execute the following commands to implement the required policies:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::$AWS_ARN_ID:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> <p>We can check to see that policies were attached to the new user:</p> <p><pre><code>aws iam list-attached-user-policies --user-name k0rdentQuickstart\n</code></pre> And you'll see output that looks like this (this is non-valid example text):</p> <pre><code>{\n    \"AttachedPolicies\": [\n        {\n            \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyArn\": \"arn:aws:iam::AWS_ARN_ID:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\"\n        },\n        {\n            \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyArn\": \"arn:aws:iam::AWS_ARN_ID:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\"\n        },\n        {\n            \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyArn\": \"arn:aws:iam::AWS_ARN_ID:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\"\n        },\n        {\n            \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyArn\": \"arn:aws:iam::AWS_ARN_ID:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\"\n        }\n    ]\n}\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-aws-credentials-for-the-mirantis-k0rdent-enterprise-user","title":"Create AWS credentials for the Mirantis k0rdent Enterprise user","text":"<p>In the AWS IAM Console, you can now create the Access Key ID and Secret Access Key for the Mirantis k0rdent Enterprise user and download them. You can also do this via the AWS CLI:</p> <pre><code>aws iam create-access-key --user-name k0rdentQuickstart\n</code></pre> <p>You should see something like this. It's important to save these credentials securely somewhere other than the management node, since the management node may end up being ephemeral. Again, this is non-valid example text:</p> <pre><code>{\n    \"AccessKey\": {\n        \"UserName\": \"k0rdentQuickstart\",\n        \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n        \"Status\": \"Active\",\n        \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n        \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-iam-credentials-secret-on-the-management-cluster","title":"Create IAM credentials secret on the management cluster","text":"<p>Next, we create a <code>Secret</code> containing credentials for the Mirantis k0rdent Enterprise user and apply this to the management cluster running Mirantis k0rdent Enterprise, in the <code>kcm-system</code> namespace. Important: if you use another namespace, Mirantis k0rdent Enterprise will be unable to read the credentials. To do this, create the following YAML in a file called <code>aws-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nstringData:\n  AccessKeyID: \"EXAMPLE_ACCESS_KEY_ID\"\n  SecretAccessKey: \"EXAMPLE_SECRET_ACCESS_KEY\"\n</code></pre> <p>Remember: the Access Key ID and Secret Access Key are the ones you generated for the Mirantis k0rdent Enterprise user, <code>k0rdentQuickStart</code>.</p> <p>Apply this YAML to the management cluster as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-the-awsclusterstaticidentity-object","title":"Create the AWSClusterStaticIdentity object","text":"<p>Next, we need to create an <code>AWSClusterStaticIdentity</code> object that uses the secret.</p> <p>To do this, create a YAML file named <code>aws-cluster-identity.yaml</code> as follows:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Note that the <code>spec.secretRef</code> is the same as the <code>metadata.name</code> of the secret we just created.</p> <p>Create the object as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-the-mirantis-k0rdent-enterprise-cluster-manager-credential-object","title":"Create the Mirantis k0rdent Enterprise Cluster Manager credential object","text":"<p>Now we create the Mirantis k0rdent Enterprise Cluster Manager credential object. As in prior steps, create a YAML file called <code>aws-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> <p>Note that <code>.spec.identityRef.kind</code> must be <code>AWSClusterStaticIdentity</code> and <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>AWSClusterStaticIdentity</code> object.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-the-mirantis-k0rdent-enterprise-cluster-identity-resource-template-configmap","title":"Create the Mirantis k0rdent Enterprise Cluster Identity resource template ConfigMap","text":"<p>Now we create the Mirantis k0rdent Enterprise Cluster Identity resource template <code>ConfigMap</code>. As in prior steps, create a YAML file called <code>aws-cluster-identity-resource-template.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\n</code></pre> <p>Note that <code>ConfigMap</code> is empty. This is expected, we don't need to template any object inside child cluster(s), but we can use that object in the future if need arises.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-resource-template.yaml -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>Mirantis k0rdent Enterprise is now fully configured to manage AWS. To create a cluster, begin by listing the available <code>ClusterTemplate</code> objects provided with Mirantis k0rdent Enterprise:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>aws-standalone-cp-1-0-12</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-2                   true\nkcm-system   aws-hosted-cp-1-0-11             true\nkcm-system   aws-standalone-cp-1-0-12         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-13           true\nkcm-system   azure-standalone-cp-1-0-13       true\nkcm-system   docker-hosted-cp-1-0-2          true\nkcm-system   gcp-gke-1-0-3                   true\nkcm-system   gcp-hosted-cp-1-0-12             true\nkcm-system   gcp-standalone-cp-1-0-12         true\nkcm-system   openstack-standalone-cp-1-0-11   true\nkcm-system   remote-cluster-1-0-11            true\nkcm-system   vsphere-hosted-cp-1-0-10         true\nkcm-system   vsphere-standalone-cp-1-0-11     true\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-aws-clusterdeployment1.yaml</code>. We'll use this to create a <code>ClusterDeployment</code> object in Mirantis k0rdent Enterprise, representing the deployed cluster. The <code>ClusterDeployment</code> identifies for Mirantis k0rdent Enterprise the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under (that of your Mirantis k0rdent Enterprise user), plus the region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-12\n  credential: aws-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n      rootVolumeSize: 32      \n    worker:\n      instanceType: t3.small\n      rootVolumeSize: 32\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the <code>ClusterDeployment</code> to deploy the cluster","text":"<p>Finally, we'll apply the <code>ClusterDeployment</code> YAML (<code>my-aws-clusterdeployment1.yaml</code>) to instruct Mirantis k0rdent Enterprise to deploy the cluster:</p> <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can watch the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <p>In a short while, you'll see output such as:</p> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's <code>kubeconfig</code>:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1.kubeconfig\n</code></pre> <p>And you can use the <code>kubeconfig</code> to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-aws-clusterdeployment1.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                        READY   STATUS\nkcm-system   my-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code>:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-aws-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstarts/quickstart-2-aws/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the Mirantis k0rdent Enterprise QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of Mirantis k0rdent Enterprise setup for production</li> <li>For details about setting up Mirantis k0rdent Enterprise to manage clusters on VMware and OpenStack</li> <li>For details about using Mirantis k0rdent Enterprise with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul>"},{"location":"quickstarts/quickstart-2-azure/","title":"QuickStart 2 - Azure target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable Mirantis k0rdent Enterprise (running on your management node) to manage clusters on Azure, and deploying a child cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an Azure account to complete this step. If you haven't yet created a management node and installed Mirantis k0rdent Enterprise, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done one of the other quickstarts, such as our AWS QuickStart (QuickStart 2 - AWS target environment), you can use the same management cluster, continuing here with steps to add the ability to manage clusters on Azure. The Mirantis k0rdent Enterprise management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to Azure (for example, it could be on an AWS EC2 virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of Mirantis k0rdent Enterprise is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Note</p> <p> Cloud Security 101: Mirantis k0rdent Enterprise requires some but not all permissions to manage Azure resources via the CAPZ (ClusterAPI for Azure) provider. </p> <p>A best practice for using Mirantis k0rdent Enterprise with Azure (this pattern is repeated with other clouds and infrastructures) is to create a new Azure Cluster Identity and Service Principal (SP) on your account with the particular permissions Mirantis k0rdent Enterprise and CAPZ require. In this section, we'll create and configure those identity abstractions, and perform other steps to make required credentials accessible to Mirantis k0rdent Enterprise in the management node.</p> <p>Note</p> <p> If you're working on a shared Azure account, please ensure that the Azure Cluster Identity and Service Principal are not already set up before creating new abstractions.</p> <p>Creating user identity abstractions with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with Kubernetes at progressively greater scales. For more on Mirantis k0rdent Enterprise security best practices, please see the Administrator Guide.</p>"},{"location":"quickstarts/quickstart-2-azure/#install-the-azure-cli-az","title":"Install the Azure CLI (az)","text":"<p>The Azure CLI (az) is required to interact with Azure resources. Install it according to instructions in How to install the Azure CLI. For Linux/Debian (i.e., Ubuntu Server), it's one command:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#log-in-with-azure-cli","title":"Log in with Azure CLI","text":"<p>Run the az login command to authenticate your session with Azure.</p> <pre><code>az login\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#register-resource-providers","title":"Register resource providers","text":"<p>Azure Resource Manager uses resource providers to manage resources of all different kinds, and required providers must be registered with an Azure account before Mirantis k0rdent Enterprise and CAPZ can work with them.</p> <p>You can list resources registered with your account using Azure CLI:</p> <pre><code>az provider list --query \"[?registrationState=='Registered']\" --output table\n</code></pre> <p>And see a listing like this:</p> <pre><code>Namespace                             RegistrationState\n-----------------------------------   -----------------\nMicrosoft.Compute                     Registered\nMicrosoft.Network                     Registered\n</code></pre> <p>You can then select from the commands below (or enter all of them) to register any unregistered resources that Mirantis k0rdent Enterprise and CAPZ require:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#get-your-azure-subscription-id","title":"Get your Azure Subscription ID","text":"<p>Use the following command to list Azure subscriptions and their IDs:</p> <pre><code>az account list -o table\n</code></pre> <p>The output will look like this:</p> <pre><code>Name                     SubscriptionId                    TenantId\n-----------------------  -------------------------------   -----------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID   TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre> <p>The Subcription ID is in the second column.</p>"},{"location":"quickstarts/quickstart-2-azure/#create-a-service-principal-for-mirantis-k0rdent-enterprise","title":"Create a Service Principal for Mirantis k0rdent Enterprise","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. To create it, run the following command with the Azure CLI, replacing <code>&lt;subscription-id&gt;</code> with the ID you copied earlier.</p> <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <p>You'll see output that resembles what's below:</p> <pre><code>{\n \"appId\": \"SP_APP_ID_SP_APP_ID\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"SP_PASSWORD_SP_PASSWORD\",\n \"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> <p>Capture this output and secure the values it contains. We'll need several of these in a moment.</p>"},{"location":"quickstarts/quickstart-2-azure/#create-a-secret-object-with-the-azure-credentials","title":"Create a Secret object with the Azure credentials","text":"<p>In this quickstart we're assuming a self-managed Azure clusters (non-AKS) so create a <code>Secret</code> object that stores the <code>clientSecret</code> (password) from the Service Principal. Create a YAML file called <code>azure-cluster-identity-secret.yaml</code>, as follows, inserting the password for the Service Principal (represented by the placeholder <code>SP_PASSWORD_SP_PASSWORD</code> above):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clientSecret: SP_PASSWORD_SP_PASSWORD # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Apply the YAML to the Mirantis k0rdent Enterprise management cluster using the following command:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>secret/azure-cluster-identity-secret created\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#create-the-azureclusteridentity-object","title":"Create the AzureClusterIdentity Object","text":"<p>This object defines the credentials Mirantis k0rdent Enterprise and CAPZ will use to manage Azure resources. It references the <code>Secret</code> you just created above.</p> <p>Create a YAML file called <code>azure-cluster-identity.yaml</code>. Make sure that <code>.spec.clientSecret.name</code> matches the <code>metadata.name</code> in the file you created above.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  name: azure-cluster-identity\n  namespace: kcm-system\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  allowedNamespaces: {}\n  clientID: SP_APP_ID_SP_APP_ID # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: SP_TENANT_SP_TENANT # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <p>Note that for non-AKS clusters <code>.spec.kind</code> must be <code>AzureClusterIdentity</code>, and <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>AzureClusterIdentity</code> object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#create-the-configmap-resource-template-object","title":"Create the <code>ConfigMap</code> resource-template Object","text":"<p>Create a YAML with the specification of our resource-template (and the necessary <code>StorageClass</code>) and save it as <code>azure-cluster-identity-resource-template.yaml</code></p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: azure-cluster-identity-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $cluster := .InfrastructureProvider -}}\n    {{- $identity := (getResource \"InfrastructureProviderIdentity\") -}}\n    {{- $secret := (getResource \"InfrastructureProviderIdentitySecret\") -}}\n    {{- $subnetName := \"\" -}}\n    {{- $securityGroupName := \"\" -}}\n    {{- $routeTableName := \"\" -}}\n    {{- range $cluster.spec.networkSpec.subnets -}}\n      {{- if eq .role \"node\" -}}\n        {{- $subnetName = .name -}}\n        {{- $securityGroupName = .securityGroup.name -}}\n        {{- $routeTableName = .routeTable.name -}}\n        {{- break -}}\n      {{- end -}}\n    {{- end -}}\n    {{- $cloudConfig := dict\n      \"aadClientId\" $identity.spec.clientID\n      \"aadClientSecret\" (index $secret.data \"clientSecret\" | b64dec)\n      \"cloud\" $cluster.spec.azureEnvironment\n      \"loadBalancerName\" \"\"\n      \"loadBalancerSku\" \"Standard\"\n      \"location\" $cluster.spec.location\n      \"maximumLoadBalancerRuleCount\" 250\n      \"resourceGroup\" $cluster.spec.resourceGroup\n      \"routeTableName\" $routeTableName\n      \"securityGroupName\" $securityGroupName\n      \"securityGroupResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n      \"subnetName\" $subnetName\n      \"subscriptionId\" $cluster.spec.subscriptionID\n      \"tenantId\" $identity.spec.tenantID\n      \"useInstanceMetadata\" true\n      \"useManagedIdentityExtension\" false\n      \"vmType\" \"vmss\"\n      \"vnetName\" $cluster.spec.networkSpec.vnet.name\n      \"vnetResourceGroup\" $cluster.spec.networkSpec.vnet.resourceGroup\n    -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: azure-cloud-provider\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-config: {{ $cloudConfig | toJson | b64enc }}\n    ---\n    apiVersion: storage.k8s.io/v1\n    kind: StorageClass\n    metadata:\n      name: managed-csi\n      annotations:\n        storageclass.kubernetes.io/is-default-class: \"true\"\n    provisioner: disk.csi.azure.com\n    parameters:\n      skuName: StandardSSD_LRS\n    reclaimPolicy: Delete\n    volumeBindingMode: WaitForFirstConsumer\n    allowVolumeExpansion: true\n</code></pre> <p>Object name needs to be exactly <code>azure-cluster-identity-resource-template.yaml</code>, <code>AzureClusterIdentity</code> object name + <code>-resource-template</code> string suffix.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-resource-template.yaml\n</code></pre> <pre><code>configmap/azure-cluster-identity-resource-template created\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-azure/#find-your-locationregion","title":"Find your location/region","text":"<p>To determine where to deploy your cluster, you may wish to begin by listing your Azure location/regions:</p> <pre><code>az account list-locations -o table\n</code></pre> <p>You'll see output like this:</p> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n. . .\n</code></pre> <p>What you'll need to insert in your <code>ClusterDeployment</code> is the name (center column) of the region to which you wish to deploy.</p>"},{"location":"quickstarts/quickstart-2-azure/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>Mirantis k0rdent Enterprise is now fully configured to manage Azure. To create a cluster, begin by listing the available <code>ClusterTemplate</code> objects:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the Azure standalone cluster template in its present version (in the example below, that's <code>azure-standalone-cp-1-0-13</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-2                   true\nkcm-system   aws-hosted-cp-1-0-11             true\nkcm-system   aws-standalone-cp-1-0-12         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-13           true\nkcm-system   azure-standalone-cp-1-0-13       true\nkcm-system   docker-hosted-cp-1-0-2          true\nkcm-system   gcp-gke-1-0-3                   true\nkcm-system   gcp-hosted-cp-1-0-12             true\nkcm-system   gcp-standalone-cp-1-0-12         true\nkcm-system   openstack-standalone-cp-1-0-11   true\nkcm-system   remote-cluster-1-0-11            true\nkcm-system   vsphere-hosted-cp-1-0-10         true\nkcm-system   vsphere-standalone-cp-1-0-11     true\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-azure-clusterdeployment1.yaml</code>. We'll use this to create a <code>ClusterDeployment</code> object in Mirantis k0rdent Enterprise, representing the deployed cluster. The <code>ClusterDeployment</code> identifies for Mirantis k0rdent Enterprise the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under, plus the location/region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-1-0-13 # name of the clustertemplate\n  credential: azure-cluster-identity-cred\n  config:\n    clusterLabels: {}\n    location: \"AZURE_LOCATION\" # Select your desired Azure Location\n    subscriptionID: SUBSCRIPTION_ID_SUBSCRIPTION_ID # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the <code>ClusterDeployment</code> YAML (<code>my-azure-clusterdeployment1.yaml</code>) to instruct Mirantis k0rdent Enterprise to deploy the cluster:</p> <pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's <code>kubeconfig</code>:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the <code>kubeconfig</code> to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstarts/quickstart-2-azure/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-azure/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-azure/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the Mirantis k0rdent Enterprise QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of Mirantis k0rdent Enterprise setup for production</li> <li>For details about setting up Mirantis k0rdent Enterprise to manage clusters on VMware and OpenStack</li> <li>For details about using Mirantis k0rdent Enterprise with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul>"},{"location":"quickstarts/quickstart-2-gcp/","title":"QuickStart 2 - GCP target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable Mirantis k0rdent Enterprise (running on your management node) to manage clusters on GCP and deploying a child cluster.</p> <p>Note that if you have already done one of the other quickstarts, such as our AWS or Azure QuickStart (QuickStart 2 - AWS target environment) or Azure QuickStart (QuickStart 2 - Azure target environment),  or  you can use the same management cluster, continuing here with steps to add the ability to manage clusters on GCP. The  Mirantis k0rdent Enterprise management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to GCP (for example, it could be on  an AWS EC2 virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should  work fine. A big benefit of Mirantis k0rdent Enterprise is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>The GCP provider is available starting with the Mirantis k0rdent Enterprise 0.2.0 release, but it's not enabled by default. To enable the GCP provider in Mirantis k0rdent Enterprise, edit the <code>Management</code> object and add <code>cluster-api-provider-gcp</code> to the list of <code>spec.providers</code>, and the GCP components will start within a few minutes.</p>"},{"location":"quickstarts/quickstart-2-gcp/#install-gcloud-cli","title":"Install Gcloud CLI","text":"<p>Follow Install the gcloud CLI instruction to install gcloud CLI on your local machine.</p>"},{"location":"quickstarts/quickstart-2-gcp/#authenticate-in-gcp-cloud","title":"Authenticate in GCP cloud","text":"<pre><code>gcloud auth login\n</code></pre> <p>By default, this command will obtain access credentials for your user account via a web-based authorization flow.</p> <p>Set the project in which you want to deploy a cluster. Run:</p> <pre><code>gcloud config set project PROJECT_ID\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#create-a-gcp-service-account","title":"Create a GCP Service Account","text":"<p>Note</p> <p> Skip this step if the Service Account already configured Follow the GCP Service Account creation guide and create a new service account with <code>Editor</code> permissions.</p> <p>If you have plans to deploy <code>GKE</code>, the Service Account will also need the <code>iam.serviceAccountTokenCreator</code> role.</p>"},{"location":"quickstarts/quickstart-2-gcp/#generate-json-key-for-the-gcp-service-account","title":"Generate JSON Key for the GCP Service Account","text":"<p>Note</p> <p> Skip this step if you're going to use an existing key Follow the Create a service account key guide and create a new key with the JSON key type.</p> <p>A JSON file will be automatically downloaded to your computer. You should keep it somewhere safe.</p> <p>The example of the JSON file: <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"project_id\",\n  \"private_key_id\": \"akdof8v8s6n39n29251be52cabbb3984c259f1\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"name@project_id.iam.gserviceaccount.com\",\n  \"client_id\": \"123456778\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/user%40project_id.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-gcp/#create-a-secret-object-with-the-gcp-gke-credentials","title":"Create a Secret object with the GCP (GKE) credentials","text":"<p>Create a <code>Secret</code> object that stores the <code>credentials</code> field under <code>data</code> section. Create a YAML file called <code>gcp-cluster-identity-secret.yaml</code>, as follows, inserting the base64-encoded GCP credentials (represented by the placeholder <code>GCP_B64ENCODED_CREDENTIALS</code> below) that you get on the previous step. To get base64 encoded credentials, run:</p> <pre><code>    GCP_B64ENCODED_CREDENTIALS=$(cat &lt;gcpJSONCredentialsFileName&gt; | base64 -w 0)\n</code></pre> <p><pre><code>cat &gt; gcp-cluster-identity-secret.yaml &lt;&lt; EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: gcp-cloud-sa\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ndata:\n  # the secret key should always equal `credentials`\n  credentials: ${GCP_B64ENCODED_CREDENTIALS}\ntype: Opaque\nEOF \n</code></pre> Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f gcp-cluster-identity-secret.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>secret/gcp-cloud-sa created\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>gcp-credential.yaml</code>.</p> <p>Note that <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>Secret</code> object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: gcp-credential\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: gcp-cloud-sa\n    namespace: kcm-system\n</code></pre> <pre><code>kubectl apply -f gcp-credential.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>credential.k0rdent.mirantis.com/gcp-credential created\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#create-the-configmap-resource-template-object","title":"Create the <code>ConfigMap</code> resource-template Object","text":"<p>Create a YAML with the specification of our resource-template and save it as <code>gcp-cloud-sa-resource-template.yaml</code></p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: gcp-cloud-sa-resource-template\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\ndata:\n  configmap.yaml: |\n    {{- $secret := (getResource \"InfrastructureProviderIdentity\") -}}\n    ---\n    apiVersion: v1\n    kind: Secret\n    metadata:\n      name: gcp-cloud-sa\n      namespace: kube-system\n    type: Opaque\n    data:\n      cloud-sa.json: {{ index $secret \"data\" \"credentials\" }}\n</code></pre> <p>Object name needs to be exactly <code>gcp-cloud-sa-resource-template</code> (credentials <code>Secret</code> object name + <code>-resource-template</code> string suffix).</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f gcp-cloud-sa-resource-template.yaml\n</code></pre> <pre><code>configmap/gcp-cloud-sa-resource-template created\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-gcp/#find-your-locationregion","title":"Find your location/region","text":"<p>To determine where to deploy your cluster, you may wish to begin by listing your GCP regions:</p> <pre><code>gcloud compute regions list\n</code></pre> <p>You'll see output like this:</p> <pre><code>NAME                     CPUS    DISKS_GB  ADDRESSES  RESERVED_ADDRESSES  STATUS  TURNDOWN_DATE\nafrica-south1            0/300   0/102400  0/575      0/175               UP\nasia-east1               0/3000  0/102400  0/575      0/175               UP\nasia-east2               0/1500  0/102400  0/575      0/175               UP\nasia-northeast1          0/1500  0/102400  0/575      0/175               UP\nasia-northeast2          0/750   0/102400  0/575      0/175               UP\n. . .\n</code></pre> <p>What you'll need to insert in your ClusterDeployment is the name (first column) of the region you wish to deploy to.</p>"},{"location":"quickstarts/quickstart-2-gcp/#optional-find-or-create-your-network","title":"(optional) Find or create your network","text":"<p>If you want to deploy in existing network, obtain all network names using <code>gcloud</code> CLI and choose one of available: <pre><code>gcloud compute networks list --format=\"value(name)\"\n</code></pre></p> <p>If you prefer to create a new network, follow this instruction.</p>"},{"location":"quickstarts/quickstart-2-gcp/#determine-the-instance-type","title":"Determine the instance type","text":"<p>Find available machine types and its parameters by running:</p> <pre><code>gcloud compute machine-types list | grep REGION\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#find-available-images","title":"Find available images","text":"<p>To list all available images in your GCP project, run:</p> <pre><code>gcloud compute images list --uri\n</code></pre> <p>You'll see output like this:</p> <pre><code>https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-stream-9-v20250311\nhttps://www.googleapis.com/compute/v1/projects/cos-cloud/global/images/cos-105-17412-535-84\nhttps://www.googleapis.com/compute/v1/projects/cos-cloud/global/images/cos-109-17800-436-79\nhttps://www.googleapis.com/compute/v1/projects/cos-cloud/global/images/cos-113-18244-291-82\n. . .\n</code></pre> <p>To build your own image, follow Building images instruction.</p> <p>Replace <code>REGION</code> with the region you'd like your cluster to live.</p>"},{"location":"quickstarts/quickstart-2-gcp/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>Mirantis k0rdent Enterprise is now fully configured to manage GCP. To create a cluster, begin by listing the available ClusterTemplates provided with Mirantis k0rdent Enterprise:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the GCP standalone cluster template in its present version (in the example below, that's <code>gcp-standalone-cp-1-0-12</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-2                   true\nkcm-system   aws-hosted-cp-1-0-11             true\nkcm-system   aws-standalone-cp-1-0-12         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-13           true\nkcm-system   azure-standalone-cp-1-0-13       true\nkcm-system   docker-hosted-cp-1-0-2          true\nkcm-system   gcp-gke-1-0-3                   true\nkcm-system   gcp-hosted-cp-1-0-12             true\nkcm-system   gcp-standalone-cp-1-0-12         true\nkcm-system   openstack-standalone-cp-1-0-11   true\nkcm-system   remote-cluster-1-0-11            true\nkcm-system   vsphere-hosted-cp-1-0-10         true\nkcm-system   vsphere-standalone-cp-1-0-11     true\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-gcp-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in Mirantis k0rdent Enterprise, representing the deployed cluster. The <code>ClusterDeployment</code> identifies for Mirantis k0rdent Enterprise the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under, the GCP project, region, network name, machine types and images you want to use to host control plane and worker nodes.</p> <p>Note</p> <p> If you decide to deploy cluster with <code>publicIP: false</code>, you should make sure your cluster can communicate with the outside world and the load balancer. To achieve this, you may create a Cloud NAT in the region you'd like your cluster to live in by following Set up and manage network address translation with Public NAT instruction.</p> <p>Note</p> <p> Image parameter must be the fully qualified GCP image path. Otherwise, the GCP cloud controller manager will not find it.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-gcp-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: gcp-standalone-cp-1-0-12 # name of the clustertemplate\n  credential: gcp-credential\n  config:\n    project: \"GCP_PROJECT_NAME\"\n    region: \"GCP_REGION\"\n    network:\n      name: \"GCP_NETWORK_NAME\" # The name of an existing network or a new network to be created by Cluster API Provider GCP\n    controlPlane:\n      instanceType: \"CP_MACHINE_TYPE\"\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n    worker:\n      instanceType: \"WORKER_MACHINE_TYPE\"\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-gcp-clusterdeployment1.yaml</code>) to instruct Mirantis k0rdent Enterprise to deploy the cluster:</p> <pre><code>kubectl apply -f my-gcp-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-gcp-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-gcp-clusterdeployment1 --watch\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-gcp-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-gcp-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-gcp-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstarts/quickstart-2-gcp/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-gcp-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-gcp/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl delete ClusterDeployment my-gcp-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-gcp-clusterdeployment1\" deleted\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-gcp/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the Mirantis k0rdent Enterprise QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of Mirantis k0rdent Enterprise setup for production</li> <li>For details about setting up Mirantis k0rdent Enterprise to manage clusters on VMware and OpenStack</li> <li>For details about using Mirantis k0rdent Enterprise with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of Mirantis k0rdent Enterprise's key features!</p>"},{"location":"quickstarts/quickstart-2-remote/","title":"QuickStart 2 - Existing infrastructure","text":"<p>In many cases, you will want Mirantis k0rdent Enterprise (running on your management node) to deploy a k0s child cluster on existing infrastructure. The remote servers will serve as worker nodes in the cluster, while the control plane components will reside within the management cluster and be managed by k0smotron. By doing this, you maximize your existing hardware and simplify administration costs by decreasing the number of servers required to run your infrastructure.</p> <p>The remote machines that will be part of the cluster must meet the following prerequisites:</p> <ol> <li>Linux-based operating system; the remote hosts should meet the k0s system requirements</li> <li>SSH access enabled for the root user</li> <li>Internet access</li> <li>Connectivity between the management cluster and the remote hosts' networks</li> </ol> <p>If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done one of the other quickstarts, such as our AWS QuickStart (QuickStart 2 - AWS target environment) or (QuickStart 2 - Azure target environment), you can use the same management cluster, continuing here with steps to add the ability to manage remote clusters. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Before proceeding, make sure your management cluster meets the following requirements:</p> <ol> <li>A default storage class is configured on the management cluster to support Persistent Volumes.</li> <li>If the API server will be exposed as a <code>LoadBalancer</code>, ensure the appropriate cloud provider is installed on the management cluster.</li> </ol>"},{"location":"quickstarts/quickstart-2-remote/#create-a-secret-object-containing-the-private-ssh-key-to-access-remote-machines","title":"Create a Secret object containing the private SSH key to access remote machines","text":"<p>Create a <code>Secret</code> object to securely store the private SSH key, under the key <code>value</code>, for accessing all remote machines that will be part of the cluster. Save this configuration in a YAML file named <code>remote-ssh-key-secret.yaml</code>. Ensure you replace the placeholder <code>PRIVATE_SSH_KEY_B64</code> with your base64-encoded private SSH key:</p> <pre><code># Setup Environment\nKEY_PATH=~/.ssh/id_ed25519\nPRIVATE_SSH_KEY_B64=$(cat $KEY_PATH | base64 -w 0)\nSECRET_NAME=remote-ssh-key\nKCM_SYSTEM_NS=kcm-system\nCREDENTIAL_NAME=remote-cred\nRESOURCE_TEMPLATE_NAME=remote-ssh-key-resource-template\nCLUSTER_DEPLOYMENT_NAME=my-remote-clusterdeployment1\nMACHINE_0_ADDRESS=127.0.0.1\nMACHINE_1_ADDRESS=127.0.0.2\n</code></pre> <pre><code>cat &gt; remote-ssh-key-secret.yaml &lt;&lt; EOF\napiVersion: v1\ndata:\n  value: $PRIVATE_SSH_KEY_B64\nkind: Secret\nmetadata:\n  name: $SECRET_NAME\n  namespace: $KCM_SYSTEM_NS\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nEOF\n</code></pre> <p>Apply the YAML to the Mirantis k0rdent Enterprise management cluster: <pre><code>kubectl apply -f remote-ssh-key-secret.yaml\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-remote/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML file with the specification of our credential and save it as <code>remote-cred.yaml</code>.</p> <p>Note that <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>Secret</code> object created in the previous step.</p> <pre><code>cat &gt; remote-cred.yaml &lt;&lt; EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: $CREDENTIAL_NAME\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: $SECRET_NAME\n    namespace: $KCM_SYSTEM_NS\nEOF\n</code></pre> <p>Apply the YAML to your cluster: <pre><code>kubectl apply -f remote-cred.yaml\n</code></pre></p> <p>You should see output of:</p> <pre><code>credential.k0rdent.mirantis.com/remote-cred created\n</code></pre>"},{"location":"quickstarts/quickstart-2-remote/#create-the-cluster-identity-resource-template-configmap","title":"Create the Cluster Identity resource template ConfigMap","text":"<p>Now we create the Mirantis k0rdent Enterprise Cluster Identity resource template <code>ConfigMap</code>. As in prior steps, create a YAML file called <code>remote-ssh-key-resource-template.yaml</code>:</p> <p><pre><code>cat &gt; remote-ssh-key-resource-template.yaml &lt;&lt; EOF\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: $RESOURCE_TEMPLATE_NAME\n  namespace: $KCM_SYSTEM_NS\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\n  annotations:\n    projectsveltos.io/template: \"true\"\nEOF\n</code></pre> Note that the <code>ConfigMap</code> is empty. This is expected, as we don't need to template any object inside child clusters, but we can use that object in the future if the need arises.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f remote-ssh-key-resource-template.yaml -n kcm-system\n</code></pre>"},{"location":"quickstarts/quickstart-2-remote/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>To create a remote cluster, begin by listing the available <code>ClusterTemplate</code> objects provided with Mirantis k0rdent Enterprise:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling the following. Make note of the name of the Remote Cluster template in its present version (in the example below, that's <code>remote-cluster-1-0-13</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-2                   true\nkcm-system   aws-hosted-cp-1-0-11             true\nkcm-system   aws-standalone-cp-1-0-12         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-13           true\nkcm-system   azure-standalone-cp-1-0-13       true\nkcm-system   docker-hosted-cp-1-0-2          true\nkcm-system   gcp-gke-1-0-3                   true\nkcm-system   gcp-hosted-cp-1-0-12             true\nkcm-system   gcp-standalone-cp-1-0-12         true\nkcm-system   openstack-standalone-cp-1-0-11   true\nkcm-system   remote-cluster-1-0-11            true\nkcm-system   vsphere-hosted-cp-1-0-10         true\nkcm-system   vsphere-standalone-cp-1-0-11     true\n</code></pre>"},{"location":"quickstarts/quickstart-2-remote/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>To deploy a cluster, create a YAML file called <code>my-remote-clusterdeployment1.yaml</code>. We'll use this to create a <code>ClusterDeployment</code> object representing the deployed cluster. The <code>ClusterDeployment</code> identifies for Mirantis k0rdent Enterprise the <code>ClusterTemplate</code> you want to use for cluster creation, the identity credential object you want to create it under, plus the machines' IP addresses (represented by the placeholder <code>MACHINE_0_ADDRESS</code> and <code>MACHINE_1_ADDRESS</code> below), the SSH port of the remote machines and the user to use when connecting to remote machines (<code>root</code>):</p> <p>Note</p> <p> The user must have root permissions.  Also, the service type should be correctly configured. If using the <code>LoadBalancer</code> service type, ensure the appropriate cloud provider is installed on the management cluster. For other service types (such as <code>ClusterIP</code> or <code>NodePort</code>), verify that the management cluster network is accessible from the host machines to allow virtual machines to connect to the API server.</p> <pre><code>cat &gt; my-remote-clusterdeployment1.yaml &lt;&lt; EOF\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: $CLUSTER_DEPLOYMENT_NAME\n  namespace: $KCM_SYSTEM_NS\nspec:\n  template: remote-cluster-1-0-9 # name of the clustertemplate\n  credential: remote-cred\n  propagateCredentials: false\n  config:\n    k0smotron:\n      service:\n        type: LoadBalancer\n    machines:\n    - address: $MACHINE_0_ADDRESS\n      user: root # The user must have root permissions \n      port: 22\n    - address: $MACHINE_1_ADDRESS\n      user: root # The user must have root permissions \n      port: 22\nEOF\n</code></pre>"},{"location":"quickstarts/quickstart-2-remote/#apply-the-clusterdeployment-to-deploy-the-management-cluster","title":"Apply the ClusterDeployment to deploy the management cluster","text":"<p>Finally, we'll apply the <code>ClusterDeployment</code> YAML (<code>my-remote-clusterdeployment1.yaml</code>) to instruct Mirantis k0rdent Enterprise to deploy the cluster:</p> <pre><code>kubectl apply -f my-remote-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm the creation:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-remote-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusters.cluster.x-k8s.io my-remote-clusterdeployment1 --watch\n</code></pre> <p>To verify that the remote machines were successfuly provisioned, run:</p> <pre><code>kubectl -n kcm-system get remotemachines.infrastructure.cluster.x-k8s.io -l helm.toolkit.fluxcd.io/name=my-remote-clusterdeployment1 -o=jsonpath={.items[*].status}\n</code></pre> <p>If the machines were provisioned, the output of this command will be similar to:</p> <pre><code>{\"ready\":true}{\"ready\":true}\n</code></pre> <p>If there is any error, the output will contain an error message.</p>"},{"location":"quickstarts/quickstart-2-remote/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-remote-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-remote-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-remote-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quickstarts/quickstart-2-remote/#list-child-clusters","title":"List child clusters","text":"<p>To verify the presence of the child cluster, list the available <code>ClusterDeployment</code> objects on the management cluster:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-remote-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-remote/#tear-down-the-child-cluster","title":"Tear down the child cluster","text":"<p>To tear down the child cluster, delete the <code>ClusterDeployment</code> from the management cluster:</p> <p><pre><code>kubectl delete ClusterDeployment my-remote-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-remote-clusterdeployment1\" deleted\n</code></pre></p>"},{"location":"quickstarts/quickstart-2-remote/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of Mirantis k0rdent Enterprise setup for production</li> <li>For details about setting up Mirantis k0rdent Enterprise to manage clusters on VMware and OpenStack</li> <li>For details about using Mirantis k0rdent Enterprise with cloud Kubernetes distros such as AWS EKS and Azure AKS</li> </ul>"},{"location":"reference/","title":"Reference Guides","text":"<p>The k0rdent documentation includes references to:</p> <ul> <li>k0rdent CRDs</li> <li>k0rdent Templates</li> </ul>"},{"location":"reference/crds/","title":"API Reference","text":"<p>Packages:</p> <ul> <li>k0rdent.mirantis.com/v1alpha</li> <li>k0rdent.mirantis.com/v1beta1</li> </ul>"},{"location":"reference/crds/#k0rdentmirantiscomv1beta1","title":"k0rdent.mirantis.com/v1beta1","text":"<p>Resource Types:</p> <ul> <li> <p>AccessManagement</p> </li> <li> <p>ClusterDeployment</p> </li> <li> <p>ClusterTemplateChain</p> </li> <li> <p>ClusterTemplate</p> </li> <li> <p>Credential</p> </li> <li> <p>ManagementBackup</p> </li> <li> <p>Management</p> </li> <li> <p>MultiClusterService</p> </li> <li> <p>ProviderInterface</p> </li> <li> <p>ProviderTemplate</p> </li> <li> <p>Release</p> </li> <li> <p>ServiceTemplateChain</p> </li> <li> <p>ServiceTemplate</p> </li> </ul>"},{"location":"reference/crds/#accessmanagement","title":"AccessManagement","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessManagement is the Schema for the AccessManagements API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string AccessManagement true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            AccessManagementSpec defines the desired state of AccessManagement false status object            AccessManagementStatus defines the observed state of AccessManagement false"},{"location":"reference/crds/#accessmanagementspec","title":"AccessManagement.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessManagementSpec defines the desired state of AccessManagement</p> Name Type Description Required accessRules []object            AccessRules is the list of access rules. Each AccessRule enforces objects distribution to the TargetNamespaces. false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindex","title":"AccessManagement.spec.accessRules[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessRule is the definition of the AccessManagement access rule. Each AccessRule enforces Templates and Credentials distribution to the TargetNamespaces</p> Name Type Description Required clusterTemplateChains []string            ClusterTemplateChains lists the names of ClusterTemplateChains whose ClusterTemplates will be distributed to all namespaces specified in TargetNamespaces. false credentials []string            Credentials is the list of Credential names that will be distributed to all the namespaces specified in TargetNamespaces. false serviceTemplateChains []string            ServiceTemplateChains lists the names of ServiceTemplateChains whose ServiceTemplates will be distributed to all namespaces specified in TargetNamespaces. false targetNamespaces object            TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset. Validations:<li>((has(self.stringSelector) ? 1 : 0) + (has(self.selector) ? 1 : 0) + (has(self.list) ? 1 : 0)) &lt;= 1: only one of spec.targetNamespaces.selector or spec.targetNamespaces.stringSelector or spec.targetNamespaces.list can be specified</li> false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindextargetnamespaces","title":"AccessManagement.spec.accessRules[index].targetNamespaces","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset.</p> Name Type Description Required list []string            List is the list of namespaces to select. Mutually exclusive with StringSelector and Selector. false selector object            Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List. false stringSelector string            StringSelector is a label query to select namespaces. Mutually exclusive with Selector and List. false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindextargetnamespacesselector","title":"AccessManagement.spec.accessRules[index].targetNamespaces.selector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindextargetnamespacesselectormatchexpressionsindex","title":"AccessManagement.spec.accessRules[index].targetNamespaces.selector.matchExpressions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"reference/crds/#accessmanagementstatus","title":"AccessManagement.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessManagementStatus defines the observed state of AccessManagement</p> Name Type Description Required current []object            Current reflects the applied access rules configuration. false error string            Error is the error message occurred during the reconciliation (if any) false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false"},{"location":"reference/crds/#accessmanagementstatuscurrentindex","title":"AccessManagement.status.current[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessRule is the definition of the AccessManagement access rule. Each AccessRule enforces Templates and Credentials distribution to the TargetNamespaces</p> Name Type Description Required clusterTemplateChains []string            ClusterTemplateChains lists the names of ClusterTemplateChains whose ClusterTemplates will be distributed to all namespaces specified in TargetNamespaces. false credentials []string            Credentials is the list of Credential names that will be distributed to all the namespaces specified in TargetNamespaces. false serviceTemplateChains []string            ServiceTemplateChains lists the names of ServiceTemplateChains whose ServiceTemplates will be distributed to all namespaces specified in TargetNamespaces. false targetNamespaces object            TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset. Validations:<li>((has(self.stringSelector) ? 1 : 0) + (has(self.selector) ? 1 : 0) + (has(self.list) ? 1 : 0)) &lt;= 1: only one of spec.targetNamespaces.selector or spec.targetNamespaces.stringSelector or spec.targetNamespaces.list can be specified</li> false"},{"location":"reference/crds/#accessmanagementstatuscurrentindextargetnamespaces","title":"AccessManagement.status.current[index].targetNamespaces","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset.</p> Name Type Description Required list []string            List is the list of namespaces to select. Mutually exclusive with StringSelector and Selector. false selector object            Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List. false stringSelector string            StringSelector is a label query to select namespaces. Mutually exclusive with Selector and List. false"},{"location":"reference/crds/#accessmanagementstatuscurrentindextargetnamespacesselector","title":"AccessManagement.status.current[index].targetNamespaces.selector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"reference/crds/#accessmanagementstatuscurrentindextargetnamespacesselectormatchexpressionsindex","title":"AccessManagement.status.current[index].targetNamespaces.selector.matchExpressions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"reference/crds/#clusterdeployment","title":"ClusterDeployment","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterDeployment is the Schema for the ClusterDeployments API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterDeployment true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterDeploymentSpec defines the desired state of ClusterDeployment false status object            ClusterDeploymentStatus defines the observed state of ClusterDeployment false"},{"location":"reference/crds/#clusterdeploymentspec","title":"ClusterDeployment.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterDeploymentSpec defines the desired state of ClusterDeployment</p> Name Type Description Required template string            Template is a reference to a Template object located in the same namespace. true config JSON            Config allows to provide parameters for template customization. If no Config provided, the field will be populated with the default values for the template and DryRun will be enabled. false credential string            Name reference to the related Credentials object. false dryRun boolean            DryRun specifies whether the template should be applied after validation or only validated. false propagateCredentials boolean            PropagateCredentials indicates whether credentials should be propagated for use by CCM (Cloud Controller Manager). Default: true false serviceSpec object            ServiceSpec is spec related to deployment of services. false"},{"location":"reference/crds/#clusterdeploymentspecservicespec","title":"ClusterDeployment.spec.serviceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceSpec is spec related to deployment of services.</p> Name Type Description Required continueOnError boolean            ContinueOnError specifies if the services deployment should continue if an error occurs. Default: false false driftExclusions []object            DriftExclusions specifies specific configurations of resources to ignore for drift detection. false driftIgnore []object            DriftIgnore specifies resources to ignore for drift detection. false priority integer            Priority sets the priority for the services defined in this spec. Higher value means higher priority and lower means lower. In case of conflict with another object managing the service, the one with higher priority will get to deploy its services. Format: int32 Default: 100 Minimum: 1 Maximum: 2.147483646e+09 false reload boolean            Reload instances via rolling upgrade when a ConfigMap/Secret mounted as volume is modified. false services []object            Services is a list of services created via ServiceTemplates that could be installed on the target cluster. false stopOnConflict boolean            StopOnConflict specifies what to do in case of a conflict. E.g. If another object is already managing a service. By default the remaining services will be deployed even if conflict is detected. If set to true, the deployment will stop after encountering the first conflict. Default: false false syncMode enum            SyncMode specifies how services are synced in the target cluster. Enum: OneTime, Continuous, ContinuousWithDriftDetection, DryRun Default: Continuous false templateResourceRefs []object            TemplateResourceRefs is a list of resources to collect from the management cluster, the values from which can be used in templates. false"},{"location":"reference/crds/#clusterdeploymentspecservicespecdriftexclusionsindex","title":"ClusterDeployment.spec.serviceSpec.driftExclusions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required paths []string            Paths is a slice of JSON6902 paths to exclude from configuration drift evaluation. true target object            Target points to the resources that the paths refers to. false"},{"location":"reference/crds/#clusterdeploymentspecservicespecdriftexclusionsindextarget","title":"ClusterDeployment.spec.serviceSpec.driftExclusions[index].target","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Target points to the resources that the paths refers to.</p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#clusterdeploymentspecservicespecdriftignoreindex","title":"ClusterDeployment.spec.serviceSpec.driftIgnore[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#clusterdeploymentspecservicespecservicesindex","title":"ClusterDeployment.spec.serviceSpec.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Service represents a Service to be deployed.</p> Name Type Description Required name string            Name is the chart release. true template string            Template is a reference to a Template object located in the same namespace. true disable boolean            Disable can be set to disable handling of this service. false namespace string            Namespace is the namespace the release will be installed in. It will default to Name if not provided. false templateChain string            TemplateChain defines the ServiceTemplateChain object that will be used to deploy the service along with desired ServiceTemplate version. false values string            Values is the helm values to be passed to the chart used by the template. The string type is used in order to allow for templating. false valuesFrom []object            ValuesFrom can reference a ConfigMap or Secret containing helm values. false"},{"location":"reference/crds/#clusterdeploymentspecservicespecservicesindexvaluesfromindex","title":"ClusterDeployment.spec.serviceSpec.services[index].valuesFrom[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required kind enum            Kind of the resource. Supported kinds are: - ConfigMap/Secret Enum: ConfigMap, Secret true name string            Name of the referenced resource. Name can be expressed as a template and instantiate using any cluster field. true namespace string            Namespace of the referenced resource. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. For Profile namespace must be left empty. The Profile namespace will be used. Namespace can be expressed as a template and instantiate using any cluster field. false optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other ValueFroms. Default: false false"},{"location":"reference/crds/#clusterdeploymentspecservicespectemplateresourcerefsindex","title":"ClusterDeployment.spec.serviceSpec.templateResourceRefs[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required identifier string            Identifier is how the resource will be referred to in the template true resource object            Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field. true optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other TemplateResourceRefs. Default: false false"},{"location":"reference/crds/#clusterdeploymentspecservicespectemplateresourcerefsindexresource","title":"ClusterDeployment.spec.serviceSpec.templateResourceRefs[index].resource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field.</p> Name Type Description Required apiVersion string            API version of the referent. false fieldPath string            If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. false kind string            Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string            Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string            Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string            UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false"},{"location":"reference/crds/#clusterdeploymentstatus","title":"ClusterDeployment.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterDeploymentStatus defines the observed state of ClusterDeployment</p> Name Type Description Required availableUpgrades []string            AvailableUpgrades is the list of ClusterTemplate names to which this cluster can be upgraded. It can be an empty array, which means no upgrades are available. false conditions []object            Conditions contains details for the current state of the ClusterDeployment. false k8sVersion string            Currently compatible exact Kubernetes version of the cluster. Being set only if provided by the corresponding ClusterTemplate. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false services []object            Services contains details for the state of services. false servicesUpgradePaths []object            ServicesUpgradePaths contains details for the state of services upgrade paths. false"},{"location":"reference/crds/#clusterdeploymentstatusconditionsindex","title":"ClusterDeployment.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#clusterdeploymentstatusservicesindex","title":"ClusterDeployment.status.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceStatus contains details for the state of services.</p> Name Type Description Required clusterName string            ClusterName is the name of the associated cluster. true clusterNamespace string            ClusterNamespace is the namespace of the associated cluster. false conditions []object            Conditions contains details for the current state of managed services. false"},{"location":"reference/crds/#clusterdeploymentstatusservicesindexconditionsindex","title":"ClusterDeployment.status.services[index].conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#clusterdeploymentstatusservicesupgradepathsindex","title":"ClusterDeployment.status.servicesUpgradePaths[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceUpgradePaths contains details for the state of service upgrade paths.</p> Name Type Description Required name string            Name is the name of the service. true namespace string            Namespace is the namespace of the service. true template string            Template is the name of the current service template. true availableUpgrades []object            AvailableUpgrades contains details for the state of available upgrades. false"},{"location":"reference/crds/#clusterdeploymentstatusservicesupgradepathsindexavailableupgradesindex","title":"ClusterDeployment.status.servicesUpgradePaths[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>UpgradePath contains details for the state of service upgrade paths.</p> Name Type Description Required upgradePaths []string            Versions contains the list of versions that service can be upgraded to. false"},{"location":"reference/crds/#clustertemplatechain","title":"ClusterTemplateChain","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplateChain is the Schema for the clustertemplatechains API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterTemplateChain true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            TemplateChainSpec defines the desired state of *TemplateChain Validations:<li>self == oldSelf: Spec is immutable</li> false status object            TemplateChainStatus defines the observed state of *TemplateChain false"},{"location":"reference/crds/#clustertemplatechainspec","title":"ClusterTemplateChain.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainSpec defines the desired state of *TemplateChain</p> Name Type Description Required supportedTemplates []object            SupportedTemplates is the list of supported Templates definitions and all available upgrade sequences for it. false"},{"location":"reference/crds/#clustertemplatechainspecsupportedtemplatesindex","title":"ClusterTemplateChain.spec.supportedTemplates[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SupportedTemplate is the supported Template definition and all available upgrade sequences for it</p> Name Type Description Required name string            Name is the name of the Template. true availableUpgrades []object            AvailableUpgrades is the list of available upgrades for the specified Template. false"},{"location":"reference/crds/#clustertemplatechainspecsupportedtemplatesindexavailableupgradesindex","title":"ClusterTemplateChain.spec.supportedTemplates[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AvailableUpgrade is the definition of the available upgrade for the Template</p> Name Type Description Required name string            Name is the name of the Template to which the upgrade is available. true"},{"location":"reference/crds/#clustertemplatechainstatus","title":"ClusterTemplateChain.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainStatus defines the observed state of *TemplateChain</p> Name Type Description Required valid boolean            Valid indicates whether the chain is valid and can be considered when calculating available upgrade paths. false validationError string            ValidationError provides information regarding issues encountered during templatechain validation. false"},{"location":"reference/crds/#clustertemplate","title":"ClusterTemplate","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplate is the Schema for the clustertemplates API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterTemplate true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterTemplateSpec defines the desired state of ClusterTemplate Validations:<li>self == oldSelf: Spec is immutable</li><li>!has(self.helm.chartSource): .spec.helm.chartSource is not supported for ClusterTemplates</li> false status object            ClusterTemplateStatus defines the observed state of ClusterTemplate false"},{"location":"reference/crds/#clustertemplatespec","title":"ClusterTemplate.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplateSpec defines the desired state of ClusterTemplate</p> Name Type Description Required helm object            HelmSpec references a Helm chart representing the KCM template Validations:<li>(has(self.chartSpec) ? (!has(self.chartSource) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartSource) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartRef) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartSource)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>has(self.chartSpec) || has(self.chartRef) || has(self.chartSource): one of chartSpec, chartRef or chartSource must be set</li> true k8sVersion string            Kubernetes exact version in the SemVer format provided by this ClusterTemplate. false providerContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false providers []string            Providers represent required CAPI providers. Should be set if not present in the Helm chart metadata. false"},{"location":"reference/crds/#clustertemplatespechelm","title":"ClusterTemplate.spec.helm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HelmSpec references a Helm chart representing the KCM template</p> Name Type Description Required chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartSource object            ChartSource is a source of a Helm chart representing the template. Validations:<li>has(self.localSourceRef) ? (self.localSourceRef.kind != 'Secret' &amp;&amp; self.localSourceRef.kind != 'ConfigMap'): true: Secret and ConfigMap are not supported as Helm chart sources</li><li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false chartSpec object            ChartSpec defines the desired state of the HelmChart to be created by the controller false"},{"location":"reference/crds/#clustertemplatespechelmchartref","title":"ClusterTemplate.spec.helm.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#clustertemplatespechelmchartsource","title":"ClusterTemplate.spec.helm.chartSource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSource is a source of a Helm chart representing the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#clustertemplatespechelmchartsourcelocalsourceref","title":"ClusterTemplate.spec.helm.chartSource.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespec","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucket","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketcertsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketproxysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketsts","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketstscertsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketstssecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgit","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitincludeindex","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitincludeindexrepository","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitproxysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitverify","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitverifysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecoci","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecocicertsecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecocilayerselector","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociproxysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecocisecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociverify","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociverifymatchoidcidentityindex","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociverifysecretref","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartspec","title":"ClusterTemplate.spec.helm.chartSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSpec defines the desired state of the HelmChart to be created by the controller</p> Name Type Description Required chart string            Chart is the name or path the Helm chart is available at in the SourceRef. true interval string            Interval at which the HelmChart SourceRef is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true sourceRef object            SourceRef is the reference to the Source the chart is available at. true ignoreMissingValuesFiles boolean            IgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing. false reconcileStrategy enum            ReconcileStrategy determines what enables the creation of a new artifact. Valid values are ('ChartVersion', 'Revision'). See the documentation of the values for an explanation on their behavior. Defaults to ChartVersion when omitted. Enum: ChartVersion, Revision Default: ChartVersion false suspend boolean            Suspend tells the controller to suspend the reconciliation of this source. false valuesFiles []string            ValuesFiles is an alternative list of values files to use as the chart values (values.yaml is not included by default), expected to be a relative path in the SourceRef. Values files are merged in the order of this list with the last file overriding the first. Ignored when omitted. false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified. false version string            Version is the chart version semver expression, ignored for charts from GitRepository and Bucket sources. Defaults to latest when omitted. Default: * false"},{"location":"reference/crds/#clustertemplatespechelmchartspecsourceref","title":"ClusterTemplate.spec.helm.chartSpec.sourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceRef is the reference to the Source the chart is available at.</p> Name Type Description Required kind enum            Kind of the referent, valid values are ('HelmRepository', 'GitRepository', 'Bucket'). Enum: HelmRepository, GitRepository, Bucket true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false"},{"location":"reference/crds/#clustertemplatespechelmchartspecverify","title":"ClusterTemplate.spec.helm.chartSpec.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#clustertemplatespechelmchartspecverifymatchoidcidentityindex","title":"ClusterTemplate.spec.helm.chartSpec.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#clustertemplatespechelmchartspecverifysecretref","title":"ClusterTemplate.spec.helm.chartSpec.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatestatus","title":"ClusterTemplate.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplateStatus defines the observed state of ClusterTemplate</p> Name Type Description Required valid boolean            Valid indicates whether the template passed validation or not. true chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartVersion string            ChartVersion represents the version of the Helm Chart associated with this template. false config JSON            Config demonstrates available parameters for template customization, that can be used when creating ClusterDeployment objects. false description string            Description contains information about the template. false k8sVersion string            Kubernetes exact version in the SemVer format provided by this ClusterTemplate. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false providerContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false providers []string            Providers represent required CAPI providers. false validationError string            ValidationError provides information regarding issues encountered during template validation. false"},{"location":"reference/crds/#clustertemplatestatuschartref","title":"ClusterTemplate.status.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#credential","title":"Credential","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Credential is the Schema for the credentials API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string Credential true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            CredentialSpec defines the desired state of Credential false status object            CredentialStatus defines the observed state of Credential false"},{"location":"reference/crds/#credentialspec","title":"Credential.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CredentialSpec defines the desired state of Credential</p> Name Type Description Required identityRef object            Reference to the Credential Identity true description string            Description of the Credential object false"},{"location":"reference/crds/#credentialspecidentityref","title":"Credential.spec.identityRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference to the Credential Identity</p> Name Type Description Required apiVersion string            API version of the referent. false fieldPath string            If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. false kind string            Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string            Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string            Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string            UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false"},{"location":"reference/crds/#credentialstatus","title":"Credential.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CredentialStatus defines the observed state of Credential</p> Name Type Description Required ready boolean            Ready holds the readiness of Credentials. Default: false true conditions []object            Conditions contains details for the current state of the Credential. false"},{"location":"reference/crds/#credentialstatusconditionsindex","title":"Credential.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#managementbackup","title":"ManagementBackup","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementBackup is the Schema for the managementbackups API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ManagementBackup true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ManagementBackupSpec defines the desired state of ManagementBackup false status object            ManagementBackupStatus defines the observed state of ManagementBackup false"},{"location":"reference/crds/#managementbackupspec","title":"ManagementBackup.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementBackupSpec defines the desired state of ManagementBackup</p> Name Type Description Required performOnManagementUpgrade boolean            PerformOnManagementUpgrade indicates that a single [ManagementBackup] should be created and stored in the [ManagementBackup] storage location if not default before the [Management] release upgrade. false schedule string            Schedule is a Cron expression defining when to run the scheduled [ManagementBackup]. If not set, the object is considered to be run only once. false storageLocation string            StorageLocation is the name of a [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.StorageLocation] where the backup should be stored. false"},{"location":"reference/crds/#managementbackupstatus","title":"ManagementBackup.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementBackupStatus defines the observed state of ManagementBackup</p> Name Type Description Required error string            Error stores messages in case of failed backup creation. false lastBackup object            Most recently [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup] that has been created. false lastBackupName string            Name of most recently created [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup]. false lastBackupTime string            Time of the most recently created [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup]. Format: date-time false nextAttempt string            NextAttempt indicates the time when the next backup will be created. Always absent for a single [ManagementBackup]. Format: date-time false"},{"location":"reference/crds/#managementbackupstatuslastbackup","title":"ManagementBackup.status.lastBackup","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Most recently [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup] that has been created.</p> Name Type Description Required backupItemOperationsAttempted integer            BackupItemOperationsAttempted is the total number of attempted async BackupItemAction operations for this backup. false backupItemOperationsCompleted integer            BackupItemOperationsCompleted is the total number of successfully completed async BackupItemAction operations for this backup. false backupItemOperationsFailed integer            BackupItemOperationsFailed is the total number of async BackupItemAction operations for this backup which ended with an error. false completionTimestamp string            CompletionTimestamp records the time a backup was completed. Completion time is recorded even on failed backups. Completion time is recorded before uploading the backup object. The server's time is used for CompletionTimestamps Format: date-time false csiVolumeSnapshotsAttempted integer            CSIVolumeSnapshotsAttempted is the total number of attempted CSI VolumeSnapshots for this backup. false csiVolumeSnapshotsCompleted integer            CSIVolumeSnapshotsCompleted is the total number of successfully completed CSI VolumeSnapshots for this backup. false errors integer            Errors is a count of all error messages that were generated during execution of the backup.  The actual errors are in the backup's log file in object storage. false expiration string            Expiration is when this Backup is eligible for garbage-collection. Format: date-time false failureReason string            FailureReason is an error that caused the entire backup to fail. false formatVersion string            FormatVersion is the backup format version, including major, minor, and patch version. false hookStatus object            HookStatus contains information about the status of the hooks. false phase enum            Phase is the current state of the Backup. Enum: New, FailedValidation, InProgress, WaitingForPluginOperations, WaitingForPluginOperationsPartiallyFailed, Finalizing, FinalizingPartiallyFailed, Completed, PartiallyFailed, Failed, Deleting false progress object            Progress contains information about the backup's execution progress. Note that this information is best-effort only -- if Velero fails to update it during a backup for any reason, it may be inaccurate/stale. false startTimestamp string            StartTimestamp records the time a backup was started. Separate from CreationTimestamp, since that value changes on restores. The server's time is used for StartTimestamps Format: date-time false validationErrors []string            ValidationErrors is a slice of all validation errors (if applicable). false version integer            Version is the backup format major version. Deprecated: Please see FormatVersion false volumeSnapshotsAttempted integer            VolumeSnapshotsAttempted is the total number of attempted volume snapshots for this backup. false volumeSnapshotsCompleted integer            VolumeSnapshotsCompleted is the total number of successfully completed volume snapshots for this backup. false warnings integer            Warnings is a count of all warning messages that were generated during execution of the backup. The actual warnings are in the backup's log file in object storage. false"},{"location":"reference/crds/#managementbackupstatuslastbackuphookstatus","title":"ManagementBackup.status.lastBackup.hookStatus","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HookStatus contains information about the status of the hooks.</p> Name Type Description Required hooksAttempted integer            HooksAttempted is the total number of attempted hooks Specifically, HooksAttempted represents the number of hooks that failed to execute and the number of hooks that executed successfully. false hooksFailed integer            HooksFailed is the total number of hooks which ended with an error false"},{"location":"reference/crds/#managementbackupstatuslastbackupprogress","title":"ManagementBackup.status.lastBackup.progress","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Progress contains information about the backup's execution progress. Note that this information is best-effort only -- if Velero fails to update it during a backup for any reason, it may be inaccurate/stale.</p> Name Type Description Required itemsBackedUp integer            ItemsBackedUp is the number of items that have actually been written to the backup tarball so far. false totalItems integer            TotalItems is the total number of items to be backed up. This number may change throughout the execution of the backup due to plugins that return additional related items to back up, the velero.io/exclude-from-backup label, and various other filters that happen as items are processed. false"},{"location":"reference/crds/#management","title":"Management","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Management is the Schema for the managements API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string Management true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ManagementSpec defines the desired state of Management false status object            ManagementStatus defines the observed state of Management false"},{"location":"reference/crds/#managementspec","title":"Management.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementSpec defines the desired state of Management</p> Name Type Description Required release string            Release references the Release object. true core object            Core holds the core Management components that are mandatory. If not specified, will be populated with the default values. false providers []object            Providers is the list of supported CAPI providers. false"},{"location":"reference/crds/#managementspeccore","title":"Management.spec.core","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Core holds the core Management components that are mandatory. If not specified, will be populated with the default values.</p> Name Type Description Required capi object            CAPI represents the core Cluster API component and references the Cluster API template. false kcm object            KCM represents the core KCM component and references the KCM template. false"},{"location":"reference/crds/#managementspeccorecapi","title":"Management.spec.core.capi","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CAPI represents the core Cluster API component and references the Cluster API template.</p> Name Type Description Required config JSON            Config allows to provide parameters for management component customization. If no Config provided, the field will be populated with the default values for the template. false template string            Template is the name of the Template associated with this component. If not specified, will be taken from the Release object. false"},{"location":"reference/crds/#managementspeccorekcm","title":"Management.spec.core.kcm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>KCM represents the core KCM component and references the KCM template.</p> Name Type Description Required config JSON            Config allows to provide parameters for management component customization. If no Config provided, the field will be populated with the default values for the template. false template string            Template is the name of the Template associated with this component. If not specified, will be taken from the Release object. false"},{"location":"reference/crds/#managementspecprovidersindex","title":"Management.spec.providers[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required name string            Name of the provider. true config JSON            Config allows to provide parameters for management component customization. If no Config provided, the field will be populated with the default values for the template. false template string            Template is the name of the Template associated with this component. If not specified, will be taken from the Release object. false"},{"location":"reference/crds/#managementstatus","title":"Management.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementStatus defines the observed state of Management</p> Name Type Description Required availableProviders []string            AvailableProviders holds all available CAPI providers. false backupName string            BackupName is a name of the management cluster scheduled backup. false capiContracts map[string]map[string]string            For each CAPI provider name holds its compatibility [contract versions] in a key-value pairs, where the key is the core CAPI contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core CAPI.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false components map[string]object            Components indicates the status of installed KCM components and CAPI providers. false conditions []object            Conditions represents the observations of a Management's current state. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false release string            Release indicates the current Release object. false"},{"location":"reference/crds/#managementstatuscomponentskey","title":"Management.status.components[key]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ComponentStatus is the status of Management component installation</p> Name Type Description Required error string            Error stores as error message in case of failed installation false exposedProviders []string            ExposedProviders is a list of CAPI providers this component exposes false success boolean            Success represents if a component installation was successful false template string            Template is the name of the Template associated with this component. false"},{"location":"reference/crds/#managementstatusconditionsindex","title":"Management.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#multiclusterservice","title":"MultiClusterService","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>MultiClusterService is the Schema for the multiclusterservices API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string MultiClusterService true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            MultiClusterServiceSpec defines the desired state of MultiClusterService false status object            MultiClusterServiceStatus defines the observed state of MultiClusterService. false"},{"location":"reference/crds/#multiclusterservicespec","title":"MultiClusterService.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>MultiClusterServiceSpec defines the desired state of MultiClusterService</p> Name Type Description Required clusterSelector object            ClusterSelector identifies target clusters to manage services on. false serviceSpec object            ServiceSpec is spec related to deployment of services. false"},{"location":"reference/crds/#multiclusterservicespecclusterselector","title":"MultiClusterService.spec.clusterSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterSelector identifies target clusters to manage services on.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"reference/crds/#multiclusterservicespecclusterselectormatchexpressionsindex","title":"MultiClusterService.spec.clusterSelector.matchExpressions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"reference/crds/#multiclusterservicespecservicespec","title":"MultiClusterService.spec.serviceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceSpec is spec related to deployment of services.</p> Name Type Description Required continueOnError boolean            ContinueOnError specifies if the services deployment should continue if an error occurs. Default: false false driftExclusions []object            DriftExclusions specifies specific configurations of resources to ignore for drift detection. false driftIgnore []object            DriftIgnore specifies resources to ignore for drift detection. false priority integer            Priority sets the priority for the services defined in this spec. Higher value means higher priority and lower means lower. In case of conflict with another object managing the service, the one with higher priority will get to deploy its services. Format: int32 Default: 100 Minimum: 1 Maximum: 2.147483646e+09 false reload boolean            Reload instances via rolling upgrade when a ConfigMap/Secret mounted as volume is modified. false services []object            Services is a list of services created via ServiceTemplates that could be installed on the target cluster. false stopOnConflict boolean            StopOnConflict specifies what to do in case of a conflict. E.g. If another object is already managing a service. By default the remaining services will be deployed even if conflict is detected. If set to true, the deployment will stop after encountering the first conflict. Default: false false syncMode enum            SyncMode specifies how services are synced in the target cluster. Enum: OneTime, Continuous, ContinuousWithDriftDetection, DryRun Default: Continuous false templateResourceRefs []object            TemplateResourceRefs is a list of resources to collect from the management cluster, the values from which can be used in templates. false"},{"location":"reference/crds/#multiclusterservicespecservicespecdriftexclusionsindex","title":"MultiClusterService.spec.serviceSpec.driftExclusions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required paths []string            Paths is a slice of JSON6902 paths to exclude from configuration drift evaluation. true target object            Target points to the resources that the paths refers to. false"},{"location":"reference/crds/#multiclusterservicespecservicespecdriftexclusionsindextarget","title":"MultiClusterService.spec.serviceSpec.driftExclusions[index].target","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Target points to the resources that the paths refers to.</p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#multiclusterservicespecservicespecdriftignoreindex","title":"MultiClusterService.spec.serviceSpec.driftIgnore[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#multiclusterservicespecservicespecservicesindex","title":"MultiClusterService.spec.serviceSpec.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Service represents a Service to be deployed.</p> Name Type Description Required name string            Name is the chart release. true template string            Template is a reference to a Template object located in the same namespace. true disable boolean            Disable can be set to disable handling of this service. false namespace string            Namespace is the namespace the release will be installed in. It will default to Name if not provided. false templateChain string            TemplateChain defines the ServiceTemplateChain object that will be used to deploy the service along with desired ServiceTemplate version. false values string            Values is the helm values to be passed to the chart used by the template. The string type is used in order to allow for templating. false valuesFrom []object            ValuesFrom can reference a ConfigMap or Secret containing helm values. false"},{"location":"reference/crds/#multiclusterservicespecservicespecservicesindexvaluesfromindex","title":"MultiClusterService.spec.serviceSpec.services[index].valuesFrom[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required kind enum            Kind of the resource. Supported kinds are: - ConfigMap/Secret Enum: ConfigMap, Secret true name string            Name of the referenced resource. Name can be expressed as a template and instantiate using any cluster field. true namespace string            Namespace of the referenced resource. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. For Profile namespace must be left empty. The Profile namespace will be used. Namespace can be expressed as a template and instantiate using any cluster field. false optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other ValueFroms. Default: false false"},{"location":"reference/crds/#multiclusterservicespecservicespectemplateresourcerefsindex","title":"MultiClusterService.spec.serviceSpec.templateResourceRefs[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required identifier string            Identifier is how the resource will be referred to in the template true resource object            Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field. true optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other TemplateResourceRefs. Default: false false"},{"location":"reference/crds/#multiclusterservicespecservicespectemplateresourcerefsindexresource","title":"MultiClusterService.spec.serviceSpec.templateResourceRefs[index].resource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field.</p> Name Type Description Required apiVersion string            API version of the referent. false fieldPath string            If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. false kind string            Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string            Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string            Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string            UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false"},{"location":"reference/crds/#multiclusterservicestatus","title":"MultiClusterService.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>MultiClusterServiceStatus defines the observed state of MultiClusterService.</p> Name Type Description Required conditions []object            Conditions contains details for the current state of the MultiClusterService. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false services []object            Services contains details for the state of services. false servicesUpgradePaths []object            ServicesUpgradePaths contains details for the state of services upgrade paths. false"},{"location":"reference/crds/#multiclusterservicestatusconditionsindex","title":"MultiClusterService.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#multiclusterservicestatusservicesindex","title":"MultiClusterService.status.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceStatus contains details for the state of services.</p> Name Type Description Required clusterName string            ClusterName is the name of the associated cluster. true clusterNamespace string            ClusterNamespace is the namespace of the associated cluster. false conditions []object            Conditions contains details for the current state of managed services. false"},{"location":"reference/crds/#multiclusterservicestatusservicesindexconditionsindex","title":"MultiClusterService.status.services[index].conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#multiclusterservicestatusservicesupgradepathsindex","title":"MultiClusterService.status.servicesUpgradePaths[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceUpgradePaths contains details for the state of service upgrade paths.</p> Name Type Description Required name string            Name is the name of the service. true namespace string            Namespace is the namespace of the service. true template string            Template is the name of the current service template. true availableUpgrades []object            AvailableUpgrades contains details for the state of available upgrades. false"},{"location":"reference/crds/#multiclusterservicestatusservicesupgradepathsindexavailableupgradesindex","title":"MultiClusterService.status.servicesUpgradePaths[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>UpgradePath contains details for the state of service upgrade paths.</p> Name Type Description Required upgradePaths []string            Versions contains the list of versions that service can be upgraded to. false"},{"location":"reference/crds/#providerinterface","title":"ProviderInterface","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderInterface is the Schema for the ProviderInterface API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ProviderInterface true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ProviderInterfaceSpec defines the desired state of ProviderInterface false status object            ProviderInterfaceStatus defines the observed state of ProviderInterface false"},{"location":"reference/crds/#providerinterfacespec","title":"ProviderInterface.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderInterfaceSpec defines the desired state of ProviderInterface</p> Name Type Description Required clusterGVKs []object            ClusterGVKs defines the Group-Version-Kind resources this provider can manage false clusterIdentityKinds []string            ClusterIdentityKinds defines the Kind of identity objects supported by this provider false description string            Description provides a human-readable explanation of what this provider does false"},{"location":"reference/crds/#providerinterfacespecclustergvksindex","title":"ProviderInterface.spec.clusterGVKs[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GroupVersionKind unambiguously identifies a kind. It doesn't anonymously include GroupVersion to avoid automatic coercion. It doesn't use a GroupVersion to avoid custom marshalling Note: mirror of https://github.com/kubernetes/apimachinery/blob/v0.32.3/pkg/runtime/schema/group_version.go#L140-L146</p> Name Type Description Required group string true kind string true version string true"},{"location":"reference/crds/#providerinterfacestatus","title":"ProviderInterface.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderInterfaceStatus defines the observed state of ProviderInterface</p> Name Type Description Required exposedProviders string            ExposedProviders contains the list of exposed provider false"},{"location":"reference/crds/#providertemplate","title":"ProviderTemplate","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderTemplate is the Schema for the providertemplates API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ProviderTemplate true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ProviderTemplateSpec defines the desired state of ProviderTemplate Validations:<li>self == oldSelf: Spec is immutable</li><li>!has(self.helm.chartSource): .spec.helm.chartSource is not supported for ProviderTemplates</li> false status object            ProviderTemplateStatus defines the observed state of ProviderTemplate false"},{"location":"reference/crds/#providertemplatespec","title":"ProviderTemplate.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderTemplateSpec defines the desired state of ProviderTemplate</p> Name Type Description Required capiContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the core CAPI contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core CAPI.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false helm object            HelmSpec references a Helm chart representing the KCM template Validations:<li>(has(self.chartSpec) ? (!has(self.chartSource) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartSource) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartRef) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartSource)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>has(self.chartSpec) || has(self.chartRef) || has(self.chartSource): one of chartSpec, chartRef or chartSource must be set</li> false providers []string            Providers represent exposed CAPI providers. Should be set if not present in the Helm chart metadata. false"},{"location":"reference/crds/#providertemplatespechelm","title":"ProviderTemplate.spec.helm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HelmSpec references a Helm chart representing the KCM template</p> Name Type Description Required chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartSource object            ChartSource is a source of a Helm chart representing the template. Validations:<li>has(self.localSourceRef) ? (self.localSourceRef.kind != 'Secret' &amp;&amp; self.localSourceRef.kind != 'ConfigMap'): true: Secret and ConfigMap are not supported as Helm chart sources</li><li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false chartSpec object            ChartSpec defines the desired state of the HelmChart to be created by the controller false"},{"location":"reference/crds/#providertemplatespechelmchartref","title":"ProviderTemplate.spec.helm.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#providertemplatespechelmchartsource","title":"ProviderTemplate.spec.helm.chartSource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSource is a source of a Helm chart representing the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#providertemplatespechelmchartsourcelocalsourceref","title":"ProviderTemplate.spec.helm.chartSource.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespec","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucket","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketcertsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketproxysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketsts","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketstscertsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketstssecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgit","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitincludeindex","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitincludeindexrepository","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitproxysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitverify","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitverifysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecoci","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecocicertsecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecocilayerselector","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociproxysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecocisecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociverify","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociverifymatchoidcidentityindex","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociverifysecretref","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartspec","title":"ProviderTemplate.spec.helm.chartSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSpec defines the desired state of the HelmChart to be created by the controller</p> Name Type Description Required chart string            Chart is the name or path the Helm chart is available at in the SourceRef. true interval string            Interval at which the HelmChart SourceRef is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true sourceRef object            SourceRef is the reference to the Source the chart is available at. true ignoreMissingValuesFiles boolean            IgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing. false reconcileStrategy enum            ReconcileStrategy determines what enables the creation of a new artifact. Valid values are ('ChartVersion', 'Revision'). See the documentation of the values for an explanation on their behavior. Defaults to ChartVersion when omitted. Enum: ChartVersion, Revision Default: ChartVersion false suspend boolean            Suspend tells the controller to suspend the reconciliation of this source. false valuesFiles []string            ValuesFiles is an alternative list of values files to use as the chart values (values.yaml is not included by default), expected to be a relative path in the SourceRef. Values files are merged in the order of this list with the last file overriding the first. Ignored when omitted. false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified. false version string            Version is the chart version semver expression, ignored for charts from GitRepository and Bucket sources. Defaults to latest when omitted. Default: * false"},{"location":"reference/crds/#providertemplatespechelmchartspecsourceref","title":"ProviderTemplate.spec.helm.chartSpec.sourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceRef is the reference to the Source the chart is available at.</p> Name Type Description Required kind enum            Kind of the referent, valid values are ('HelmRepository', 'GitRepository', 'Bucket'). Enum: HelmRepository, GitRepository, Bucket true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false"},{"location":"reference/crds/#providertemplatespechelmchartspecverify","title":"ProviderTemplate.spec.helm.chartSpec.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#providertemplatespechelmchartspecverifymatchoidcidentityindex","title":"ProviderTemplate.spec.helm.chartSpec.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#providertemplatespechelmchartspecverifysecretref","title":"ProviderTemplate.spec.helm.chartSpec.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatestatus","title":"ProviderTemplate.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderTemplateStatus defines the observed state of ProviderTemplate</p> Name Type Description Required valid boolean            Valid indicates whether the template passed validation or not. true capiContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the core CAPI contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core CAPI.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartVersion string            ChartVersion represents the version of the Helm Chart associated with this template. false config JSON            Config demonstrates available parameters for template customization, that can be used when creating ClusterDeployment objects. false description string            Description contains information about the template. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false providers []string            Providers represent exposed CAPI providers. false validationError string            ValidationError provides information regarding issues encountered during template validation. false"},{"location":"reference/crds/#providertemplatestatuschartref","title":"ProviderTemplate.status.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#release","title":"Release","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Release is the Schema for the releases API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1alpha1 true kind string Release true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ReleaseSpec defines the desired state of Release false status object            ReleaseStatus defines the observed state of Release false"},{"location":"reference/crds/#releasespec","title":"Release.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ReleaseSpec defines the desired state of Release</p> Name Type Description Required capi object            CAPI references the Cluster API template. true kcm object            KCM references the KCM template. true version string            Version of the KCM Release in the semver format. true providers []object            Providers contains a list of Providers associated with the Release. false"},{"location":"reference/crds/#releasespeccapi","title":"Release.spec.capi","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CAPI references the Cluster API template.</p> Name Type Description Required template string            Template references the Template associated with the provider. true"},{"location":"reference/crds/#releasespeckcm","title":"Release.spec.kcm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>KCM references the KCM template.</p> Name Type Description Required template string            Template references the Template associated with the provider. true"},{"location":"reference/crds/#releasespecprovidersindex","title":"Release.spec.providers[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required name string            Name of the provider. true template string            Template references the Template associated with the provider. true"},{"location":"reference/crds/#releasestatus","title":"Release.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ReleaseStatus defines the observed state of Release</p> Name Type Description Required conditions []object            Conditions contains details for the current state of the Release false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false ready boolean            Ready indicates whether KCM is ready to be upgraded to this Release. false"},{"location":"reference/crds/#releasestatusconditionsindex","title":"Release.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#servicetemplatechain","title":"ServiceTemplateChain","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplateChain is the Schema for the servicetemplatechains API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ServiceTemplateChain true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            TemplateChainSpec defines the desired state of *TemplateChain Validations:<li>self == oldSelf: Spec is immutable</li> false status object            TemplateChainStatus defines the observed state of *TemplateChain false"},{"location":"reference/crds/#servicetemplatechainspec","title":"ServiceTemplateChain.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainSpec defines the desired state of *TemplateChain</p> Name Type Description Required supportedTemplates []object            SupportedTemplates is the list of supported Templates definitions and all available upgrade sequences for it. false"},{"location":"reference/crds/#servicetemplatechainspecsupportedtemplatesindex","title":"ServiceTemplateChain.spec.supportedTemplates[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SupportedTemplate is the supported Template definition and all available upgrade sequences for it</p> Name Type Description Required name string            Name is the name of the Template. true availableUpgrades []object            AvailableUpgrades is the list of available upgrades for the specified Template. false"},{"location":"reference/crds/#servicetemplatechainspecsupportedtemplatesindexavailableupgradesindex","title":"ServiceTemplateChain.spec.supportedTemplates[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AvailableUpgrade is the definition of the available upgrade for the Template</p> Name Type Description Required name string            Name is the name of the Template to which the upgrade is available. true"},{"location":"reference/crds/#servicetemplatechainstatus","title":"ServiceTemplateChain.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainStatus defines the observed state of *TemplateChain</p> Name Type Description Required valid boolean            Valid indicates whether the chain is valid and can be considered when calculating available upgrade paths. false validationError string            ValidationError provides information regarding issues encountered during templatechain validation. false"},{"location":"reference/crds/#servicetemplate","title":"ServiceTemplate","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplate is the Schema for the servicetemplates API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ServiceTemplate true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ServiceTemplateSpec defines the desired state of ServiceTemplate Validations:<li>self == oldSelf: Spec is immutable</li><li>has(self.helm) ? (!has(self.kustomize) &amp;&amp; !has(self.resources)): true: Helm, Kustomize and Resources are mutually exclusive.</li><li>has(self.kustomize) ? (!has(self.helm) &amp;&amp; !has(self.resources)): true: Helm, Kustomize and Resources are mutually exclusive.</li><li>has(self.resources) ? (!has(self.kustomize) &amp;&amp; !has(self.helm)): true: Helm, Kustomize and Resources are mutually exclusive.</li><li>has(self.helm) || has(self.kustomize) || has(self.resources): One of Helm, Kustomize, or Resources must be specified.</li> false status object            ServiceTemplateStatus defines the observed state of ServiceTemplate false"},{"location":"reference/crds/#servicetemplatespec","title":"ServiceTemplate.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplateSpec defines the desired state of ServiceTemplate</p> Name Type Description Required helm object            Helm contains the Helm chart information for the template. Validations:<li>(has(self.chartSpec) ? (!has(self.chartSource) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartSource) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartRef) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartSource)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>has(self.chartSpec) || has(self.chartRef) || has(self.chartSource): one of chartSpec, chartRef or chartSource must be set</li> false k8sConstraint string            Constraint describing compatible K8S versions of the cluster set in the SemVer format. false kustomize object            Kustomize contains the Kustomize configuration for the template. Validations:<li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false resources object            Resources contains the resource configuration for the template. Validations:<li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false version string            Version is the semantic version of the application backed by template. false"},{"location":"reference/crds/#servicetemplatespechelm","title":"ServiceTemplate.spec.helm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Helm contains the Helm chart information for the template.</p> Name Type Description Required chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartSource object            ChartSource is a source of a Helm chart representing the template. Validations:<li>has(self.localSourceRef) ? (self.localSourceRef.kind != 'Secret' &amp;&amp; self.localSourceRef.kind != 'ConfigMap'): true: Secret and ConfigMap are not supported as Helm chart sources</li><li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false chartSpec object            ChartSpec defines the desired state of the HelmChart to be created by the controller false"},{"location":"reference/crds/#servicetemplatespechelmchartref","title":"ServiceTemplate.spec.helm.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#servicetemplatespechelmchartsource","title":"ServiceTemplate.spec.helm.chartSource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSource is a source of a Helm chart representing the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#servicetemplatespechelmchartsourcelocalsourceref","title":"ServiceTemplate.spec.helm.chartSource.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespec","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucket","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketcertsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketproxysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketsts","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketstscertsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketstssecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgit","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitincludeindex","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitincludeindexrepository","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitproxysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitverify","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitverifysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecoci","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecocicertsecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecocilayerselector","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociproxysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecocisecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociverify","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociverifymatchoidcidentityindex","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociverifysecretref","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartspec","title":"ServiceTemplate.spec.helm.chartSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSpec defines the desired state of the HelmChart to be created by the controller</p> Name Type Description Required chart string            Chart is the name or path the Helm chart is available at in the SourceRef. true interval string            Interval at which the HelmChart SourceRef is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true sourceRef object            SourceRef is the reference to the Source the chart is available at. true ignoreMissingValuesFiles boolean            IgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing. false reconcileStrategy enum            ReconcileStrategy determines what enables the creation of a new artifact. Valid values are ('ChartVersion', 'Revision'). See the documentation of the values for an explanation on their behavior. Defaults to ChartVersion when omitted. Enum: ChartVersion, Revision Default: ChartVersion false suspend boolean            Suspend tells the controller to suspend the reconciliation of this source. false valuesFiles []string            ValuesFiles is an alternative list of values files to use as the chart values (values.yaml is not included by default), expected to be a relative path in the SourceRef. Values files are merged in the order of this list with the last file overriding the first. Ignored when omitted. false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified. false version string            Version is the chart version semver expression, ignored for charts from GitRepository and Bucket sources. Defaults to latest when omitted. Default: * false"},{"location":"reference/crds/#servicetemplatespechelmchartspecsourceref","title":"ServiceTemplate.spec.helm.chartSpec.sourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceRef is the reference to the Source the chart is available at.</p> Name Type Description Required kind enum            Kind of the referent, valid values are ('HelmRepository', 'GitRepository', 'Bucket'). Enum: HelmRepository, GitRepository, Bucket true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false"},{"location":"reference/crds/#servicetemplatespechelmchartspecverify","title":"ServiceTemplate.spec.helm.chartSpec.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespechelmchartspecverifymatchoidcidentityindex","title":"ServiceTemplate.spec.helm.chartSpec.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespechelmchartspecverifysecretref","title":"ServiceTemplate.spec.helm.chartSpec.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomize","title":"ServiceTemplate.spec.kustomize","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Kustomize contains the Kustomize configuration for the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#servicetemplatespeckustomizelocalsourceref","title":"ServiceTemplate.spec.kustomize.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespec","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucket","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketcertsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketproxysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketsts","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketstscertsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketstssecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgit","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitincludeindex","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitincludeindexrepository","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitproxysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitverify","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitverifysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecoci","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecocicertsecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecocilayerselector","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociproxysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecocisecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociverify","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociverifymatchoidcidentityindex","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociverifysecretref","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresources","title":"ServiceTemplate.spec.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resources contains the resource configuration for the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#servicetemplatespecresourceslocalsourceref","title":"ServiceTemplate.spec.resources.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespec","title":"ServiceTemplate.spec.resources.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucket","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketcertsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketproxysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketsts","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketstscertsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketstssecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgit","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitincludeindex","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitincludeindexrepository","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitproxysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitverify","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitverifysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecoci","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecocicertsecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecocilayerselector","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociproxysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecocisecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociverify","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociverifymatchoidcidentityindex","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociverifysecretref","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatestatus","title":"ServiceTemplate.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplateStatus defines the observed state of ServiceTemplate</p> Name Type Description Required valid boolean            Valid indicates whether the template passed validation or not. true chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartVersion string            ChartVersion represents the version of the Helm Chart associated with this template. false config JSON            Config demonstrates available parameters for template customization, that can be used when creating ClusterDeployment objects. false description string            Description contains information about the template. false k8sConstraint string            Constraint describing compatible K8S versions of the cluster set in the SemVer format. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false sourceStatus object            SourceStatus reflects the status of the source. false validationError string            ValidationError provides information regarding issues encountered during template validation. false"},{"location":"reference/crds/#servicetemplatestatuschartref","title":"ServiceTemplate.status.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#servicetemplatestatussourcestatus","title":"ServiceTemplate.status.sourceStatus","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceStatus reflects the status of the source.</p> Name Type Description Required kind string            Kind is the kind of the remote source. true name string            Name is the name of the remote source. true namespace string            Namespace is the namespace of the remote source. true artifact object            Artifact is the artifact that was generated from the template source. false conditions []object            Conditions reflects the conditions of the remote source object. false observedGeneration integer            ObservedGeneration is the latest source generation observed by the controller. Format: int64 false"},{"location":"reference/crds/#servicetemplatestatussourcestatusartifact","title":"ServiceTemplate.status.sourceStatus.artifact","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Artifact is the artifact that was generated from the template source.</p> Name Type Description Required lastUpdateTime string            LastUpdateTime is the timestamp corresponding to the last update of the Artifact. Format: date-time true path string            Path is the relative file path of the Artifact. It can be used to locate the file in the root of the Artifact storage on the local file system of the controller managing the Source. true revision string            Revision is a human-readable identifier traceable in the origin source system. It can be a Git commit SHA, Git tag, a Helm chart version, etc. true url string            URL is the HTTP address of the Artifact as exposed by the controller managing the Source. It can be used to retrieve the Artifact for consumption, e.g. by another controller applying the Artifact contents. true digest string            Digest is the digest of the file in the form of ':'. false metadata map[string]string            Metadata holds upstream information such as OCI annotations. false size integer            Size is the number of bytes in the file. Format: int64 false"},{"location":"reference/crds/#servicetemplatestatussourcestatusconditionsindex","title":"ServiceTemplate.status.sourceStatus.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#k0rdentmirantiscomv1beta1_1","title":"k0rdent.mirantis.com/v1beta1","text":"<p>Resource Types:</p> <ul> <li> <p>AccessManagement</p> </li> <li> <p>ClusterDeployment</p> </li> <li> <p>ClusterIPAMClaim</p> </li> <li> <p>ClusterIPAM</p> </li> <li> <p>ClusterTemplateChain</p> </li> <li> <p>ClusterTemplate</p> </li> <li> <p>Credential</p> </li> <li> <p>ManagementBackup</p> </li> <li> <p>Management</p> </li> <li> <p>MultiClusterService</p> </li> <li> <p>ProviderInterface</p> </li> <li> <p>ProviderTemplate</p> </li> <li> <p>Release</p> </li> <li> <p>ServiceTemplateChain</p> </li> <li> <p>ServiceTemplate</p> </li> </ul>"},{"location":"reference/crds/#accessmanagement_1","title":"AccessManagement","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessManagement is the Schema for the AccessManagements API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string AccessManagement true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            AccessManagementSpec defines the desired state of AccessManagement false status object            AccessManagementStatus defines the observed state of AccessManagement false"},{"location":"reference/crds/#accessmanagementspec_1","title":"AccessManagement.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessManagementSpec defines the desired state of AccessManagement</p> Name Type Description Required accessRules []object            AccessRules is the list of access rules. Each AccessRule enforces objects distribution to the TargetNamespaces. false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindex_1","title":"AccessManagement.spec.accessRules[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessRule is the definition of the AccessManagement access rule. Each AccessRule enforces Templates and Credentials distribution to the TargetNamespaces</p> Name Type Description Required clusterTemplateChains []string            ClusterTemplateChains lists the names of ClusterTemplateChains whose ClusterTemplates will be distributed to all namespaces specified in TargetNamespaces. false credentials []string            Credentials is the list of Credential names that will be distributed to all the namespaces specified in TargetNamespaces. false serviceTemplateChains []string            ServiceTemplateChains lists the names of ServiceTemplateChains whose ServiceTemplates will be distributed to all namespaces specified in TargetNamespaces. false targetNamespaces object            TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset. Validations:<li>((has(self.stringSelector) ? 1 : 0) + (has(self.selector) ? 1 : 0) + (has(self.list) ? 1 : 0)) &lt;= 1: only one of spec.targetNamespaces.selector or spec.targetNamespaces.stringSelector or spec.targetNamespaces.list can be specified</li> false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindextargetnamespaces_1","title":"AccessManagement.spec.accessRules[index].targetNamespaces","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset.</p> Name Type Description Required list []string            List is the list of namespaces to select. Mutually exclusive with StringSelector and Selector. false selector object            Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List. false stringSelector string            StringSelector is a label query to select namespaces. Mutually exclusive with Selector and List. false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindextargetnamespacesselector_1","title":"AccessManagement.spec.accessRules[index].targetNamespaces.selector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"reference/crds/#accessmanagementspecaccessrulesindextargetnamespacesselectormatchexpressionsindex_1","title":"AccessManagement.spec.accessRules[index].targetNamespaces.selector.matchExpressions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"reference/crds/#accessmanagementstatus_1","title":"AccessManagement.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessManagementStatus defines the observed state of AccessManagement</p> Name Type Description Required current []object            Current reflects the applied access rules configuration. false error string            Error is the error message occurred during the reconciliation (if any) false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false"},{"location":"reference/crds/#accessmanagementstatuscurrentindex_1","title":"AccessManagement.status.current[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AccessRule is the definition of the AccessManagement access rule. Each AccessRule enforces Templates and Credentials distribution to the TargetNamespaces</p> Name Type Description Required clusterTemplateChains []string            ClusterTemplateChains lists the names of ClusterTemplateChains whose ClusterTemplates will be distributed to all namespaces specified in TargetNamespaces. false credentials []string            Credentials is the list of Credential names that will be distributed to all the namespaces specified in TargetNamespaces. false serviceTemplateChains []string            ServiceTemplateChains lists the names of ServiceTemplateChains whose ServiceTemplates will be distributed to all namespaces specified in TargetNamespaces. false targetNamespaces object            TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset. Validations:<li>((has(self.stringSelector) ? 1 : 0) + (has(self.selector) ? 1 : 0) + (has(self.list) ? 1 : 0)) &lt;= 1: only one of spec.targetNamespaces.selector or spec.targetNamespaces.stringSelector or spec.targetNamespaces.list can be specified</li> false"},{"location":"reference/crds/#accessmanagementstatuscurrentindextargetnamespaces_1","title":"AccessManagement.status.current[index].targetNamespaces","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TargetNamespaces defines the namespaces where selected objects will be distributed. Templates and Credentials will be distributed to all namespaces if unset.</p> Name Type Description Required list []string            List is the list of namespaces to select. Mutually exclusive with StringSelector and Selector. false selector object            Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List. false stringSelector string            StringSelector is a label query to select namespaces. Mutually exclusive with Selector and List. false"},{"location":"reference/crds/#accessmanagementstatuscurrentindextargetnamespacesselector_1","title":"AccessManagement.status.current[index].targetNamespaces.selector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Selector is a structured label query to select namespaces. Mutually exclusive with StringSelector and List.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"reference/crds/#accessmanagementstatuscurrentindextargetnamespacesselectormatchexpressionsindex_1","title":"AccessManagement.status.current[index].targetNamespaces.selector.matchExpressions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"reference/crds/#clusterdeployment_1","title":"ClusterDeployment","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterDeployment is the Schema for the ClusterDeployments API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterDeployment true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterDeploymentSpec defines the desired state of ClusterDeployment false status object            ClusterDeploymentStatus defines the observed state of ClusterDeployment false"},{"location":"reference/crds/#clusterdeploymentspec_1","title":"ClusterDeployment.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterDeploymentSpec defines the desired state of ClusterDeployment</p> Name Type Description Required template string            Template is a reference to a Template object located in the same namespace. true config JSON            Config allows to provide parameters for template customization. If no Config provided, the field will be populated with the default values for the template and DryRun will be enabled. false credential string            Name reference to the related Credentials object. false dryRun boolean            DryRun specifies whether the template should be applied after validation or only validated. false ipamClaim object            IPAMClaim defines IP Address Management (IPAM) requirements for the cluster. It can either reference an existing IPAM claim or specify an inline claim. false propagateCredentials boolean            PropagateCredentials indicates whether credentials should be propagated for use by CCM (Cloud Controller Manager). Default: true false serviceSpec object            ServiceSpec is spec related to deployment of services. false"},{"location":"reference/crds/#clusterdeploymentspecipamclaim","title":"ClusterDeployment.spec.ipamClaim","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>IPAMClaim defines IP Address Management (IPAM) requirements for the cluster. It can either reference an existing IPAM claim or specify an inline claim.</p> Name Type Description Required ref string            ClusterIPAMClaimRef is the name of an existing ClusterIPAMClaim resource to use. false spec object            ClusterIPAMClaimSpec defines the inline IPAM claim specification if no reference is provided. This allows for dynamic IP address allocation during cluster provisioning. false"},{"location":"reference/crds/#clusterdeploymentspecipamclaimspec","title":"ClusterDeployment.spec.ipamClaim.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMClaimSpec defines the inline IPAM claim specification if no reference is provided. This allows for dynamic IP address allocation during cluster provisioning.</p> Name Type Description Required provider enum            Provider is the name of the provider that this claim will be consumed by Enum: in-cluster, ipam-infoblox true cluster string            Cluster is the reference to the [ClusterDeployment] that this claim is for Validations:<li>oldSelf == '' || self == oldSelf: Cluster reference is immutable once set</li> false clusterIPAMRef string            ClusterIPAMRef is the reference to the [ClusterIPAM] resource that this claim is for Validations:<li>oldSelf == '' || self == oldSelf: ClusterIPAM reference is immutable once set</li> false clusterNetwork object            ClusterNetwork defines the allocation for requisitioning ip addresses for use by the k8s cluster itself false externalNetwork object            ExternalNetwork defines the allocation for requisitioning ip addresses for use by services such as load balancers false nodeNetwork object            NodeNetwork defines the allocation requisitioning ip addresses for cluster nodes false"},{"location":"reference/crds/#clusterdeploymentspecipamclaimspecclusternetwork","title":"ClusterDeployment.spec.ipamClaim.spec.clusterNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterNetwork defines the allocation for requisitioning ip addresses for use by the k8s cluster itself</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusterdeploymentspecipamclaimspecexternalnetwork","title":"ClusterDeployment.spec.ipamClaim.spec.externalNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ExternalNetwork defines the allocation for requisitioning ip addresses for use by services such as load balancers</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusterdeploymentspecipamclaimspecnodenetwork","title":"ClusterDeployment.spec.ipamClaim.spec.nodeNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>NodeNetwork defines the allocation requisitioning ip addresses for cluster nodes</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusterdeploymentspecservicespec_1","title":"ClusterDeployment.spec.serviceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceSpec is spec related to deployment of services.</p> Name Type Description Required continueOnError boolean            ContinueOnError specifies if the services deployment should continue if an error occurs. Default: false false driftExclusions []object            DriftExclusions specifies specific configurations of resources to ignore for drift detection. false driftIgnore []object            DriftIgnore specifies resources to ignore for drift detection. false priority integer            Priority sets the priority for the services defined in this spec. Higher value means higher priority and lower means lower. In case of conflict with another object managing the service, the one with higher priority will get to deploy its services. Format: int32 Default: 100 Minimum: 1 Maximum: 2.147483646e+09 false reload boolean            Reload instances via rolling upgrade when a ConfigMap/Secret mounted as volume is modified. false services []object            Services is a list of services created via ServiceTemplates that could be installed on the target cluster. false stopOnConflict boolean            StopOnConflict specifies what to do in case of a conflict. E.g. If another object is already managing a service. By default the remaining services will be deployed even if conflict is detected. If set to true, the deployment will stop after encountering the first conflict. Default: false false syncMode enum            SyncMode specifies how services are synced in the target cluster. Enum: OneTime, Continuous, ContinuousWithDriftDetection, DryRun Default: Continuous false templateResourceRefs []object            TemplateResourceRefs is a list of resources to collect from the management cluster, the values from which can be used in templates. false"},{"location":"reference/crds/#clusterdeploymentspecservicespecdriftexclusionsindex_1","title":"ClusterDeployment.spec.serviceSpec.driftExclusions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required paths []string            Paths is a slice of JSON6902 paths to exclude from configuration drift evaluation. true target object            Target points to the resources that the paths refers to. false"},{"location":"reference/crds/#clusterdeploymentspecservicespecdriftexclusionsindextarget_1","title":"ClusterDeployment.spec.serviceSpec.driftExclusions[index].target","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Target points to the resources that the paths refers to.</p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#clusterdeploymentspecservicespecdriftignoreindex_1","title":"ClusterDeployment.spec.serviceSpec.driftIgnore[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#clusterdeploymentspecservicespecservicesindex_1","title":"ClusterDeployment.spec.serviceSpec.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Service represents a Service to be deployed.</p> Name Type Description Required name string            Name is the chart release. true template string            Template is a reference to a Template object located in the same namespace. true disable boolean            Disable can be set to disable handling of this service. false namespace string            Namespace is the namespace the release will be installed in. It will default to Name if not provided. false templateChain string            TemplateChain defines the ServiceTemplateChain object that will be used to deploy the service along with desired ServiceTemplate version. false values string            Values is the helm values to be passed to the chart used by the template. The string type is used in order to allow for templating. false valuesFrom []object            ValuesFrom can reference a ConfigMap or Secret containing helm values. false"},{"location":"reference/crds/#clusterdeploymentspecservicespecservicesindexvaluesfromindex_1","title":"ClusterDeployment.spec.serviceSpec.services[index].valuesFrom[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required kind enum            Kind of the resource. Supported kinds are: - ConfigMap/Secret Enum: ConfigMap, Secret true name string            Name of the referenced resource. Name can be expressed as a template and instantiate using any cluster field. true namespace string            Namespace of the referenced resource. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. For Profile namespace must be left empty. The Profile namespace will be used. Namespace can be expressed as a template and instantiate using any cluster field. false optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other ValueFroms. Default: false false"},{"location":"reference/crds/#clusterdeploymentspecservicespectemplateresourcerefsindex_1","title":"ClusterDeployment.spec.serviceSpec.templateResourceRefs[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required identifier string            Identifier is how the resource will be referred to in the template true resource object            Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field. true optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other TemplateResourceRefs. Default: false false"},{"location":"reference/crds/#clusterdeploymentspecservicespectemplateresourcerefsindexresource_1","title":"ClusterDeployment.spec.serviceSpec.templateResourceRefs[index].resource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field.</p> Name Type Description Required apiVersion string            API version of the referent. false fieldPath string            If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. false kind string            Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string            Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string            Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string            UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false"},{"location":"reference/crds/#clusterdeploymentstatus_1","title":"ClusterDeployment.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterDeploymentStatus defines the observed state of ClusterDeployment</p> Name Type Description Required availableUpgrades []string            AvailableUpgrades is the list of ClusterTemplate names to which this cluster can be upgraded. It can be an empty array, which means no upgrades are available. false conditions []object            Conditions contains details for the current state of the ClusterDeployment. false k8sVersion string            Currently compatible exact Kubernetes version of the cluster. Being set only if provided by the corresponding ClusterTemplate. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false services []object            Services contains details for the state of services. false servicesUpgradePaths []object            ServicesUpgradePaths contains details for the state of services upgrade paths. false"},{"location":"reference/crds/#clusterdeploymentstatusconditionsindex_1","title":"ClusterDeployment.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#clusterdeploymentstatusservicesindex_1","title":"ClusterDeployment.status.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceStatus contains details for the state of services.</p> Name Type Description Required clusterName string            ClusterName is the name of the associated cluster. true clusterNamespace string            ClusterNamespace is the namespace of the associated cluster. false conditions []object            Conditions contains details for the current state of managed services. false"},{"location":"reference/crds/#clusterdeploymentstatusservicesindexconditionsindex_1","title":"ClusterDeployment.status.services[index].conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#clusterdeploymentstatusservicesupgradepathsindex_1","title":"ClusterDeployment.status.servicesUpgradePaths[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceUpgradePaths contains details for the state of service upgrade paths.</p> Name Type Description Required name string            Name is the name of the service. true namespace string            Namespace is the namespace of the service. true template string            Template is the name of the current service template. true availableUpgrades []object            AvailableUpgrades contains details for the state of available upgrades. false"},{"location":"reference/crds/#clusterdeploymentstatusservicesupgradepathsindexavailableupgradesindex_1","title":"ClusterDeployment.status.servicesUpgradePaths[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>UpgradePath contains details for the state of service upgrade paths.</p> Name Type Description Required upgradePaths []string            Versions contains the list of versions that service can be upgraded to. false"},{"location":"reference/crds/#clusteripamclaim","title":"ClusterIPAMClaim","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMClaim is the Schema for the clusteripamclaims API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterIPAMClaim true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterIPAMClaimSpec defines the desired state of ClusterIPAMClaim false status object            ClusterIPAMClaimStatus defines the observed state of ClusterIPAMClaim false"},{"location":"reference/crds/#clusteripamclaimspec","title":"ClusterIPAMClaim.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMClaimSpec defines the desired state of ClusterIPAMClaim</p> Name Type Description Required provider enum            Provider is the name of the provider that this claim will be consumed by Enum: in-cluster, ipam-infoblox true cluster string            Cluster is the reference to the [ClusterDeployment] that this claim is for Validations:<li>oldSelf == '' || self == oldSelf: Cluster reference is immutable once set</li> false clusterIPAMRef string            ClusterIPAMRef is the reference to the [ClusterIPAM] resource that this claim is for Validations:<li>oldSelf == '' || self == oldSelf: ClusterIPAM reference is immutable once set</li> false clusterNetwork object            ClusterNetwork defines the allocation for requisitioning ip addresses for use by the k8s cluster itself false externalNetwork object            ExternalNetwork defines the allocation for requisitioning ip addresses for use by services such as load balancers false nodeNetwork object            NodeNetwork defines the allocation requisitioning ip addresses for cluster nodes false"},{"location":"reference/crds/#clusteripamclaimspecclusternetwork","title":"ClusterIPAMClaim.spec.clusterNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterNetwork defines the allocation for requisitioning ip addresses for use by the k8s cluster itself</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusteripamclaimspecexternalnetwork","title":"ClusterIPAMClaim.spec.externalNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ExternalNetwork defines the allocation for requisitioning ip addresses for use by services such as load balancers</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusteripamclaimspecnodenetwork","title":"ClusterIPAMClaim.spec.nodeNetwork","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>NodeNetwork defines the allocation requisitioning ip addresses for cluster nodes</p> Name Type Description Required cidr string            CIDR notation of the allocated address space false ipAddresses []string            IPAddresses to be allocated false"},{"location":"reference/crds/#clusteripamclaimstatus","title":"ClusterIPAMClaim.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMClaimStatus defines the observed state of ClusterIPAMClaim</p> Name Type Description Required bound boolean            Bound is a flag to indicate that the claim is bound because all ip addresses are allocated Default: false true conditions []object            Conditions contains details for the current state of the [ClusterIPAMClaim] false"},{"location":"reference/crds/#clusteripamclaimstatusconditionsindex","title":"ClusterIPAMClaim.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#clusteripam","title":"ClusterIPAM","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAM is the Schema for the clusteripams API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterIPAM true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterIPAMSpec defines the desired state of ClusterIPAM false status object            ClusterIPAMStatus defines the observed state of ClusterIPAM false"},{"location":"reference/crds/#clusteripamspec","title":"ClusterIPAM.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMSpec defines the desired state of ClusterIPAM</p> Name Type Description Required clusterIPAMClaimRef string            ClusterIPAMClaimRef is a reference to the [ClusterIPAMClaim] that this [ClusterIPAM] is bound to. Validations:<li>oldSelf == '' || self == oldSelf: Claim reference is immutable once set</li> false provider enum            The provider that this claim will be consumed by Enum: in-cluster, ipam-infoblox false"},{"location":"reference/crds/#clusteripamstatus","title":"ClusterIPAM.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterIPAMStatus defines the observed state of ClusterIPAM</p> Name Type Description Required phase enum            Phase is the current phase of the ClusterIPAM. Enum: Pending, Bound false providerData []object            ProviderData is the provider specific data produced for the ClusterIPAM. This field is represented as a list, because it will store multiple entries for different networks - nodes, cluster (pods, services), external - for the same provider. false"},{"location":"reference/crds/#clusteripamstatusproviderdataindex","title":"ClusterIPAM.status.providerData[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required config JSON            Data is the IPAM provider specific data false name string            Name of the IPAM provider data false ready boolean            Ready indicates that the IPAM provider data is ready false"},{"location":"reference/crds/#clustertemplatechain_1","title":"ClusterTemplateChain","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplateChain is the Schema for the clustertemplatechains API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterTemplateChain true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            TemplateChainSpec defines the desired state of *TemplateChain Validations:<li>self == oldSelf: Spec is immutable</li> false status object            TemplateChainStatus defines the observed state of *TemplateChain false"},{"location":"reference/crds/#clustertemplatechainspec_1","title":"ClusterTemplateChain.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainSpec defines the desired state of *TemplateChain</p> Name Type Description Required supportedTemplates []object            SupportedTemplates is the list of supported Templates definitions and all available upgrade sequences for it. false"},{"location":"reference/crds/#clustertemplatechainspecsupportedtemplatesindex_1","title":"ClusterTemplateChain.spec.supportedTemplates[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SupportedTemplate is the supported Template definition and all available upgrade sequences for it</p> Name Type Description Required name string            Name is the name of the Template. true availableUpgrades []object            AvailableUpgrades is the list of available upgrades for the specified Template. false"},{"location":"reference/crds/#clustertemplatechainspecsupportedtemplatesindexavailableupgradesindex_1","title":"ClusterTemplateChain.spec.supportedTemplates[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AvailableUpgrade is the definition of the available upgrade for the Template</p> Name Type Description Required name string            Name is the name of the Template to which the upgrade is available. true"},{"location":"reference/crds/#clustertemplatechainstatus_1","title":"ClusterTemplateChain.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainStatus defines the observed state of *TemplateChain</p> Name Type Description Required valid boolean            Valid indicates whether the chain is valid and can be considered when calculating available upgrade paths. false validationError string            ValidationError provides information regarding issues encountered during templatechain validation. false"},{"location":"reference/crds/#clustertemplate_1","title":"ClusterTemplate","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplate is the Schema for the clustertemplates API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ClusterTemplate true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ClusterTemplateSpec defines the desired state of ClusterTemplate Validations:<li>self == oldSelf: Spec is immutable</li><li>!has(self.helm.chartSource): .spec.helm.chartSource is not supported for ClusterTemplates</li> false status object            ClusterTemplateStatus defines the observed state of ClusterTemplate false"},{"location":"reference/crds/#clustertemplatespec_1","title":"ClusterTemplate.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplateSpec defines the desired state of ClusterTemplate</p> Name Type Description Required helm object            HelmSpec references a Helm chart representing the KCM template Validations:<li>(has(self.chartSpec) ? (!has(self.chartSource) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartSource) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartRef) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartSource)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>has(self.chartSpec) || has(self.chartRef) || has(self.chartSource): one of chartSpec, chartRef or chartSource must be set</li> true k8sVersion string            Kubernetes exact version in the SemVer format provided by this ClusterTemplate. false providerContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false providers []string            Providers represent required CAPI providers. Should be set if not present in the Helm chart metadata. false"},{"location":"reference/crds/#clustertemplatespechelm_1","title":"ClusterTemplate.spec.helm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HelmSpec references a Helm chart representing the KCM template</p> Name Type Description Required chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartSource object            ChartSource is a source of a Helm chart representing the template. Validations:<li>has(self.localSourceRef) ? (self.localSourceRef.kind != 'Secret' &amp;&amp; self.localSourceRef.kind != 'ConfigMap'): true: Secret and ConfigMap are not supported as Helm chart sources</li><li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false chartSpec object            ChartSpec defines the desired state of the HelmChart to be created by the controller false"},{"location":"reference/crds/#clustertemplatespechelmchartref_1","title":"ClusterTemplate.spec.helm.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#clustertemplatespechelmchartsource_1","title":"ClusterTemplate.spec.helm.chartSource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSource is a source of a Helm chart representing the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#clustertemplatespechelmchartsourcelocalsourceref_1","title":"ClusterTemplate.spec.helm.chartSource.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespec_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucket_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketcertsecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketproxysecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketsecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketsts_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketstscertsecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecbucketstssecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgit_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitincludeindex_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitincludeindexrepository_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitproxysecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitsecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitverify_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecgitverifysecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecoci_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecocicertsecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecocilayerselector_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociproxysecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecocisecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociverify_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociverifymatchoidcidentityindex_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#clustertemplatespechelmchartsourceremotesourcespecociverifysecretref_1","title":"ClusterTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatespechelmchartspec_1","title":"ClusterTemplate.spec.helm.chartSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSpec defines the desired state of the HelmChart to be created by the controller</p> Name Type Description Required chart string            Chart is the name or path the Helm chart is available at in the SourceRef. true interval string            Interval at which the HelmChart SourceRef is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true sourceRef object            SourceRef is the reference to the Source the chart is available at. true ignoreMissingValuesFiles boolean            IgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing. false reconcileStrategy enum            ReconcileStrategy determines what enables the creation of a new artifact. Valid values are ('ChartVersion', 'Revision'). See the documentation of the values for an explanation on their behavior. Defaults to ChartVersion when omitted. Enum: ChartVersion, Revision Default: ChartVersion false suspend boolean            Suspend tells the controller to suspend the reconciliation of this source. false valuesFiles []string            ValuesFiles is an alternative list of values files to use as the chart values (values.yaml is not included by default), expected to be a relative path in the SourceRef. Values files are merged in the order of this list with the last file overriding the first. Ignored when omitted. false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified. false version string            Version is the chart version semver expression, ignored for charts from GitRepository and Bucket sources. Defaults to latest when omitted. Default: * false"},{"location":"reference/crds/#clustertemplatespechelmchartspecsourceref_1","title":"ClusterTemplate.spec.helm.chartSpec.sourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceRef is the reference to the Source the chart is available at.</p> Name Type Description Required kind enum            Kind of the referent, valid values are ('HelmRepository', 'GitRepository', 'Bucket'). Enum: HelmRepository, GitRepository, Bucket true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false"},{"location":"reference/crds/#clustertemplatespechelmchartspecverify_1","title":"ClusterTemplate.spec.helm.chartSpec.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#clustertemplatespechelmchartspecverifymatchoidcidentityindex_1","title":"ClusterTemplate.spec.helm.chartSpec.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#clustertemplatespechelmchartspecverifysecretref_1","title":"ClusterTemplate.spec.helm.chartSpec.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#clustertemplatestatus_1","title":"ClusterTemplate.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterTemplateStatus defines the observed state of ClusterTemplate</p> Name Type Description Required valid boolean            Valid indicates whether the template passed validation or not. true chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartVersion string            ChartVersion represents the version of the Helm Chart associated with this template. false config JSON            Config demonstrates available parameters for template customization, that can be used when creating ClusterDeployment objects. false description string            Description contains information about the template. false k8sVersion string            Kubernetes exact version in the SemVer format provided by this ClusterTemplate. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false providerContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false providers []string            Providers represent required CAPI providers. false validationError string            ValidationError provides information regarding issues encountered during template validation. false"},{"location":"reference/crds/#clustertemplatestatuschartref_1","title":"ClusterTemplate.status.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#credential_1","title":"Credential","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Credential is the Schema for the credentials API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string Credential true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            CredentialSpec defines the desired state of Credential false status object            CredentialStatus defines the observed state of Credential false"},{"location":"reference/crds/#credentialspec_1","title":"Credential.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CredentialSpec defines the desired state of Credential</p> Name Type Description Required identityRef object            Reference to the Credential Identity true description string            Description of the Credential object false"},{"location":"reference/crds/#credentialspecidentityref_1","title":"Credential.spec.identityRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference to the Credential Identity</p> Name Type Description Required apiVersion string            API version of the referent. false fieldPath string            If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. false kind string            Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string            Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string            Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string            UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false"},{"location":"reference/crds/#credentialstatus_1","title":"Credential.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CredentialStatus defines the observed state of Credential</p> Name Type Description Required ready boolean            Ready holds the readiness of Credentials. Default: false true conditions []object            Conditions contains details for the current state of the Credential. false"},{"location":"reference/crds/#credentialstatusconditionsindex_1","title":"Credential.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#managementbackup_1","title":"ManagementBackup","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementBackup is the Schema for the managementbackups API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ManagementBackup true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ManagementBackupSpec defines the desired state of ManagementBackup false status object            ManagementBackupStatus defines the observed state of ManagementBackup false"},{"location":"reference/crds/#managementbackupspec_1","title":"ManagementBackup.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementBackupSpec defines the desired state of ManagementBackup</p> Name Type Description Required performOnManagementUpgrade boolean            PerformOnManagementUpgrade indicates that a single [ManagementBackup] should be created and stored in the [ManagementBackup] storage location if not default before the [Management] release upgrade. false schedule string            Schedule is a Cron expression defining when to run the scheduled [ManagementBackup]. If not set, the object is considered to be run only once. false storageLocation string            StorageLocation is the name of a [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.StorageLocation] where the backup should be stored. false"},{"location":"reference/crds/#managementbackupstatus_1","title":"ManagementBackup.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementBackupStatus defines the observed state of ManagementBackup</p> Name Type Description Required error string            Error stores messages in case of failed backup creation. false lastBackup object            Most recently [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup] that has been created. false lastBackupName string            Name of most recently created [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup]. false lastBackupTime string            Time of the most recently created [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup]. Format: date-time false nextAttempt string            NextAttempt indicates the time when the next backup will be created. Always absent for a single [ManagementBackup]. Format: date-time false"},{"location":"reference/crds/#managementbackupstatuslastbackup_1","title":"ManagementBackup.status.lastBackup","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Most recently [github.com/vmware-tanzu/velero/pkg/apis/velero/v1.Backup] that has been created.</p> Name Type Description Required backupItemOperationsAttempted integer            BackupItemOperationsAttempted is the total number of attempted async BackupItemAction operations for this backup. false backupItemOperationsCompleted integer            BackupItemOperationsCompleted is the total number of successfully completed async BackupItemAction operations for this backup. false backupItemOperationsFailed integer            BackupItemOperationsFailed is the total number of async BackupItemAction operations for this backup which ended with an error. false completionTimestamp string            CompletionTimestamp records the time a backup was completed. Completion time is recorded even on failed backups. Completion time is recorded before uploading the backup object. The server's time is used for CompletionTimestamps Format: date-time false csiVolumeSnapshotsAttempted integer            CSIVolumeSnapshotsAttempted is the total number of attempted CSI VolumeSnapshots for this backup. false csiVolumeSnapshotsCompleted integer            CSIVolumeSnapshotsCompleted is the total number of successfully completed CSI VolumeSnapshots for this backup. false errors integer            Errors is a count of all error messages that were generated during execution of the backup.  The actual errors are in the backup's log file in object storage. false expiration string            Expiration is when this Backup is eligible for garbage-collection. Format: date-time false failureReason string            FailureReason is an error that caused the entire backup to fail. false formatVersion string            FormatVersion is the backup format version, including major, minor, and patch version. false hookStatus object            HookStatus contains information about the status of the hooks. false phase enum            Phase is the current state of the Backup. Enum: New, FailedValidation, InProgress, WaitingForPluginOperations, WaitingForPluginOperationsPartiallyFailed, Finalizing, FinalizingPartiallyFailed, Completed, PartiallyFailed, Failed, Deleting false progress object            Progress contains information about the backup's execution progress. Note that this information is best-effort only -- if Velero fails to update it during a backup for any reason, it may be inaccurate/stale. false startTimestamp string            StartTimestamp records the time a backup was started. Separate from CreationTimestamp, since that value changes on restores. The server's time is used for StartTimestamps Format: date-time false validationErrors []string            ValidationErrors is a slice of all validation errors (if applicable). false version integer            Version is the backup format major version. Deprecated: Please see FormatVersion false volumeSnapshotsAttempted integer            VolumeSnapshotsAttempted is the total number of attempted volume snapshots for this backup. false volumeSnapshotsCompleted integer            VolumeSnapshotsCompleted is the total number of successfully completed volume snapshots for this backup. false warnings integer            Warnings is a count of all warning messages that were generated during execution of the backup. The actual warnings are in the backup's log file in object storage. false"},{"location":"reference/crds/#managementbackupstatuslastbackuphookstatus_1","title":"ManagementBackup.status.lastBackup.hookStatus","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HookStatus contains information about the status of the hooks.</p> Name Type Description Required hooksAttempted integer            HooksAttempted is the total number of attempted hooks Specifically, HooksAttempted represents the number of hooks that failed to execute and the number of hooks that executed successfully. false hooksFailed integer            HooksFailed is the total number of hooks which ended with an error false"},{"location":"reference/crds/#managementbackupstatuslastbackupprogress_1","title":"ManagementBackup.status.lastBackup.progress","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Progress contains information about the backup's execution progress. Note that this information is best-effort only -- if Velero fails to update it during a backup for any reason, it may be inaccurate/stale.</p> Name Type Description Required itemsBackedUp integer            ItemsBackedUp is the number of items that have actually been written to the backup tarball so far. false totalItems integer            TotalItems is the total number of items to be backed up. This number may change throughout the execution of the backup due to plugins that return additional related items to back up, the velero.io/exclude-from-backup label, and various other filters that happen as items are processed. false"},{"location":"reference/crds/#management_1","title":"Management","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Management is the Schema for the managements API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string Management true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ManagementSpec defines the desired state of Management false status object            ManagementStatus defines the observed state of Management false"},{"location":"reference/crds/#managementspec_1","title":"Management.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementSpec defines the desired state of Management</p> Name Type Description Required release string            Release references the Release object. true core object            Core holds the core Management components that are mandatory. If not specified, will be populated with the default values. false providers []object            Providers is the list of supported CAPI providers. false"},{"location":"reference/crds/#managementspeccore_1","title":"Management.spec.core","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Core holds the core Management components that are mandatory. If not specified, will be populated with the default values.</p> Name Type Description Required capi object            CAPI represents the core Cluster API component and references the Cluster API template. false kcm object            KCM represents the core KCM component and references the KCM template. false"},{"location":"reference/crds/#managementspeccorecapi_1","title":"Management.spec.core.capi","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CAPI represents the core Cluster API component and references the Cluster API template.</p> Name Type Description Required config JSON            Config allows to provide parameters for management component customization. If no Config provided, the field will be populated with the default values for the template. false template string            Template is the name of the Template associated with this component. If not specified, will be taken from the Release object. false"},{"location":"reference/crds/#managementspeccorekcm_1","title":"Management.spec.core.kcm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>KCM represents the core KCM component and references the KCM template.</p> Name Type Description Required config JSON            Config allows to provide parameters for management component customization. If no Config provided, the field will be populated with the default values for the template. false template string            Template is the name of the Template associated with this component. If not specified, will be taken from the Release object. false"},{"location":"reference/crds/#managementspecprovidersindex_1","title":"Management.spec.providers[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required name string            Name of the provider. true config JSON            Config allows to provide parameters for management component customization. If no Config provided, the field will be populated with the default values for the template. false template string            Template is the name of the Template associated with this component. If not specified, will be taken from the Release object. false"},{"location":"reference/crds/#managementstatus_1","title":"Management.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ManagementStatus defines the observed state of Management</p> Name Type Description Required availableProviders []string            AvailableProviders holds all available CAPI providers. false backupName string            BackupName is a name of the management cluster scheduled backup. false capiContracts map[string]map[string]string            For each CAPI provider name holds its compatibility [contract versions] in a key-value pairs, where the key is the core CAPI contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core CAPI.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false components map[string]object            Components indicates the status of installed KCM components and CAPI providers. false conditions []object            Conditions represents the observations of a Management's current state. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false release string            Release indicates the current Release object. false"},{"location":"reference/crds/#managementstatuscomponentskey_1","title":"Management.status.components[key]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ComponentStatus is the status of Management component installation</p> Name Type Description Required error string            Error stores as error message in case of failed installation false exposedProviders []string            ExposedProviders is a list of CAPI providers this component exposes false success boolean            Success represents if a component installation was successful false template string            Template is the name of the Template associated with this component. false"},{"location":"reference/crds/#managementstatusconditionsindex_1","title":"Management.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#multiclusterservice_1","title":"MultiClusterService","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>MultiClusterService is the Schema for the multiclusterservices API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string MultiClusterService true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            MultiClusterServiceSpec defines the desired state of MultiClusterService false status object            MultiClusterServiceStatus defines the observed state of MultiClusterService. false"},{"location":"reference/crds/#multiclusterservicespec_1","title":"MultiClusterService.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>MultiClusterServiceSpec defines the desired state of MultiClusterService</p> Name Type Description Required clusterSelector object            ClusterSelector identifies target clusters to manage services on. false serviceSpec object            ServiceSpec is spec related to deployment of services. false"},{"location":"reference/crds/#multiclusterservicespecclusterselector_1","title":"MultiClusterService.spec.clusterSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ClusterSelector identifies target clusters to manage services on.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"reference/crds/#multiclusterservicespecclusterselectormatchexpressionsindex_1","title":"MultiClusterService.spec.clusterSelector.matchExpressions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"reference/crds/#multiclusterservicespecservicespec_1","title":"MultiClusterService.spec.serviceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceSpec is spec related to deployment of services.</p> Name Type Description Required continueOnError boolean            ContinueOnError specifies if the services deployment should continue if an error occurs. Default: false false driftExclusions []object            DriftExclusions specifies specific configurations of resources to ignore for drift detection. false driftIgnore []object            DriftIgnore specifies resources to ignore for drift detection. false priority integer            Priority sets the priority for the services defined in this spec. Higher value means higher priority and lower means lower. In case of conflict with another object managing the service, the one with higher priority will get to deploy its services. Format: int32 Default: 100 Minimum: 1 Maximum: 2.147483646e+09 false reload boolean            Reload instances via rolling upgrade when a ConfigMap/Secret mounted as volume is modified. false services []object            Services is a list of services created via ServiceTemplates that could be installed on the target cluster. false stopOnConflict boolean            StopOnConflict specifies what to do in case of a conflict. E.g. If another object is already managing a service. By default the remaining services will be deployed even if conflict is detected. If set to true, the deployment will stop after encountering the first conflict. Default: false false syncMode enum            SyncMode specifies how services are synced in the target cluster. Enum: OneTime, Continuous, ContinuousWithDriftDetection, DryRun Default: Continuous false templateResourceRefs []object            TemplateResourceRefs is a list of resources to collect from the management cluster, the values from which can be used in templates. false"},{"location":"reference/crds/#multiclusterservicespecservicespecdriftexclusionsindex_1","title":"MultiClusterService.spec.serviceSpec.driftExclusions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required paths []string            Paths is a slice of JSON6902 paths to exclude from configuration drift evaluation. true target object            Target points to the resources that the paths refers to. false"},{"location":"reference/crds/#multiclusterservicespecservicespecdriftexclusionsindextarget_1","title":"MultiClusterService.spec.serviceSpec.driftExclusions[index].target","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Target points to the resources that the paths refers to.</p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#multiclusterservicespecservicespecdriftignoreindex_1","title":"MultiClusterService.spec.serviceSpec.driftIgnore[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required annotationSelector string            AnnotationSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource annotations. false group string            Group is the API group to select resources from. Together with Version and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false kind string            Kind of the API Group to select resources from. Together with Group and Version it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false labelSelector string            LabelSelector is a string that follows the label selection expression https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#api It matches with the resource labels. false name string            Name to match resources with. false namespace string            Namespace to select resources from. false version string            Version of the API Group to select resources from. Together with Group and Kind it is capable of unambiguously identifying and/or selecting resources. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md false"},{"location":"reference/crds/#multiclusterservicespecservicespecservicesindex_1","title":"MultiClusterService.spec.serviceSpec.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Service represents a Service to be deployed.</p> Name Type Description Required name string            Name is the chart release. true template string            Template is a reference to a Template object located in the same namespace. true disable boolean            Disable can be set to disable handling of this service. false namespace string            Namespace is the namespace the release will be installed in. It will default to Name if not provided. false templateChain string            TemplateChain defines the ServiceTemplateChain object that will be used to deploy the service along with desired ServiceTemplate version. false values string            Values is the helm values to be passed to the chart used by the template. The string type is used in order to allow for templating. false valuesFrom []object            ValuesFrom can reference a ConfigMap or Secret containing helm values. false"},{"location":"reference/crds/#multiclusterservicespecservicespecservicesindexvaluesfromindex_1","title":"MultiClusterService.spec.serviceSpec.services[index].valuesFrom[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required kind enum            Kind of the resource. Supported kinds are: - ConfigMap/Secret Enum: ConfigMap, Secret true name string            Name of the referenced resource. Name can be expressed as a template and instantiate using any cluster field. true namespace string            Namespace of the referenced resource. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. For Profile namespace must be left empty. The Profile namespace will be used. Namespace can be expressed as a template and instantiate using any cluster field. false optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other ValueFroms. Default: false false"},{"location":"reference/crds/#multiclusterservicespecservicespectemplateresourcerefsindex_1","title":"MultiClusterService.spec.serviceSpec.templateResourceRefs[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required identifier string            Identifier is how the resource will be referred to in the template true resource object            Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field. true optional boolean            Optional indicates that the referenced resource is not mandatory. If set to true and the resource is not found, the error will be ignored, and Sveltos will continue processing other TemplateResourceRefs. Default: false false"},{"location":"reference/crds/#multiclusterservicespecservicespectemplateresourcerefsindexresource_1","title":"MultiClusterService.spec.serviceSpec.templateResourceRefs[index].resource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resource references a Kubernetes instance in the management cluster to fetch and use during template instantiation. For ClusterProfile namespace can be left empty. In such a case, namespace will be implicit set to cluster's namespace. Name and namespace can be expressed as a template and instantiate using any cluster field.</p> Name Type Description Required apiVersion string            API version of the referent. false fieldPath string            If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. false kind string            Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string            Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string            Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string            UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false"},{"location":"reference/crds/#multiclusterservicestatus_1","title":"MultiClusterService.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>MultiClusterServiceStatus defines the observed state of MultiClusterService.</p> Name Type Description Required conditions []object            Conditions contains details for the current state of the MultiClusterService. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false services []object            Services contains details for the state of services. false servicesUpgradePaths []object            ServicesUpgradePaths contains details for the state of services upgrade paths. false"},{"location":"reference/crds/#multiclusterservicestatusconditionsindex_1","title":"MultiClusterService.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#multiclusterservicestatusservicesindex_1","title":"MultiClusterService.status.services[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceStatus contains details for the state of services.</p> Name Type Description Required clusterName string            ClusterName is the name of the associated cluster. true clusterNamespace string            ClusterNamespace is the namespace of the associated cluster. false conditions []object            Conditions contains details for the current state of managed services. false"},{"location":"reference/crds/#multiclusterservicestatusservicesindexconditionsindex_1","title":"MultiClusterService.status.services[index].conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#multiclusterservicestatusservicesupgradepathsindex_1","title":"MultiClusterService.status.servicesUpgradePaths[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceUpgradePaths contains details for the state of service upgrade paths.</p> Name Type Description Required name string            Name is the name of the service. true namespace string            Namespace is the namespace of the service. true template string            Template is the name of the current service template. true availableUpgrades []object            AvailableUpgrades contains details for the state of available upgrades. false"},{"location":"reference/crds/#multiclusterservicestatusservicesupgradepathsindexavailableupgradesindex_1","title":"MultiClusterService.status.servicesUpgradePaths[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>UpgradePath contains details for the state of service upgrade paths.</p> Name Type Description Required upgradePaths []string            Versions contains the list of versions that service can be upgraded to. false"},{"location":"reference/crds/#providerinterface_1","title":"ProviderInterface","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderInterface is the Schema for the ProviderInterface API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ProviderInterface true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ProviderInterfaceSpec defines the desired state of ProviderInterface false status object            ProviderInterfaceStatus defines the observed state of ProviderInterface false"},{"location":"reference/crds/#providerinterfacespec_1","title":"ProviderInterface.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderInterfaceSpec defines the desired state of ProviderInterface</p> Name Type Description Required clusterGVKs []object            ClusterGVKs defines the Group-Version-Kind resources this provider can manage false clusterIdentityKinds []string            ClusterIdentityKinds defines the Kind of identity objects supported by this provider false description string            Description provides a human-readable explanation of what this provider does false"},{"location":"reference/crds/#providerinterfacespecclustergvksindex_1","title":"ProviderInterface.spec.clusterGVKs[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GroupVersionKind unambiguously identifies a kind. It doesn't anonymously include GroupVersion to avoid automatic coercion. It doesn't use a GroupVersion to avoid custom marshalling Note: mirror of https://github.com/kubernetes/apimachinery/blob/v0.32.3/pkg/runtime/schema/group_version.go#L140-L146</p> Name Type Description Required group string true kind string true version string true"},{"location":"reference/crds/#providerinterfacestatus_1","title":"ProviderInterface.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderInterfaceStatus defines the observed state of ProviderInterface</p> Name Type Description Required exposedProviders string            ExposedProviders contains the list of exposed provider false"},{"location":"reference/crds/#providertemplate_1","title":"ProviderTemplate","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderTemplate is the Schema for the providertemplates API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ProviderTemplate true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ProviderTemplateSpec defines the desired state of ProviderTemplate Validations:<li>self == oldSelf: Spec is immutable</li><li>!has(self.helm.chartSource): .spec.helm.chartSource is not supported for ProviderTemplates</li> false status object            ProviderTemplateStatus defines the observed state of ProviderTemplate false"},{"location":"reference/crds/#providertemplatespec_1","title":"ProviderTemplate.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderTemplateSpec defines the desired state of ProviderTemplate</p> Name Type Description Required capiContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the core CAPI contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core CAPI.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false helm object            HelmSpec references a Helm chart representing the KCM template Validations:<li>(has(self.chartSpec) ? (!has(self.chartSource) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartSource) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartRef) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartSource)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>has(self.chartSpec) || has(self.chartRef) || has(self.chartSource): one of chartSpec, chartRef or chartSource must be set</li> false providers []string            Providers represent exposed CAPI providers. Should be set if not present in the Helm chart metadata. false"},{"location":"reference/crds/#providertemplatespechelm_1","title":"ProviderTemplate.spec.helm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HelmSpec references a Helm chart representing the KCM template</p> Name Type Description Required chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartSource object            ChartSource is a source of a Helm chart representing the template. Validations:<li>has(self.localSourceRef) ? (self.localSourceRef.kind != 'Secret' &amp;&amp; self.localSourceRef.kind != 'ConfigMap'): true: Secret and ConfigMap are not supported as Helm chart sources</li><li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false chartSpec object            ChartSpec defines the desired state of the HelmChart to be created by the controller false"},{"location":"reference/crds/#providertemplatespechelmchartref_1","title":"ProviderTemplate.spec.helm.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#providertemplatespechelmchartsource_1","title":"ProviderTemplate.spec.helm.chartSource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSource is a source of a Helm chart representing the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#providertemplatespechelmchartsourcelocalsourceref_1","title":"ProviderTemplate.spec.helm.chartSource.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespec_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucket_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketcertsecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketproxysecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketsecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketsts_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketstscertsecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecbucketstssecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgit_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitincludeindex_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitincludeindexrepository_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitproxysecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitsecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitverify_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecgitverifysecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecoci_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecocicertsecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecocilayerselector_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociproxysecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecocisecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociverify_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociverifymatchoidcidentityindex_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#providertemplatespechelmchartsourceremotesourcespecociverifysecretref_1","title":"ProviderTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatespechelmchartspec_1","title":"ProviderTemplate.spec.helm.chartSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSpec defines the desired state of the HelmChart to be created by the controller</p> Name Type Description Required chart string            Chart is the name or path the Helm chart is available at in the SourceRef. true interval string            Interval at which the HelmChart SourceRef is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true sourceRef object            SourceRef is the reference to the Source the chart is available at. true ignoreMissingValuesFiles boolean            IgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing. false reconcileStrategy enum            ReconcileStrategy determines what enables the creation of a new artifact. Valid values are ('ChartVersion', 'Revision'). See the documentation of the values for an explanation on their behavior. Defaults to ChartVersion when omitted. Enum: ChartVersion, Revision Default: ChartVersion false suspend boolean            Suspend tells the controller to suspend the reconciliation of this source. false valuesFiles []string            ValuesFiles is an alternative list of values files to use as the chart values (values.yaml is not included by default), expected to be a relative path in the SourceRef. Values files are merged in the order of this list with the last file overriding the first. Ignored when omitted. false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified. false version string            Version is the chart version semver expression, ignored for charts from GitRepository and Bucket sources. Defaults to latest when omitted. Default: * false"},{"location":"reference/crds/#providertemplatespechelmchartspecsourceref_1","title":"ProviderTemplate.spec.helm.chartSpec.sourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceRef is the reference to the Source the chart is available at.</p> Name Type Description Required kind enum            Kind of the referent, valid values are ('HelmRepository', 'GitRepository', 'Bucket'). Enum: HelmRepository, GitRepository, Bucket true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false"},{"location":"reference/crds/#providertemplatespechelmchartspecverify_1","title":"ProviderTemplate.spec.helm.chartSpec.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#providertemplatespechelmchartspecverifymatchoidcidentityindex_1","title":"ProviderTemplate.spec.helm.chartSpec.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#providertemplatespechelmchartspecverifysecretref_1","title":"ProviderTemplate.spec.helm.chartSpec.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#providertemplatestatus_1","title":"ProviderTemplate.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProviderTemplateStatus defines the observed state of ProviderTemplate</p> Name Type Description Required valid boolean            Valid indicates whether the template passed validation or not. true capiContracts map[string]string            Holds key-value pairs with compatibility [contract versions], where the key is the core CAPI contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core CAPI.  [contract versions]: https://cluster-api.sigs.k8s.io/developer/providers/contracts false chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartVersion string            ChartVersion represents the version of the Helm Chart associated with this template. false config JSON            Config demonstrates available parameters for template customization, that can be used when creating ClusterDeployment objects. false description string            Description contains information about the template. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false providers []string            Providers represent exposed CAPI providers. false validationError string            ValidationError provides information regarding issues encountered during template validation. false"},{"location":"reference/crds/#providertemplatestatuschartref_1","title":"ProviderTemplate.status.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#release_1","title":"Release","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Release is the Schema for the releases API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string Release true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ReleaseSpec defines the desired state of Release false status object            ReleaseStatus defines the observed state of Release false"},{"location":"reference/crds/#releasespec_1","title":"Release.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ReleaseSpec defines the desired state of Release</p> Name Type Description Required capi object            CAPI references the Cluster API template. true kcm object            KCM references the KCM template. true version string            Version of the KCM Release in the semver format. true providers []object            Providers contains a list of Providers associated with the Release. false"},{"location":"reference/crds/#releasespeccapi_1","title":"Release.spec.capi","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CAPI references the Cluster API template.</p> Name Type Description Required template string            Template references the Template associated with the provider. true"},{"location":"reference/crds/#releasespeckcm_1","title":"Release.spec.kcm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>KCM references the KCM template.</p> Name Type Description Required template string            Template references the Template associated with the provider. true"},{"location":"reference/crds/#releasespecprovidersindex_1","title":"Release.spec.providers[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required name string            Name of the provider. true template string            Template references the Template associated with the provider. true"},{"location":"reference/crds/#releasestatus_1","title":"Release.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ReleaseStatus defines the observed state of Release</p> Name Type Description Required conditions []object            Conditions contains details for the current state of the Release false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false ready boolean            Ready indicates whether KCM is ready to be upgraded to this Release. false"},{"location":"reference/crds/#releasestatusconditionsindex_1","title":"Release.status.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/crds/#servicetemplatechain_1","title":"ServiceTemplateChain","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplateChain is the Schema for the servicetemplatechains API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ServiceTemplateChain true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            TemplateChainSpec defines the desired state of *TemplateChain Validations:<li>self == oldSelf: Spec is immutable</li> false status object            TemplateChainStatus defines the observed state of *TemplateChain false"},{"location":"reference/crds/#servicetemplatechainspec_1","title":"ServiceTemplateChain.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainSpec defines the desired state of *TemplateChain</p> Name Type Description Required supportedTemplates []object            SupportedTemplates is the list of supported Templates definitions and all available upgrade sequences for it. false"},{"location":"reference/crds/#servicetemplatechainspecsupportedtemplatesindex_1","title":"ServiceTemplateChain.spec.supportedTemplates[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SupportedTemplate is the supported Template definition and all available upgrade sequences for it</p> Name Type Description Required name string            Name is the name of the Template. true availableUpgrades []object            AvailableUpgrades is the list of available upgrades for the specified Template. false"},{"location":"reference/crds/#servicetemplatechainspecsupportedtemplatesindexavailableupgradesindex_1","title":"ServiceTemplateChain.spec.supportedTemplates[index].availableUpgrades[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>AvailableUpgrade is the definition of the available upgrade for the Template</p> Name Type Description Required name string            Name is the name of the Template to which the upgrade is available. true"},{"location":"reference/crds/#servicetemplatechainstatus_1","title":"ServiceTemplateChain.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>TemplateChainStatus defines the observed state of *TemplateChain</p> Name Type Description Required valid boolean            Valid indicates whether the chain is valid and can be considered when calculating available upgrade paths. false validationError string            ValidationError provides information regarding issues encountered during templatechain validation. false"},{"location":"reference/crds/#servicetemplate_1","title":"ServiceTemplate","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplate is the Schema for the servicetemplates API</p> Name Type Description Required apiVersion string k0rdent.mirantis.com/v1beta1 true kind string ServiceTemplate true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            ServiceTemplateSpec defines the desired state of ServiceTemplate Validations:<li>self == oldSelf: Spec is immutable</li><li>has(self.helm) ? (!has(self.kustomize) &amp;&amp; !has(self.resources)): true: Helm, Kustomize and Resources are mutually exclusive.</li><li>has(self.kustomize) ? (!has(self.helm) &amp;&amp; !has(self.resources)): true: Helm, Kustomize and Resources are mutually exclusive.</li><li>has(self.resources) ? (!has(self.kustomize) &amp;&amp; !has(self.helm)): true: Helm, Kustomize and Resources are mutually exclusive.</li><li>has(self.helm) || has(self.kustomize) || has(self.resources): One of Helm, Kustomize, or Resources must be specified.</li> false status object            ServiceTemplateStatus defines the observed state of ServiceTemplate false"},{"location":"reference/crds/#servicetemplatespec_1","title":"ServiceTemplate.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplateSpec defines the desired state of ServiceTemplate</p> Name Type Description Required helm object            Helm contains the Helm chart information for the template. Validations:<li>(has(self.chartSpec) ? (!has(self.chartSource) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartSource) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartRef)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>(has(self.chartRef) ? (!has(self.chartSpec) &amp;&amp; !has(self.chartSource)): true): chartSpec, chartSource and chartRef are mutually exclusive</li><li>has(self.chartSpec) || has(self.chartRef) || has(self.chartSource): one of chartSpec, chartRef or chartSource must be set</li> false k8sConstraint string            Constraint describing compatible K8S versions of the cluster set in the SemVer format. false kustomize object            Kustomize contains the Kustomize configuration for the template. Validations:<li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false resources object            Resources contains the resource configuration for the template. Validations:<li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false version string            Version is the semantic version of the application backed by template. false"},{"location":"reference/crds/#servicetemplatespechelm_1","title":"ServiceTemplate.spec.helm","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Helm contains the Helm chart information for the template.</p> Name Type Description Required chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartSource object            ChartSource is a source of a Helm chart representing the template. Validations:<li>has(self.localSourceRef) ? (self.localSourceRef.kind != 'Secret' &amp;&amp; self.localSourceRef.kind != 'ConfigMap'): true: Secret and ConfigMap are not supported as Helm chart sources</li><li>has(self.localSourceRef) ? !has(self.remoteSourceSpec): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.remoteSourceSpec) ? !has(self.localSourceRef): true: LocalSource and RemoteSource are mutually exclusive.</li><li>has(self.localSourceRef) || has(self.remoteSourceSpec): One of LocalSource or RemoteSource must be specified.</li> false chartSpec object            ChartSpec defines the desired state of the HelmChart to be created by the controller false"},{"location":"reference/crds/#servicetemplatespechelmchartref_1","title":"ServiceTemplate.spec.helm.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#servicetemplatespechelmchartsource_1","title":"ServiceTemplate.spec.helm.chartSource","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSource is a source of a Helm chart representing the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#servicetemplatespechelmchartsourcelocalsourceref_1","title":"ServiceTemplate.spec.helm.chartSource.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespec_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucket_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketcertsecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketproxysecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketsecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketsts_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketstscertsecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecbucketstssecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgit_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitincludeindex_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitincludeindexrepository_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitproxysecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitsecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitverify_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecgitverifysecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecoci_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecocicertsecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecocilayerselector_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociproxysecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecocisecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociverify_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociverifymatchoidcidentityindex_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespechelmchartsourceremotesourcespecociverifysecretref_1","title":"ServiceTemplate.spec.helm.chartSource.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespechelmchartspec_1","title":"ServiceTemplate.spec.helm.chartSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartSpec defines the desired state of the HelmChart to be created by the controller</p> Name Type Description Required chart string            Chart is the name or path the Helm chart is available at in the SourceRef. true interval string            Interval at which the HelmChart SourceRef is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true sourceRef object            SourceRef is the reference to the Source the chart is available at. true ignoreMissingValuesFiles boolean            IgnoreMissingValuesFiles controls whether to silently ignore missing values files rather than failing. false reconcileStrategy enum            ReconcileStrategy determines what enables the creation of a new artifact. Valid values are ('ChartVersion', 'Revision'). See the documentation of the values for an explanation on their behavior. Defaults to ChartVersion when omitted. Enum: ChartVersion, Revision Default: ChartVersion false suspend boolean            Suspend tells the controller to suspend the reconciliation of this source. false valuesFiles []string            ValuesFiles is an alternative list of values files to use as the chart values (values.yaml is not included by default), expected to be a relative path in the SourceRef. Values files are merged in the order of this list with the last file overriding the first. Ignored when omitted. false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified. false version string            Version is the chart version semver expression, ignored for charts from GitRepository and Bucket sources. Defaults to latest when omitted. Default: * false"},{"location":"reference/crds/#servicetemplatespechelmchartspecsourceref_1","title":"ServiceTemplate.spec.helm.chartSpec.sourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceRef is the reference to the Source the chart is available at.</p> Name Type Description Required kind enum            Kind of the referent, valid values are ('HelmRepository', 'GitRepository', 'Bucket'). Enum: HelmRepository, GitRepository, Bucket true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false"},{"location":"reference/crds/#servicetemplatespechelmchartspecverify_1","title":"ServiceTemplate.spec.helm.chartSpec.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. This field is only supported when using HelmRepository source with spec.type 'oci'. Chart dependencies, which are not bundled in the umbrella chart artifact, are not verified.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespechelmchartspecverifymatchoidcidentityindex_1","title":"ServiceTemplate.spec.helm.chartSpec.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespechelmchartspecverifysecretref_1","title":"ServiceTemplate.spec.helm.chartSpec.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomize_1","title":"ServiceTemplate.spec.kustomize","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Kustomize contains the Kustomize configuration for the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#servicetemplatespeckustomizelocalsourceref_1","title":"ServiceTemplate.spec.kustomize.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespec_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucket_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketcertsecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketproxysecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketsecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketsts_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketstscertsecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecbucketstssecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgit_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitincludeindex_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitincludeindexrepository_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitproxysecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitsecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitverify_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecgitverifysecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecoci_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecocicertsecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecocilayerselector_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociproxysecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecocisecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociverify_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociverifymatchoidcidentityindex_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespeckustomizeremotesourcespecociverifysecretref_1","title":"ServiceTemplate.spec.kustomize.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresources_1","title":"ServiceTemplate.spec.resources","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resources contains the resource configuration for the template.</p> Name Type Description Required deploymentType enum            DeploymentType is the type of the deployment. This field is ignored, when ResourceSpec is used as part of Helm chart configuration. Enum: Local, Remote Default: Remote true path string            Path to the directory containing the resource manifest. true localSourceRef object            LocalSourceRef is the local source of the kustomize manifest. false remoteSourceSpec object            RemoteSourceSpec is the remote source of the kustomize manifest. Validations:<li>has(self.git) ? (!has(self.bucket) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.bucket) ? (!has(self.git) &amp;&amp; !has(self.oci)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.oci) ? (!has(self.git) &amp;&amp; !has(self.bucket)) : true: Git, Bucket and OCI are mutually exclusive.</li><li>has(self.git) || has(self.bucket) || has(self.oci): One of Git, Bucket or OCI must be specified.</li> false"},{"location":"reference/crds/#servicetemplatespecresourceslocalsourceref_1","title":"ServiceTemplate.spec.resources.localSourceRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LocalSourceRef is the local source of the kustomize manifest.</p> Name Type Description Required kind enum            Kind is the kind of the local source. Enum: ConfigMap, Secret, GitRepository, Bucket, OCIRepository true name string            Name is the name of the local source. true namespace string            Namespace is the namespace of the local source. Cross-namespace references are only allowed when the Kind is one of [github.com/fluxcd/source-controller/api/v1.GitRepository], [github.com/fluxcd/source-controller/api/v1.Bucket] or [github.com/fluxcd/source-controller/api/v1beta2.OCIRepository]. If the Kind is ConfigMap or Secret, the namespace will be ignored. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespec_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>RemoteSourceSpec is the remote source of the kustomize manifest.</p> Name Type Description Required bucket object            Bucket is the definition of bucket source. Validations:<li>self.provider == 'aws' || self.provider == 'generic' || !has(self.sts): STS configuration is only supported for the 'aws' and 'generic' Bucket providers</li><li>self.provider != 'aws' || !has(self.sts) || self.sts.provider == 'aws': 'aws' is the only supported STS provider for the 'aws' Bucket provider</li><li>self.provider != 'generic' || !has(self.sts) || self.sts.provider == 'ldap': 'ldap' is the only supported STS provider for the 'generic' Bucket provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.secretRef): spec.sts.secretRef is not required for the 'aws' STS provider</li><li>!has(self.sts) || self.sts.provider != 'aws' || !has(self.sts.certSecretRef): spec.sts.certSecretRef is not required for the 'aws' STS provider</li> false git object            Git is the definition of git repository source. false oci object            OCI is the definition of OCI repository source. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucket_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Bucket is the definition of bucket source.</p> Name Type Description Required bucketName string            BucketName is the name of the object storage bucket. true endpoint string            Endpoint is the object storage address the BucketName is located at. true interval string            Interval at which the Bucket Endpoint is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `generic` provider. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP Endpoint. false prefix string            Prefix to use for server-side filtering of files in the Bucket. false provider enum            Provider of the object storage bucket. Defaults to 'generic', which expects an S3 (API) compatible object storage. Enum: generic, aws, gcp, azure Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server. false region string            Region of the Endpoint where the BucketName is located in. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the Bucket. false sts object            STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.  This field is only supported for the `aws` and `generic` providers. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this Bucket. false timeout string            Timeout for fetch operations, defaults to 60s. Default: 60s false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketcertsecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the bucket. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>generic</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketproxysecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Bucket server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketsecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the Bucket.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketsts_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.sts","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>STS specifies the required configuration to use a Security Token Service for fetching temporary credentials to authenticate in a Bucket provider.</p> <p>This field is only supported for the <code>aws</code> and <code>generic</code> providers.</p> Name Type Description Required endpoint string            Endpoint is the HTTP/S endpoint of the Security Token Service from where temporary credentials will be fetched. true provider enum            Provider of the Security Token Service. Enum: aws, ldap true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  This field is only supported for the `ldap` provider. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields `username` and `password` and is supported only for the `ldap` provider. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketstscertsecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.sts.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the STS endpoint. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>This field is only supported for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecbucketstssecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.bucket.sts.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the STS endpoint. This Secret must contain the fields <code>username</code> and <code>password</code> and is supported only for the <code>ldap</code> provider.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgit_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git is the definition of git repository source.</p> Name Type Description Required interval string            Interval at which the GitRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL specifies the Git repository URL, it can be an HTTP/S or SSH address. true ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false include []object            Include specifies a list of GitRepository resources which Artifacts should be included in the Artifact produced for this GitRepository. false provider enum            Provider used for authentication, can be 'azure', 'github', 'generic'. When not specified, defaults to 'generic'. Enum: generic, azure, github false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server. false recurseSubmodules boolean            RecurseSubmodules enables the initialization of all submodules within the GitRepository as cloned from the URL, using their default settings. false ref object            Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch. false secretRef object            SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields. false suspend boolean            Suspend tells the controller to suspend the reconciliation of this GitRepository. false timeout string            Timeout for Git operations like cloning, defaults to 60s. Default: 60s false verify object            Verification specifies the configuration to verify the Git commit signature(s). false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitincludeindex_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.include[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryInclude specifies a local reference to a GitRepository which Artifact (sub-)contents must be included, and where they should be placed.</p> Name Type Description Required repository object            GitRepositoryRef specifies the GitRepository which Artifact contents must be included. true fromPath string            FromPath specifies the path to copy contents from, defaults to the root of the Artifact. false toPath string            ToPath specifies the path to copy contents to, defaults to the name of the GitRepositoryRef. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitincludeindexrepository_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.include[index].repository","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GitRepositoryRef specifies the GitRepository which Artifact contents must be included.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitproxysecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the Git server.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Reference specifies the Git reference to resolve and monitor for changes, defaults to the 'master' branch.</p> Name Type Description Required branch string            Branch to check out, defaults to 'master' if no other field is defined. false commit string            Commit SHA to check out, takes precedence over all reference fields.  This can be combined with Branch to shallow clone the branch, in which the commit is expected to exist. false name string            Name of the reference to check out; takes precedence over Branch, Tag and SemVer.  It must be a valid Git reference: https://git-scm.com/docs/git-check-ref-format#_description Examples: \"refs/heads/main\", \"refs/tags/v0.1.0\", \"refs/pull/420/head\", \"refs/merge-requests/1/head\" false semver string            SemVer tag expression to check out, takes precedence over Tag. false tag string            Tag to check out, takes precedence over Branch. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitsecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing authentication credentials for the GitRepository. For HTTPS repositories the Secret must contain 'username' and 'password' fields for basic auth or 'bearerToken' field for token auth. For SSH repositories the Secret must contain 'identity' and 'known_hosts' fields.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitverify_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verification specifies the configuration to verify the Git commit signature(s).</p> Name Type Description Required secretRef object            SecretRef specifies the Secret containing the public keys of trusted Git authors. true mode enum            Mode specifies which Git object(s) should be verified.  The variants \"head\" and \"HEAD\" both imply the same thing, i.e. verify the commit that the HEAD of the Git repository points to. The variant \"head\" solely exists to ensure backwards compatibility. Enum: head, HEAD, Tag, TagAndHEAD Default: HEAD false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecgitverifysecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.git.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Secret containing the public keys of trusted Git authors.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecoci_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OCI is the definition of OCI repository source.</p> Name Type Description Required interval string            Interval at which the OCIRepository URL is checked for updates. This interval is approximate and may be subject to jitter to ensure efficient use of resources. true url string            URL is a reference to an OCI artifact repository hosted on a remote container registry. true certSecretRef object            CertSecretRef can be given the name of a Secret containing either or both of  - a PEM-encoded client certificate (`tls.crt`) and private key (`tls.key`); - a PEM-encoded CA certificate (`ca.crt`)  and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type `Opaque` or `kubernetes.io/tls`.  Note: Support for the `caFile`, `certFile` and `keyFile` keys have been deprecated. false ignore string            Ignore overrides the set of excluded patterns in the .sourceignore format (which is the same as .gitignore). If not provided, a default will be used, consult the documentation for your version to find out what those are. false insecure boolean            Insecure allows connecting to a non-TLS HTTP container registry. false layerSelector object            LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected. false provider enum            The provider used for authentication, can be 'aws', 'azure', 'gcp' or 'generic'. When not specified, defaults to 'generic'. Enum: generic, aws, azure, gcp Default: generic false proxySecretRef object            ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry. false ref object            The OCI reference to pull and monitor for changes, defaults to the latest tag. false secretRef object            SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson. false serviceAccountName string            ServiceAccountName is the name of the Kubernetes ServiceAccount used to authenticate the image pull if the service account has attached pull secrets. For more information: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account false suspend boolean            This flag tells the controller to suspend the reconciliation of this source. false timeout string            The timeout for remote OCI Repository operations like pulling, defaults to 60s. Default: 60s false verify object            Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecocicertsecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.certSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>CertSecretRef can be given the name of a Secret containing either or both of</p> <ul> <li>a PEM-encoded client certificate (<code>tls.crt</code>) and private key (<code>tls.key</code>);</li> <li>a PEM-encoded CA certificate (<code>ca.crt</code>)</li> </ul> <p>and whichever are supplied, will be used for connecting to the registry. The client cert and key are useful if you are authenticating with a certificate; the CA cert is useful if you are using a self-signed server certificate. The Secret must be of type <code>Opaque</code> or <code>kubernetes.io/tls</code>.</p> <p>Note: Support for the <code>caFile</code>, <code>certFile</code> and <code>keyFile</code> keys have been deprecated.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecocilayerselector_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.layerSelector","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>LayerSelector specifies which layer should be extracted from the OCI artifact. When not specified, the first layer found in the artifact is selected.</p> Name Type Description Required mediaType string            MediaType specifies the OCI media type of the layer which should be extracted from the OCI Artifact. The first layer matching this type is selected. false operation enum            Operation specifies how the selected layer should be processed. By default, the layer compressed content is extracted to storage. When the operation is set to 'copy', the layer compressed content is persisted to storage as it is. Enum: extract, copy false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociproxysecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.proxySecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ProxySecretRef specifies the Secret containing the proxy configuration to use while communicating with the container registry.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.ref","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>The OCI reference to pull and monitor for changes, defaults to the latest tag.</p> Name Type Description Required digest string            Digest is the image digest to pull, takes precedence over SemVer. The value should be in the format 'sha256:'. false semver string            SemVer is the range of tags to pull selecting the latest within the range, takes precedence over Tag. false semverFilter string            SemverFilter is a regex pattern to filter the tags within the SemVer range. false tag string            Tag is the image tag to pull, defaults to latest. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecocisecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef contains the secret name containing the registry login credentials to resolve image metadata. The secret must be of type kubernetes.io/dockerconfigjson.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociverify_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.verify","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Verify contains the secret name containing the trusted public keys used to verify the signature and specifies which provider to use to check whether OCI image is authentic.</p> Name Type Description Required provider enum            Provider specifies the technology used to sign the OCI Artifact. Enum: cosign, notation Default: cosign true matchOIDCIdentity []object            MatchOIDCIdentity specifies the identity matching criteria to use while verifying an OCI artifact which was signed using Cosign keyless signing. The artifact's identity is deemed to be verified if any of the specified matchers match against the identity. false secretRef object            SecretRef specifies the Kubernetes Secret containing the trusted public keys. false"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociverifymatchoidcidentityindex_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.verify.matchOIDCIdentity[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>OIDCIdentityMatch specifies options for verifying the certificate identity, i.e. the issuer and the subject of the certificate.</p> Name Type Description Required issuer string            Issuer specifies the regex pattern to match against to verify the OIDC issuer in the Fulcio certificate. The pattern must be a valid Go regular expression. true subject string            Subject specifies the regex pattern to match against to verify the identity subject in the Fulcio certificate. The pattern must be a valid Go regular expression. true"},{"location":"reference/crds/#servicetemplatespecresourcesremotesourcespecociverifysecretref_1","title":"ServiceTemplate.spec.resources.remoteSourceSpec.oci.verify.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SecretRef specifies the Kubernetes Secret containing the trusted public keys.</p> Name Type Description Required name string            Name of the referent. true"},{"location":"reference/crds/#servicetemplatestatus_1","title":"ServiceTemplate.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ServiceTemplateStatus defines the observed state of ServiceTemplate</p> Name Type Description Required valid boolean            Valid indicates whether the template passed validation or not. true chartRef object            ChartRef is a reference to a source controller resource containing the Helm chart representing the template. false chartVersion string            ChartVersion represents the version of the Helm Chart associated with this template. false config JSON            Config demonstrates available parameters for template customization, that can be used when creating ClusterDeployment objects. false description string            Description contains information about the template. false k8sConstraint string            Constraint describing compatible K8S versions of the cluster set in the SemVer format. false observedGeneration integer            ObservedGeneration is the last observed generation. Format: int64 false sourceStatus object            SourceStatus reflects the status of the source. false validationError string            ValidationError provides information regarding issues encountered during template validation. false"},{"location":"reference/crds/#servicetemplatestatuschartref_1","title":"ServiceTemplate.status.chartRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>ChartRef is a reference to a source controller resource containing the Helm chart representing the template.</p> Name Type Description Required kind enum            Kind of the referent. Enum: OCIRepository, HelmChart true name string            Name of the referent. true apiVersion string            APIVersion of the referent. false namespace string            Namespace of the referent, defaults to the namespace of the Kubernetes resource object that contains the reference. false"},{"location":"reference/crds/#servicetemplatestatussourcestatus_1","title":"ServiceTemplate.status.sourceStatus","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>SourceStatus reflects the status of the source.</p> Name Type Description Required kind string            Kind is the kind of the remote source. true name string            Name is the name of the remote source. true namespace string            Namespace is the namespace of the remote source. true artifact object            Artifact is the artifact that was generated from the template source. false conditions []object            Conditions reflects the conditions of the remote source object. false observedGeneration integer            ObservedGeneration is the latest source generation observed by the controller. Format: int64 false"},{"location":"reference/crds/#servicetemplatestatussourcestatusartifact_1","title":"ServiceTemplate.status.sourceStatus.artifact","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Artifact is the artifact that was generated from the template source.</p> Name Type Description Required lastUpdateTime string            LastUpdateTime is the timestamp corresponding to the last update of the Artifact. Format: date-time true path string            Path is the relative file path of the Artifact. It can be used to locate the file in the root of the Artifact storage on the local file system of the controller managing the Source. true revision string            Revision is a human-readable identifier traceable in the origin source system. It can be a Git commit SHA, Git tag, a Helm chart version, etc. true url string            URL is the HTTP address of the Artifact as exposed by the controller managing the Source. It can be used to retrieve the Artifact for consumption, e.g. by another controller applying the Artifact contents. true digest string            Digest is the digest of the file in the form of ':'. false metadata map[string]string            Metadata holds upstream information such as OCI annotations. false size integer            Size is the number of bytes in the file. Format: int64 false"},{"location":"reference/crds/#servicetemplatestatussourcestatusconditionsindex_1","title":"ServiceTemplate.status.sourceStatus.conditions[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Condition contains details for one aspect of the current state of this API Resource.</p> Name Type Description Required lastTransitionTime string            lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message string            message is a human readable message indicating details about the transition. This may be an empty string. true reason string            reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum            status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type string            type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration integer            observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false"},{"location":"reference/template/","title":"The Templates system","text":"<p>By default, Mirantis k0rdent Enterprise delivers a set of default <code>ProviderTemplate</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects:</p> <ul> <li><code>ProviderTemplate</code>    The template containing the configuration of the provider (for example, k0smotron or AWS). These are cluster-scoped.</li> <li><code>ClusterTemplate</code>    The template containing the configuration of the cluster objects. These are namespace-scoped.</li> <li><code>ServiceTemplate</code>    The template containing the configuration of the service to be installed on the cluster deployment. These are namespace-scoped.</li> </ul> <p>All Templates are immutable, so if you want to change something about a cluster that has been deployed, you have to apply a whole new template. You can also build your own templates and use them for deployment along with the templates shipped with Mirantis k0rdent Enterprise.</p>"},{"location":"reference/template/#template-naming-convention","title":"Template Naming Convention","text":"<p>The templates can have any name. However, since they are immutable, we have adopted a naming convention that includes semantic versioning in the name, as in <code>template-&lt;major&gt;-&lt;minor&gt;-&lt;patch&gt;</code>. Below are some examples for each of the templates.</p> <p>Example</p> <p>An example of a <code>ProviderTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ProviderTemplate\nmetadata:\n  name: cluster-api-1-1-0\nspec:\n  helm:\n    chartSpec:\n      chart: cluster-api\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n      version: 1.1.0\nstatus:\n  capiContracts:\n    v1alpha3: \"\"\n    v1alpha4: \"\"\n    v1beta1: \"\"\n  chartRef:\n    kind: HelmChart\n    name: cluster-api-0-0-4\n    namespace: kcm-system\n  config:\n    config: {}\n    configSecret:\n      create: false\n      name: \"\"\n      namespace: \"\"\n  description: A Helm chart for Cluster API core components\n  observedGeneration: 1\n  valid: true\n</code></pre></p> <p>Example</p> <p>An example of a <code>ClusterTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\nmetadata:\n  name: aws-standalone-cp-1-0-12\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: aws-standalone-cp\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n      version: 1.1.0\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: aws-standalone-cp-1-0-12\n    namespace: kcm-system\n  config:\n    bastion:\n      allowedCIDRBlocks: []\n      ami: \"\"\n      disableIngressRules: false\n      enabled: false\n      instanceType: t2.micro\n    clusterIdentity:\n      kind: AWSClusterStaticIdentity\n      name: \"\"\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n        - 10.244.0.0/16\n      services:\n        cidrBlocks:\n        - 10.96.0.0/12\n    controlPlane:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    controlPlaneNumber: 3\n    extensions:\n      chartRepository: \"\"\n      imageRepository: \"\"\n    k0s:\n      version: v1.31.1+k0s.1\n    publicIP: false\n    region: \"\"\n    sshKeyName: \"\"\n    worker:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    workersNumber: 2\n  description: 'An kcm template to deploy a k0s cluster on AWS with bootstrapped control\n    plane nodes. '\n  observedGeneration: 1\n  providerContracts:\n    bootstrap-k0smotron: v1beta1\n    control-plane-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n  providers:\n  - bootstrap-k0smotron\n  - control-plane-k0smotron\n  - infrastructure-aws\n  valid: true\n</code></pre></p> <p>Example</p> <p>An example of a <code>ServiceTemplate</code> with its status. <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n      version: 3.2.6\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: kyverno-3-2-6\n    namespace: kcm-system\n  description: A Helm chart to refer the official kyverno helm chart\n  observedGeneration: 1\n  valid: true\n</code></pre></p>"},{"location":"reference/template/#template-life-cycle-management","title":"Template Life Cycle Management","text":"<p>Cluster and Service Templates can be delivered to target namespaces using the <code>AccessManagement</code>, <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects. The <code>AccessManagement</code> object contains the list of access rules to apply. Each access rule contains the namespaces' definition for delivering templates into and the template chains to deliver. Each <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> contains the supported templates and the upgrade sequences for them.</p> <p>The example of <code>ClusterTemplate</code> Management:</p> <ol> <li> <p>Create a <code>ClusterTemplateChain</code> object in the system namespace (defaults to <code>kcm-system</code>). Properly configure     the list of <code>.spec.supportedTemplates[].availableUpgrades</code> for the specified <code>ClusterTemplate</code> if you want to     allow upgrading. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-2\n      availableUpgrades:\n        - name: aws-standalone-cp-1-0-12\n    - name: aws-standalone-cp-1-0-12\n</code></pre> </li> <li> <p>Edit the <code>AccessManagement</code> object and configure the <code>.spec.accessRules</code>.     For example, to apply all templates and upgrade sequences defined in the <code>aws</code> <code>ClusterTemplateChain</code> to the     <code>default</code> namespace, add the following <code>accessRule</code>:</p> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - default\n    clusterTemplateChains:\n      - aws\n</code></pre> </li> </ol> <p>The kcm controllers will deliver all the <code>ClusterTemplate</code> objects across the target namespaces. As a result, the following new objects should be created:</p> <ul> <li><code>ClusterTemplateChain</code> <code>default/aws</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-2</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-1-0-12</code> (available for the upgrade from <code>aws-standalone-cp-0-0-2</code>)</li> </ul> <p>Note</p> <ol> <li>The target <code>ClusterTemplate</code> defined as being available for the upgrade should reference the same helm chart name as the source <code>ClusterTemplate</code>. Otherwise, after the upgrade is triggered, the cluster will be removed and then recreated from scratch, even if the objects in the helm chart are the same.</li> <li>The target template should not affect immutable fields or any other incompatible internal objects upgrades, otherwise the upgrade will fail.</li> </ol> <ul> <li>Removing predefined templates</li> <li>Bring-your-own (BYO) templates</li> <li>Templates for Amazon Web Services</li> <li>Templates for Azure</li> <li>Templates for OpenStack</li> <li>Templates for vSphere</li> <li>Templates for Remote deployments using SSH</li> </ul>"},{"location":"reference/template/template-aws/","title":"AWS template parameters","text":""},{"location":"reference/template/template-aws/#aws-ami","title":"AWS AMI","text":"<p>For AWS, by default Mirantis k0rdent Enterprise looks up the AMI ID automatically, using the latest Amazon Linux 2 image.</p> <p>You can override lookup parameters to search your desired image automatically or you can use a specific AMI ID directly. If both the AMI ID and lookup parameters are defined, the AMI ID will have higher precedence.</p>"},{"location":"reference/template/template-aws/#image-lookup","title":"Image lookup","text":"<p>To configure automatic AMI lookup, Mirantis k0rdent Enterprise uses three parameters:</p> <ul> <li> <p><code>.imageLookup.format</code> - Used directly as a value for the <code>name</code> filter (see the describe-images filters). This field supports substitutions for <code>{{.BaseOS}}</code> and <code>{{.K8sVersion}}</code> with the base OS and kubernetes version, respectively.</p> </li> <li> <p><code>.imageLookup.org</code> - The AWS org ID that will be used as value for the <code>owner-id</code> filter.</p> </li> <li> <p><code>.imageLookup.baseOS</code> - The string to be used as a value for the <code>{{.BaseOS}}</code> substitution in the <code>.imageLookup.format</code> string.</p> </li> </ul>"},{"location":"reference/template/template-aws/#ami-id","title":"AMI ID","text":"<p>The AMI ID can be directly used in the <code>.amiID</code> parameter.</p>"},{"location":"reference/template/template-aws/#capa-prebuilt-amis","title":"CAPA prebuilt AMIs","text":"<p>Use <code>clusterawsadm</code> to get available AMIs to create a <code>ClusterDeployment</code>:</p> <pre><code>clusterawsadm ami list\n</code></pre> <p>For details, see Pre-built Kubernetes AMIs.</p>"},{"location":"reference/template/template-aws/#ssh-access-to-cluster-nodes","title":"SSH access to cluster nodes","text":"<p>To access nodes using SSH you'll need to do two things:</p> <ul> <li>Add an SSH key added in the region where you want to deploy the cluster</li> <li>Enable Bastion host is enabled</li> </ul>"},{"location":"reference/template/template-aws/#ssh-keys","title":"SSH keys","text":"<p>Only one SSH key is supported and it should be added in AWS prior to creating the <code>ClusterDeployment</code> object. The name of the key should then be placed under the <code>.spec.config.sshKeyName</code>.</p> <p>The same SSH key will be used for all machines and a bastion host, or jump box. The bastion host is used as a single entry point that provides access to the rest of the cluster, enabling you to more tightly control access.</p> <p>To enable the bastion, set the <code>.spec.config.bastion.enabled</code> option in the <code>ClusterDeployment</code> object to <code>true</code>.</p> <p>You can get a full list of the bastion configuration options in the CAPA docs.</p> <p>The resulting <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-standalone-cp-1-0-12\n  credential: aws-cred\n  config:\n    clusterLabels: {}\n    sshKeyName: foobar\n    bastion:\n      enabled: true\n...\n</code></pre>"},{"location":"reference/template/template-aws/#eks-templates","title":"EKS templates","text":"<p>Warning</p> <p> When deploying an EKS cluster please note that additional steps may be needed for proper VPC removal.</p> <p>Warning</p> <p> You may encounter an issue where EKS machines are not created due to the <code>ControlPlaneIsStable</code> preflight check failure during EKS cluster deployment. Please follow this  instruction to apply the workaround.</p> <p>EKS templates use parameters similar to AWS and the resulting EKS <code>ClusterDeployment</code> looks something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-eks-1-0-2\n  credential: aws-cred\n  config:\n    clusterLabels: {}\n    sshKeyName: foobar\n    region: ${AWS_REGION}\n    workersNumber: 1\n...\n</code></pre>"},{"location":"reference/template/template-aws/#non-root-volumes","title":"Non-root volumes","text":"<p>To configure options for non-root storage volumes, set the corresponding fields in the <code>ClusterDeployment</code> responsible for these settings, which depend on the template in use:</p> Template Section(s) <code>aws-standalone-cp-1-0-12</code> <code>.spec.controlPlane.nonRootVolumes</code>,<code>.spec.worker.nonRootVolumes</code> <code>aws-hosted-cp-1-0-11</code> <code>.spec.nonRootVolumes</code> <code>aws-eks-1-0-2</code> <code>.spec.worker.nonRootVolumes</code> <p>The <code>nonRootVolumes</code> field is a list of Volumes.</p>"},{"location":"reference/template/template-azure/","title":"Azure machine parameters","text":""},{"location":"reference/template/template-azure/#ssh","title":"SSH","text":"<p>The SSH public key can be passed to <code>.spec.config.sshPublicKey</code>  parameter (in the case of a hosted control plane) or <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (in the case of a standalone control plane) of the <code>ClusterDeployment</code> object.</p> <p>It should be encoded in base64 format.</p>"},{"location":"reference/template/template-azure/#vm-size","title":"VM size","text":"<p>Azure supports various VM sizes which can be retrieved with the following command:</p> <pre><code>az vm list-sizes --location \"&lt;location&gt;\" -o table\n</code></pre> <p>Then desired VM size could be passed to the:</p> <ul> <li><code>.spec.config.vmSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.vmSize</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.vmSize</code> - for worker nodes in the standalone deployment.</li> </ul> <p>Example: Standard_A4_v2</p>"},{"location":"reference/template/template-azure/#root-volume-size","title":"Root Volume size","text":"<p>Root volume size of the VM (in GB) can be changed through the following parameters:</p> <ul> <li><code>.spec.config.rootVolumeSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.rootVolumeSize</code> - for control plane nodes in the   standalone deployment.</li> <li><code>.spec.config.worker.rootVolumeSize</code> - for worker nodes in the standalone   deployment.</li> </ul> <p>Default value: 30</p> <p>Please note that this value can't be less than size of the root volume  defined in your image.</p>"},{"location":"reference/template/template-azure/#vm-image","title":"VM Image","text":"<p>You can define the image which will be used for your machine using the following parameters:</p> <p>*<code>.spec.config.image</code> - for hosted CP deployment. * <code>.spec.config.controlPlane.image</code> - for control plane nodes in the standalone   deployment. * <code>.spec.config.worker.image</code> - for worker nodes in the standalone deployment.</p> <p>There are multiple self-excluding ways to define the image source (for example Azure Compute Gallery, Azure Marketplace, and so on).</p> <p>Detailed information regarding image can be found in CAPZ documentation</p> <p>By default, the latest official CAPZ Ubuntu based image is used.</p>"},{"location":"reference/template/template-byo/","title":"Bring Your Own Templates","text":"<p>In addition to the templates that ship with Mirantis k0rdent Enterprise, it's possible to make your own. These might represent different types of clusters, or they may represent additional services to add to a cluster. Follow these steps:</p>"},{"location":"reference/template/template-byo/#create-a-source-object","title":"Create a Source Object","text":"<p>Info</p> <p> Skip this step if you're using an existing source.</p> <p><code>ClusterTemplate</code> and <code>ProviderTemplate</code> are based on a Helm chart. <code>ServiceTemplate</code>, apart from Helm chart, can be also based on kustomization or raw resources. Regardless of the type of the resources to be deployed using template, the corresponding source object should be created.</p> <p>The source can be one of the following types:</p> Template <code>HelmRepository</code> <code>Bucket</code> <code>OCIRepository</code> <code>ConfigMap</code> <code>Secret</code> <code>ClusterTemplate</code> V X X X X <code>ProviderTemplate</code> V X X X X <code>ServiceTemplate</code> V V V V V <ul> <li>HelmRepository</li> <li>GitRepository</li> <li>Bucket</li> </ul> <p>Info</p> <p><code>ConfigMap</code> and <code>Secret</code> can only be used as a source of kustomization or raw resources for <code>ServiceTemplate</code>. To deploy kustomization using <code>ConfigMap</code> or <code>Secret</code> the kustomization folder must be archived in .tar.gz and then <code>ConfigMap</code> or <code>Secret</code> must be created from resulting archive: <pre><code>kubectl create configmap foo --from-file=kustomization.tar.gz\n</code></pre> To deploy raw resources using <code>ConfigMap</code> or <code>Secret</code> source object must be created from raw resource files: <pre><code>kubectl create configmap bar --from-file=namespace.yaml --from-file=deployment.yaml\n</code></pre> Note that it's important to pay attention to where the source resides. Cluster-scoped <code>ProviderTemplate</code> objects must reside in the system* namespace (<code>kcm-system</code> by default) but other template sources must reside in the same namespace as the templates that will come from them.</p> <p>For example, this YAML describes a custom <code>Source</code> object of <code>kind</code> <code>HelmRepository</code>:</p> <p>Note</p> <p> The custom <code>HelmRepository</code> must have the label <code>k0rdent.mirantis.com/managed: \"true\"</code>.</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: k0rdent-catalog\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/k0rdent/kcm/charts\n</code></pre>"},{"location":"reference/template/template-byo/#create-the-template","title":"Create the Template","text":"<p>Once you have the source, you can create the actual template. This template can be one of three types:</p> <ul> <li><code>ClusterTemplate</code></li> <li><code>ServiceTemplate</code></li> <li><code>ProviderTemplate</code></li> </ul> <p>For <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects, configure the namespace where this template should reside (<code>metadata.namespace</code>).</p> <p>For defining the Helm chart, you have three choices. You can either:</p> <ol> <li>Specify the actual Helm chart definition in the <code>.spec.helm.chartSpec</code> field of the    HelmChartSpec kind,</li> <li>Reference an existing <code>HelmChart</code> object in <code>.spec.helm.chartRef</code>, or</li> </ol> <p>Note</p> <p> <code>spec.helm.chartSpec</code> and <code>spec.helm.chartRef</code> are mutually exclusive.</p> <p>To automatically create the <code>HelmChart</code> for the <code>Template</code>, configure the following custom helm chart parameters under <code>spec.helm.chartSpec</code>:</p> Field Description <code>sourceRef</code>LocalHelmChartSourceReference Reference to the source object (for example, <code>HelmRepository</code>, <code>GitRepository</code>, or <code>Bucket</code>) in the same namespace as the Template. <code>chart</code>string The name of the Helm chart available in the source. <code>version</code>string Version is the chart version semver expression. Defaults to latest when omitted. <code>interval</code>Kubernetes meta/v1.Duration The frequency at which the <code>sourceRef</code> is checked for updates. Defaults to 10 minutes. <p>For the complete list of the <code>HelmChart</code> parameters, see: HelmChartSpec.</p> <p>The controller automatically creates the <code>HelmChart</code> object based on the chartSpec defined in <code>.spec.helm.chartSpec</code>.</p> <p>Note</p> <p> <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects should reside in the same namespace as the <code>ClusterDeployment</code> referencing them. The <code>ClusterDeployment</code> can't reference the Template from another namespace (the creation request will be declined by the admission webhook). All <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects shipped with KCM reside in the system namespace (defaults to <code>kcm-system</code>). To get the instructions on how to distribute Templates along multiple namespaces, read Template Life Cycle Management.</p>"},{"location":"reference/template/template-byo/#alternative-template-sources","title":"Alternative Template Sources","text":"<p>Aside from Helm charts, <code>ServiceTemplate</code> also supports alternative resource definitions using either Kustomize or raw resources.</p> <p>You can use one of the following fields in <code>.spec</code> (they are mutually exclusive):</p> <ul> <li><code>.spec.kustomize</code></li> <li><code>.spec.resources</code></li> </ul> <p>Each of these fields accepts a <code>SourceSpec</code>, which defines the origin of the template content. Only one can be used at a time.</p> <p>A <code>SourceSpec</code> includes:</p> Field Description <code>deploymentType</code> Must be either <code>Local</code> or <code>Remote</code>. Defines whether resources will be deployed to management (local) or to managed (remote) cluster. <code>localSourceRef</code> Reference to a local source (e.g., <code>ConfigMap</code>, <code>Secret</code>, <code>GitRepository</code>, <code>Bucket</code>, <code>OCIRepository</code>). <code>remoteSourceSpec</code> Configuration for a remote source. Includes support for <code>Git</code>, <code>Bucket</code>, or <code>OCI</code> repositories. <code>path</code> Path within the source object pointing to the manifests or kustomize config. Ignored when deploying raw resources using <code>ConfigMap</code> or <code>Secret</code> <p>Note</p> <p> Fields <code>.spec.*.remoteSourceSpec.git</code>, <code>.spec.*.remoteSource.Spec.bucket</code> and <code>.spec.*.remoteSourceSpec.oci</code> are mutually exclusive. Example using <code>.spec.kustomize</code>: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n   name: example-kustomization\n   namespace: kcm-system\nspec:\n  kustomize:\n    deploymentType: Remote\n    remoteSourceSpec:\n      git:\n        url: https://github.com/example/repo\n        ref:\n          branch: main\n    path: ./overlays/dev\n</code></pre></p> <p>Example using <code>.spec.resources</code>: <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n   name: example-resources\n   namespace: kcm-system\nspec:\n  resources:\n    deploymentType: Local\n    localSourceRef:\n      kind: ConfigMap\n      name: my-configmap\n    path: ./manifests\n</code></pre></p> <p>All of the above follow the same mutual exclusivity and version constraint rules as Helm.</p>"},{"location":"reference/template/template-byo/#required-and-exposed-providers-definition","title":"Required and exposed providers definition","text":"<p>The <code>*Template</code> object must specify the list of Cluster API providers that are either required (for <code>ClusterTemplates</code> and <code>ServiceTemplates</code>) or exposed (for <code>ProviderTemplates</code>). These providers include <code>infrastructure</code>, <code>bootstrap</code>, and <code>control-plane</code>. This can be achieved in two ways:</p> <ol> <li>By listing the providers explicitly in the <code>spec.providers</code> field.</li> <li>Alternatively, by including specific annotations in the <code>Chart.yaml</code> of the referenced Helm chart. The annotations should list the providers as a <code>comma-separated</code> value.</li> </ol> <p>For example:</p> <p>In a <code>Template</code> spec:</p> <pre><code>spec:\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n</code></pre> <p>In a <code>Chart.yaml</code>:</p> <pre><code>annotations:\n    cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n</code></pre>"},{"location":"reference/template/template-byo/#compatibility-attributes","title":"Compatibility attributes","text":"<p>Each of the <code>*Template</code> resources has compatibility versions attributes to constraint the core <code>CAPI</code>, <code>CAPI</code> provider or Kubernetes versions. CAPI-related version constraints must be set in the <code>CAPI</code> contract format. Kubernetes version constraints must be set in the Semantic Version format. Each attribute can be set either via the corresponding <code>.spec</code> fields or via the annotations. Values set via the <code>.spec</code> have precedence over the values set via the annotations.</p> <p>Note</p> <p> All of the compatibility attributes are optional, and validation checks only take place if both of the corresponding type attributes (e.g. provider contract versions in both <code>ProviderTemplate</code> and <code>ClusterTemplate</code>) are set.</p> <ol> <li> <p>The <code>ProviderTemplate</code> resource has dedicated fields to set compatible <code>CAPI</code> contract versions along with CRDs contract versions supported by the provider. Given contract versions will then be set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the core <code>CAPI</code> contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core <code>CAPI</code>. For the core <code>CAPI</code> Template values should be empty.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ProviderTemplate\n# ...\nspec:\n  providers:\n  - infrastructure-aws\n  capiContracts:\n    # commented is the example exclusively for the core CAPI Template\n    # v1alpha3: \"\"\n    # v1alpha4: \"\"\n    # v1beta1: \"\"\n    v1alpha3: v1alpha3\n    v1alpha4: v1alpha4\n    v1beta1: v1beta1_v1beta2\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code> with the same logic as in the <code>.spec</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws\n  cluster.x-k8s.io/v1alpha3: v1alpha3\n  cluster.x-k8s.io/v1alpha4: v1alpha4\n  cluster.x-k8s.io/v1beta1: v1beta1_v1beta2\n</code></pre> </li> <li> <p>The <code>ClusterTemplate</code> resource has dedicated fields to set an exact compatible Kubernetes version in the Semantic Version format and required contract versions per each provider to match against the related <code>ProviderTemplate</code> objects. Given compatibility attributes will be then set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterTemplate\n# ...\nspec:\n  k8sVersion: 1.30.0 # only exact semantic version is applicable\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n  providerContracts:\n    bootstrap-k0sproject-k0smotron: v1beta1 # only a single contract version is applicable\n    control-plane-k0sproject-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n</code></pre> <p>Example with the <code>.annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n  cluster.x-k8s.io/bootstrap-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/control-plane-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/infrastructure-aws: v1beta2\n  k0rdent.mirantis.com/k8s-version: 1.30.0\n</code></pre> </li> <li> <p>The <code>ServiceTemplate</code> resource has dedicated fields to set an compatibility constrained Kubernetes version to match against the related <code>ClusterTemplate</code> objects. Given compatibility values will be then set accordingly in the <code>.status</code> field.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\n# ...\nspec:\n  k8sConstraint: \"^1.30.0\" # only semantic version constraints are applicable\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>k0rdent.mirantis.com/k8s-version-constraint: ^1.30.0\n</code></pre> </li> </ol>"},{"location":"reference/template/template-byo/#compatibility-attributes-enforcement","title":"Compatibility attributes enforcement","text":"<p>The aforedescribed attributes are checked for compliance with the following rules:</p> <ul> <li>Both the exact and constraint version of the same type (for example <code>k8sVersion</code> and <code>k8sConstraint</code>) must be set, otherwise no check is performed;</li> <li>If a <code>ClusterTemplate</code> object's provider's contract version does not satisfy contract versions from the related <code>ProviderTemplate</code> object, updates to the <code>ClusterDeployment</code> object will be blocked;</li> <li>If a <code>ProviderTemplate</code> object's <code>CAPI</code> contract version (for example, in a <code>v1beta1: v1beta1_v1beta2</code> key-value pair, the key <code>v1beta1</code> is the core <code>CAPI</code> contract version) is not listed in the core <code>CAPI</code> <code>ProviderTemplate</code> object, the updates to the <code>Management</code> object will be blocked;</li> <li>If a <code>ClusterTemplate</code> object's exact kubernetes version does not satisfy the kubernetes version constraint from the related <code>ServiceTemplate</code> object, the updates to the <code>ClusterDeployment</code> object will be blocked.</li> </ul>"},{"location":"reference/template/template-gcp/","title":"GCP template parameters","text":""},{"location":"reference/template/template-gcp/#common-clusterdeployment-parameters","title":"Common ClusterDeployment parameters","text":"<ul> <li><code>controlPlaneNumber</code> (number): The number of the control plane nodes (or pods for the hosted cluster). Not available for the GKE cluster template.</li> <li><code>workersNumber</code> (number): The number of the worker nodes. For GKE clusters this parameter should be divisible by the number of zones in <code>machines.nodeLocations</code>. If <code>nodeLocations</code> is not specified, must be divisible by the number of zones in this region (default: <code>3</code>).</li> <li><code>clusterNetwork.apiServerPort</code> (number): The port the API Server should bind to. Not available for the GKE cluster template.</li> <li><code>clusterNetwork.pods.cidrBlocks</code> (array): A list of CIDR blocks from which Pod networks are allocated.</li> <li><code>clusterNetwork.services.cidrBlocks</code> (array): A list of CIDR blocks from which service VIPs are allocated.</li> <li><code>clusterLabels</code> (object): Labels to apply to the <code>Cluster</code> object.</li> <li><code>clusterAnnotations</code> (object): Annotations to apply to the <code>Cluster</code> object.</li> </ul>"},{"location":"reference/template/template-gcp/#gcp-cluster-parameters","title":"GCP Cluster parameters","text":"<p>The following parameters are available for standalone and hosted cluster templates:</p> <ul> <li><code>project</code> (string): The name of the project in which to deploy the cluster.</li> <li><code>region</code> (string): The GCP Region in which the cluster lives.</li> <li><code>network.name</code> (string): The name of an existing GCP network or a new network to be created by Cluster API Provider GCP.</li> <li><code>network.mtu</code> (number): Maximum Transmission Unit in bytes.</li> <li><code>additionalLabels</code> (object): Additional set of labels to add to all the GCP resources.</li> </ul>"},{"location":"reference/template/template-gcp/#gcp-machines-parameters","title":"GCP Machines parameters","text":"<p>The following parameters are available for <code>controlPlane</code> (for standalone cluster template) and <code>worker</code> (for standalone and hosted cluster templates) machines (for example, <code>controlPlane.instanceType</code>):</p> <ul> <li><code>instanceType</code> (string): The type of instance to create.</li> <li><code>subnet</code> (string): A reference to the subnetwork to use for this instance.</li> <li><code>providerID</code> (string): The unique identifier as specified by the cloud provider.</li> <li><code>imageFamily</code> (string): The full reference to a valid image family to be used for this machine.</li> <li><code>image</code> (string): The full reference to a valid image to be used for this machine. Takes precedence over imageFamily.</li> <li><code>additionalLabels</code> (object): Additional set of labels to add to an instance.</li> <li><code>publicIP</code> (boolean): Specifies whether the instance should get a public IP.</li> <li><code>additionalNetworkTags</code> (array): A list of network tags that should be applied to the instance.</li> <li><code>rootDeviceSize</code> (number): The size of the root volume in GB.</li> <li><code>rootDeviceType</code> (string): The type of the root volume. One of: <code>pd-standard</code>,<code>pd-ssd</code>,<code>pd-balanced</code>,<code>hyperdisk-balanced</code>.</li> <li><code>serviceAccount.email</code> (string): Email address of the service account.</li> <li><code>serviceAccount.scopes</code> (array): The list of scopes to be made available for this service account.</li> <li><code>ipForwarding</code> (string): Allows this instance to send and receive packets with non-matching destination or source IPs. One of: <code>Enabled</code>,<code>Disabled</code>.</li> </ul>"},{"location":"reference/template/template-gcp/#k0s-parameters","title":"K0s Parameters","text":"<ul> <li><code>k0s.version</code> (string): K0s version.</li> <li><code>k0s.api.extraArgs</code> (object): Map of key-values (strings) for any extra arguments to pass down to the Kubernetes API server process.</li> </ul>"},{"location":"reference/template/template-gcp/#k0smotron-parameters","title":"K0smotron Parameters","text":"<p>Available for the hosted cluster template only.</p> <ul> <li><code>k0smotron.service.type</code> (string): An ingress method for a service. One of: <code>ClusterIP</code>, <code>NodePort</code>, <code>LoadBalancer</code>. Defaults to: <code>LoadBalancer</code>.</li> <li><code>k0smotron.service.apiPort</code> (number): The Kubernetes API port. If empty, K0smotron will pick it automatically.</li> <li><code>k0smotron.service.konnectivityPort</code> (number): The Konnectivity port. If empty, K0smotron will pick it automatically.</li> </ul>"},{"location":"reference/template/template-gcp/#extensions-parameters","title":"Extensions parameters","text":"<ul> <li><code>extensions.chartRepository</code> (string): Custom Helm repository.</li> <li><code>extensions.imageRepository</code> (string): Custom images\u2019 repository.</li> </ul>"},{"location":"reference/template/template-gcp/#gke-template-parameters","title":"GKE template parameters","text":""},{"location":"reference/template/template-gcp/#gke-cluster-parameters","title":"GKE Cluster Parameters","text":"<ul> <li><code>gkeClusterName</code> (string): The name of the GKE cluster. If unspecified, a default name is created based on the namespace and managed control plane name.</li> <li><code>project</code> (string): The name of the GCP project where the cluster is deployed.</li> <li><code>enableAutopilot</code> (boolean): Indicates whether to enable Autopilot for this GKE cluster.</li> <li><code>releaseChannel</code> (string): The release channel of the GKE cluster.</li> <li><code>controlPlaneVersion</code> (string): The control plane version of the GKE cluster. If unspecified, the default version supported by GKE is used.</li> <li><code>masterAuthorizedNetworksConfig</code> (object): Configuration options for the master authorized networks feature. If unspecified, the feature is disabled.</li> <li><code>region</code> (string): The GCP region where the cluster is deployed.</li> <li><code>location</code> (string): The location where the GKE cluster is created. If unspecified, the cluster is regional; otherwise, specifying a location creates a zonal cluster.</li> <li><code>network.name</code> (string): The GCP network name.</li> <li><code>network.mtu</code> (number): Maximum Transmission Unit (MTU) in bytes.</li> </ul>"},{"location":"reference/template/template-gcp/#gke-managed-machines-parameters","title":"GKE Managed Machines Parameters","text":"<ul> <li><code>machines.nodePoolName</code> (string): The name of the GKE node pool. If unspecified, a default name is created based on the namespace and managed machine pool name.</li> <li><code>machines.machineType</code> (string): The name of a Google Compute Engine machine type. Defaults to <code>e2-medium</code>.</li> <li><code>machines.diskSizeGB</code> (number): The size of the disk attached to each node (in GB). The smallest allowed disk size is 10GB. If unspecified, the default disk size is 100GB.</li> <li><code>machines.localSsdCount</code> (number,null): The number of local SSD disks attached to the node.</li> <li><code>machines.scaling.enableAutoscaling</code> (boolean): Indicates if autoscaling is enabled for this node pool. Defaults to true.</li> <li><code>machines.scaling.minCount</code> (number,null): The minimum number of nodes in the node pool.</li> <li><code>machines.scaling.maxCount</code> (number,null): The maximum number of nodes in the node pool.</li> <li><code>machines.scaling.locationPolicy</code> (string): Location policy used when scaling up a node pool.</li> <li><code>machines.nodeLocations</code> (array): The list of zones where the node pool\u2019s nodes are located.</li> <li><code>machines.imageType</code> (string): The image type used for this node pool.</li> <li><code>machines.instanceType</code> (string): The Compute Engine machine type.</li> <li><code>machines.diskType</code> (string): The type of disk attached to each node.</li> <li><code>machines.maxPodsPerNode</code> (number,null): The maximum number of pods allowed per node.</li> <li><code>machines.kubernetesLabels</code> (object): Labels applied to the nodes of the node pool.</li> <li><code>machines.kubernetesTaints</code> (array): Taints applied to the nodes of the node pool.</li> <li><code>machines.additionalLabels</code> (object): Additional labels added to GCP resources managed by the provider.</li> <li><code>machines.management.autoUpgrade</code> (boolean): Specifies if node auto-upgrade is enabled. Defaul</li> <li><code>machines.management.autoRepair</code> (boolean): Specifies if node auto-repair is enabled.</li> </ul> <p>The resulting GCP standalone <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: gcp-standalone-cp-1-0-12\n  credential: gcp-cred\n  config:\n    clusterLabels:\n      foo: bar\n    project: dev\n    region: us-east4\n    network:\n      name: default\n    controlPlane:\n      instanceType: n1-standard-2\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n      rootDeviceSize: 100\n      ipForwarding: Enabled\n    controlPlaneNumber: 1\n    worker:\n      instanceType: n1-standard-2\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n      rootDeviceSize: 50\n      ipForwarding: Enabled\n    workersNumber: 1\n...\n</code></pre> <p>The resulting GCP hosted <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: gcp-hosted-cp-1-0-12\n  credential: gcp-cred\n  config:\n    clusterAnnotations:\n      foo: bar\n    project: dev\n    region: us-east4\n    network:\n      name: default\n    controlPlaneNumber: 3\n    worker:\n      instanceType: n1-standard-2\n      image: projects/ubuntu-os-cloud/global/images/ubuntu-2004-focal-v20250213\n      publicIP: true\n      rootDeviceSize: 50\n      ipForwarding: Enabled\n    workersNumber: 2\n...\n</code></pre> <p>The resulting GCP GKE <code>ClusterDeployment</code> might look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: gcp-gke-1-0-3\n  credential: gcp-cred\n  propagateCredentials: false\n  config:\n    clusterLabels:\n      env: dev\n    gkeClusterName: \"dev-cluster-1\"\n    workersNumber: 3 # Should be divisible by the number of zones in `machines.nodeLocations`. If `machines.nodeLocations` is not specified, must be divisible by the number of zones in this region (default: 3)\n    project: dev\n    region: us-east4\n    network:\n      name: default\n    machines:\n      diskSizeGB: 120\n      scaling:\n        enableAutoscaling: true\n        minCount: 5\n        maxCount: 25\n      maxPodsPerNode: 250\n...\n</code></pre>"},{"location":"reference/template/template-openstack/","title":"OpenStack Machine parameters","text":""},{"location":"reference/template/template-openstack/#clusterdeployment-parameters","title":"ClusterDeployment Parameters","text":"<p>To deploy an OpenStack cluster, the following are the primary parameters in the <code>ClusterDeployment</code> resource:</p> Parameter Example Description .spec.credential <code>openstack-cluster-identity-cred</code> Reference to the Credential object. .spec.template <code>openstack-standalone-cp-1-0-11</code> Reference to the ClusterTemplate. .spec.config.authURL <code>https://keystone.yourorg.net/</code> Keystone authentication endpoint for OpenStack. .spec.config.controlPlaneNumber <code>3</code> Number of control plane nodes. .spec.config.workersNumber <code>2</code> Number of worker nodes. .spec.config.clusterLabels(optional) <code>k0rdent: demo</code> Labels to apply to the cluster. Used by MultiClusterService. .spec.config.ccmRegional(optional) <code>true</code> Enables the OpenStack CCM OS_CCM_REGIONAL envvar feature and allows OpenStack CCM to define the region in nodes."},{"location":"reference/template/template-openstack/#ssh-configuration","title":"SSH Configuration","text":"<p>To access deployed machines over ssh requires two things:</p> <ul> <li><code>sshKeyName</code> - the reference name for an existing SSH key configured in OpenStack.</li> <li><code>bastion</code> - bastion host being enabled and its flavor and image specified.</li> </ul>"},{"location":"reference/template/template-openstack/#ssh-keys","title":"SSH keys","text":"<p>Specify the SSH public key using the <code>.spec.config.controlPlane.sshKeyName</code> and <code>.spec.config.worker.sshKeyName</code> parameters (for the standalone control plane) or <code>spec.config.sshKeyName</code> parameter (for the hosted control plane).</p>"},{"location":"reference/template/template-openstack/#bastion","title":"Bastion","text":"<p>Specify <code>.spec.config.bastion.enabled</code> to enable it as well as provide <code>sshKeyName</code>, <code>flavor</code> and <code>image</code> in <code>.spec.config.bastion.spec</code>, similarly to workers and control plane.</p> <p>Example `ClusterDeployment with enabled bastion can be found below.</p>"},{"location":"reference/template/template-openstack/#machine-configuration","title":"Machine Configuration","text":"<p>Configurations for control plane and worker nodes are specified separately under <code>.spec.config.controlPlane</code> and <code>.spec.config.worker</code> for standalone control plane or under <code>spec.config</code> for hosted control plane.</p> Parameter Example Description <code>flavor</code> <code>m1.medium</code> OpenStack flavor for the instance. <code>image.filter.name</code> <code>ubuntu-22.04-x86_64</code> Name of the image. <code>sshKeyName</code> <code>ramesses-pk</code> Reference name for an existing SSH key. <code>securityGroups[].filter.name</code> <code>default</code> Security group for the instance. <p>Note</p> <p> Make sure <code>.spec.credential</code> references the <code>Credential</code> object. The recommended minimum vCPU value for the control plane flavor is 2, while for the worker node flavor, it is 1. For detailed information, refer to the machine-flavor CAPI docs.</p>"},{"location":"reference/template/template-openstack/#external-network-configuration","title":"External Network Configuration","text":"<p>If your OpenStack cloud contains more than one network marked as external it is necessary to provide which one clusterapi should use when creating a cluster. You do this by providing <code>.spec.config.externlNetwork.filter.name</code> value with the name of your external network.</p>"},{"location":"reference/template/template-openstack/#load-balancer-configuration","title":"Load Balancer Configuration","text":"<p>If your user doesn't have access to or your cloud doesn't utilize octavia load balancer it is possible to disable usage of it by specifying <code>.spec.config.apiServerLoadBalancer.enabled</code> as <code>false</code> (for standalone control plane only).</p> <p>Warning</p> <p>Disabling loadbalancer blocks usage of <code>LoadBalancer</code> type services in cluster until one is manually installed.</p>"},{"location":"reference/template/template-openstack/#configuring-some-of-k0s-parameters","title":"Configuring some of k0s parameters","text":"<ul> <li><code>k0s.arch</code> (string): Defines K0s Arch in its download URL. Available if global.k0sURL   is set. Possible values: <code>\"amd64\"</code> (default).</li> <li><code>k0s.cpArgs</code> (array of strings): A list of extra arguments to be passed to k0s controller. For standalone control plane only.   See: https://docs.k0sproject.io/stable/cli/k0s_controller.</li> <li><code>k0s.workerArgs</code> (array of strings): A list of extra arguments for configuring the k0s worker node. See: https://docs.k0sproject.io/stable/cli/k0s_worker.</li> </ul>"},{"location":"reference/template/template-openstack/#hosted-clusterdeployment-parameters","title":"Hosted ClusterDeployment parameters","text":""},{"location":"reference/template/template-openstack/#network-configuration","title":"Network configuration","text":"<p>The following parameters under <code>spec.config</code> are specific to the hosted cluster deployment:</p> <ul> <li><code>network.filter</code> (object): Specifies a query to select an OpenStack network. The value of NetworkFilter type. Required.</li> </ul> <p>Example:</p> <pre><code>```yaml\n  network:\n    filter:\n      name: my-network-name\n```\n</code></pre> <ul> <li><code>subnets[].filter</code> (array): Specifies a query to select an OpenStack subnet. The value of SubnetFilter type. Required.</li> </ul> <p>Example:</p> <pre><code>```yaml\n  subnets:\n  - filter:\n      name: my-subnet-name\n```\n</code></pre> <ul> <li><code>router.filter</code> (object): Specifies a query to select an OpenStack router. The value of RouterFilter type. Required.</li> </ul> <p>Example:</p> <pre><code>```yaml\n  router:\n    filter:\n      name: my-router-name\n```\n</code></pre> <ul> <li><code>ports[].network.filter</code> (object): Specifies a query to select an OpenStack network. The value of NetworkFilter type. Required.</li> </ul> <p>Example:</p> <pre><code>```yaml\n  ports:\n  - network:\n      filter:\n        name: my-network-name\n```\n</code></pre> <ul> <li><code>managedSecurityGroups</code> (object): Defines the desired state of security groups and rules for the cluster. When not   defined, security groups will not be created. The value of ManagedSecurityGroups type.</li> </ul> <p>Example:     <pre><code>  managedSecurityGroups:\n    allowAllInClusterTraffic: false\n</code></pre></p>"},{"location":"reference/template/template-openstack/#k0smotron-parameters","title":"K0smotron Parameters","text":"<p>Available for the hosted cluster template only.</p> <ul> <li><code>k0smotron.service.type</code> (string): An ingress method for a service. One of: <code>ClusterIP</code>, <code>NodePort</code>, <code>LoadBalancer</code>. Defaults to: <code>LoadBalancer</code>.</li> <li><code>k0smotron.service.apiPort</code> (number): The Kubernetes API port. If empty, K0smotron will pick it automatically.</li> <li><code>k0smotron.service.konnectivityPort</code> (number): The Konnectivity port. If empty, K0smotron will pick it automatically.</li> <li><code>k0smotron.controllerPlaneFlags</code> (array of strings): The <code>controllerPlaneFlags</code> parameter enables you to configure additional flags for the k0s control plane   and to override existing flags. The default flags are kept unless they are explicitly overriden. Flags with arguments must be specified as a single   string, such as <code>--some-flag=argument</code>.</li> </ul>"},{"location":"reference/template/template-openstack/#example-clusterdeployment","title":"Example ClusterDeployment","text":"<p>The standalone <code>ClusterDeployment</code> may look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-1-0-11\n  credential: openstack-cluster-identity-cred\n  config:\n    clusterLabels:\n      k0rdent: demo\n    controlPlaneNumber: 1\n    workersNumber: 1\n    bastion:\n      enabled: true\n      spec:\n        sshKeyName: my-public-key\n        flavor: m1.small\n        image:\n          filter:\n            name: ubuntu-22.04-x86_64\n    controlPlane:\n      sshKeyName: bastion-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      sshKeyName: bastion-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    externalNetwork:\n      filter:\n        name: \"public\"\n    authURL: https://my-keystone-openstack-url.com\n    identityRef:\n      name: openstack-cloud-config\n      cloudName: openstack\n      region: RegionOne\n</code></pre> <p>The hosted <code>ClusterDeployment</code> may look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-hosted-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-hosted-cp-1-0-2\n  credential: openstack-cluster-identity-cred\n  config:\n    clusterLabels:\n      k0rdent: demo\n    workersNumber: 3\n    bastion:\n      enabled: true\n      spec:\n        sshKeyName: my-ssh-key\n        flavor: m1.medium\n        image:\n          filter:\n            name: ubuntu-22.04-x86_64\n    sshKeyName: my-ssh-key\n    flavor: m1.medium\n    image:\n      filter:\n        name: ubuntu-22.04-x86_64\n    externalNetwork:\n      filter:\n        name: \"public\"\n    identityRef:\n      name: \"openstack-cloud-config\"\n      cloudName: \"openstack\"\n      region: RegionOne\n\n    managedSecurityGroups:\n      allowAllInClusterTraffic: false\n    network:\n      filter:\n        name: my-network-name\n    router:\n      filter:\n        name: my-router-name\n    subnets:\n    - filter:\n        name: my-subnet-name\n    ports:\n    - network:\n        filter:\n          name: my-network-name\n    securityGroups:\n    - filter:\n        name: my-security-group-name\n</code></pre>"},{"location":"reference/template/template-predefined/","title":"Removing predefined templates","text":""},{"location":"reference/template/template-predefined/#remove-templates-shipped-with-mirantis-k0rdent-enterprise","title":"Remove Templates shipped with Mirantis k0rdent Enterprise","text":"<p>If you need to limit the templates that exist in your Mirantis k0rdent Enterprise installation, follow the instructions below:</p> <ol> <li> <p>Get the list of <code>ProviderTemplate</code>, <code>ClusterTemplate</code> or <code>ServiceTemplate</code> objects shipped with Mirantis k0rdent Enterprise. For example, for <code>ClusterTemplate</code> objects, run:</p> <p><pre><code>kubectl get clustertemplates -n kcm-system -l helm.toolkit.fluxcd.io/name=kcm-templates\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-1-0           true\nkcm-system   aws-eks-1-0-2                   true\nkcm-system   aws-hosted-cp-1-0-11             true\nkcm-system   aws-standalone-cp-1-0-12         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-13           true\nkcm-system   azure-standalone-cp-1-0-13       true\nkcm-system   openstack-standalone-cp-1-0-11   true\nkcm-system   vsphere-hosted-cp-1-0-10         true\nkcm-system   vsphere-standalone-cp-1-0-11     true\n</code></pre></p> </li> <li> <p>Remove the template from the list using <code>kubectl delete</code>, as in:</p> <pre><code>kubectl delete clustertemplate -n kcm-system &lt;template-name&gt;\n</code></pre> </li> </ol>"},{"location":"reference/template/template-remote/","title":"Remote machine parameters","text":""},{"location":"reference/template/template-remote/#clusterdeployment-parameters","title":"ClusterDeployment Parameters","text":"<p>To deploy a cluster using Mirantis k0rdent Enterprise on any SSH accessible Linux host, configure the following parameters:</p>"},{"location":"reference/template/template-remote/#cluster-parameters","title":"Cluster parameters","text":"<ul> <li><code>controlPlaneNumber</code> (number): Specifies how many control plane pods you want to deploy. For example, 3 means three control plane pods will be set up.</li> <li><code>clusterLabels</code> (map[string]string): A dictionary of labels that can be applied to the cluster for better management and filtering. Example: <code>{\"environment\": \"production\"}</code>.</li> <li><code>clusterAnnotations</code> (map[string]string): A dictionary for annotations, which are often used for non-identifying information. Example: <code>{\"team\": \"devops\"}</code>.</li> <li><code>clusterNetwork.pods.cidrBlocks</code> (array of strings): The IP address range allocated for Pods in the cluster. Example: <code>[\"10.244.0.0/16\"]</code>.</li> <li><code>clusterNetwork.services.cidrBlocks</code> (array of strings): The IP address range allocated for Services. Example: <code>[\"10.96.0.0/12\"]</code>.</li> </ul>"},{"location":"reference/template/template-remote/#machines-parameters","title":"Machines Parameters","text":"<ul> <li><code>machines[].address</code> (string): The IP address of the remote machine. Example: <code>\"10.130.0.237\"</code>.</li> <li><code>machines[].port</code> (number): The SSH port of the remote machine. Default is 22. Example: <code>22</code>.</li> <li><code>machines[].user</code> (string): The user name for SSH login. Default is root. Example: <code>\"root\"</code>.</li> <li><code>machines[].useSudo</code> (boolean): Whether or not to use sudo for running commands on the remote machine. Example: <code>false</code>.</li> <li><code>machines[].provisionJob.scpCommand</code> (string): The command to use for copying files to remote machines. Example: <code>\"scp\"</code>.</li> <li><code>machines[].provisionJob.sshCommand</code> (string): The command to use for connecting to remote machines. Example: <code>\"ssh\"</code>.</li> <li><code>machines[].provisionJob.jobSpecTemplate.metadata</code>: Kubernetes metadata for the provisioning job, such as labels or annotations. See: https://docs.k0smotron.io/stable/resource-reference/#remotemachinespecprovisionjobjobspectemplatemetadata for more information.</li> <li><code>machines[].provisionJob.jobSpecTemplate.spec</code>: Specification for the provisioning job, detailing the job\u2019s behavior and configuration. See: https://docs.k0smotron.io/stable/resource-reference/#remotemachinespecprovisionjobjobspectemplatespec for more information.</li> <li><code>machines[].k0s.args</code> (array of strings): A list of extra arguments for configuring the k0s worker node. Example: <code>[\"--extra-arg\"]</code>.</li> </ul>"},{"location":"reference/template/template-remote/#k0smotron-parameters","title":"K0smotron Parameters","text":"<ul> <li><code>k0smotron.controllerPlaneFlags</code> (array of strings): The <code>controllerPlaneFlags</code> parameter enables you to configure additional flags for the k0s control plane and to override existing flags. The default flags are kept unless they are explicitly overriden. Flags with arguments must be specified as a single string, such as <code>--some-flag=argument</code>.</li> <li><code>k0smotron.persistence.type</code> (string): This parameter defines the persistence type for the control plane\u2019s state. Example: <code>\"EmptyDir\"</code>. See https://docs.k0smotron.io/stable/configuration/#persistence for more information.</li> <li><code>k0smotron.service.type</code> (string): This parameter defines the type of service for the Kubernetes API server: <code>\"ClusterIP\"</code>, <code>\"NodePort\"</code>, or <code>\"LoadBalancer\"</code>.</li> <li><code>k0smotron.service.apiPort</code> (number): This parameter defines the port for accessing the Kubernetes API server. Example: <code>30443</code>.</li> <li><code>k0smotron.service.konnectivityPort</code> (number): This parameter indicates the port for the Konnectivity service. Example: <code>30132</code>.</li> </ul>"},{"location":"reference/template/template-remote/#k0s-parameters","title":"K0s Parameters","text":"<ul> <li><code>k0s.version</code> (string): Specifies the version of the k0s Kubernetes distribution. Example: <code>\"v1.32.2+k0s.0\"</code>.</li> <li><code>k0s.api.extraArgs</code>: Additional arguments to pass to the Kubernetes API server. Example: <code>{\"--some-arg\": \"value\"}</code>.</li> <li><code>k0s.network</code>: Network settings for the k0s cluster. Example: <code>{\"dns\": \"8.8.8.8\"}</code>. </li> <li><code>k0s.extensions.helm.repositories</code> (array of objects): Helm repositories to add during the cluster setup. Example: <code>[{ \"name\": \"repo1\", \"url\": \"https://charts.repo1.com\" }]</code>.</li> <li><code>k0s.extensions.helm.charts</code> (array of objects): Helm charts to be installed during bootstrap. Example: <code>[{ \"name\": \"chart1\", \"namespace\": \"kube-system\", \"chartname\": \"repo1/chart1\" }]</code>.</li> </ul>"},{"location":"reference/template/template-remote/#example-clusterdeployment","title":"Example ClusterDeployment","text":"<pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-remote-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: remote-cluster-1-0-9\n  credential: remote-cred\n  propagateCredentials: false\n  config:\n    controlPlaneNumber: 1\n    clusterLabels:\n      k0rdent: demo\n    clusterAnnotations:\n      k0rdent: demo\n    k0smotron:\n      service:\n        type: LoadBalancer\n    machines:\n      - address: 10.130.0.237\n        user: root\n        port: 22\n        k0s:\n          args:\n            - --debug\n      - address: 10.130.0.174\n        user: root\n        port: 22\n    k0s:\n      version: v1.31.5+k0s.0\n      extension:\n        helm:\n          repositories:\n            - name: custom-repo\n              url: https://custom-repo-url\n          charts:\n            - name: custom-chart\n              namespace: kube-system\n              chartname: custom-repo/custom-chart\n              version: \"0.0.8\"\n              values: |\n                customKey: customValue\n</code></pre>"},{"location":"reference/template/template-vsphere/","title":"vSphere cluster template parameters","text":""},{"location":"reference/template/template-vsphere/#clusterdeployment-parameters","title":"ClusterDeployment parameters","text":"<p>To create a cluster deployment a number of parameters should be passed to the <code>ClusterDeployment</code> object.</p>"},{"location":"reference/template/template-vsphere/#parameter-list","title":"Parameter list","text":"<p>The following is the list of vSphere specific parameters that are required for successful cluster creation.</p> Parameter Example Description <code>.spec.config.vsphere.server</code> <code>vcenter.example.com</code> Address of the vSphere instance <code>.spec.config.vsphere.thumbprint</code> <code>\"00:00:00:...\"</code> Certificate thumbprint <code>.spec.config.vsphere.datacenter</code> <code>DC</code> Datacenter name <code>.spec.config.vsphere.datastore</code> <code>/DC/datastore/DS</code> Datastore path <code>.spec.config.vsphere.resourcePool</code> <code>/DC/host/vCluster/Resources/ResPool</code> Resource pool path <code>.spec.config.vsphere.folder</code> <code>/DC/vm/example</code> Folder path <code>.spec.config.controlPlane.network</code> <code>/DC/network/vm_net</code> Network path for <code>controlPlane</code> <code>.spec.config.worker.network</code> <code>/DC/network/vm_net</code> Network path for <code>worker</code> <code>.spec.config.*.ssh.publicKey</code> <code>\"ssh-ed25519 AAAA...\"</code> SSH public key in <code>authorized_keys</code> format <code>.spec.config.*.vmTemplate</code> <code>/DC/vm/templates/ubuntu</code> VM template image path <code>.spec.config.controlPlaneEndpointIP</code> <code>172.16.0.10</code> <code>kube-vip</code> vIP which will be created for control plane endpoint <p>To get the vSphere certificate thumbprint you can use the following command:</p> <pre><code>curl -sw %{certs} https://vcenter.example.com | openssl x509 -sha256 -fingerprint -noout | awk -F '=' '{print $2}'\n</code></pre> <p><code>govc</code>, a vSphere CLI, can also help to discover proper values for some of the parameters:</p> <pre><code># vsphere.datacenter\ngovc ls\n\n# vsphere.datastore\ngovc ls /*/datastore/*\n\n# vsphere.resourcePool\ngovc ls /*/host/*/Resources/*\n\n# vsphere.folder\ngovc ls -l /*/vm/**\n\n# controlPlane.network, worker.network\ngovc ls /*/network/*\n\n# *.vmTemplate\ngovc vm.info -t '*'\n</code></pre> <p>Note</p> <p> Follow official <code>govc</code> installation instructions from here. The <code>govc</code> usage guide is here.</p> <p>Minimal <code>govc</code> configuration requires setting: <code>GOVC_URL</code>, <code>GOVC_USERNAME</code>, <code>GOVC_PASSWORD</code> environment variables.</p>"},{"location":"reference/template/template-vsphere/#example-of-a-clusterdeployment-cr","title":"Example of a ClusterDeployment CR","text":"<p>With all above parameters provided your <code>ClusterDeployment</code> can look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-standalone-cp-1-0-11\n  credential: vsphere-credential\n  config:\n    clusterLabels: {}\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"&lt;VSPHERE_SERVER&gt;\"\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n</code></pre> <p>Don't forget to replace placeholder values such as <code>VSPHERE_SERVER</code> with actual values for your environment.</p>"},{"location":"reference/template/template-vsphere/#ssh","title":"SSH","text":"<p>Currently SSH configuration on vSphere expects that the user is already created before template creation. Because of that you must pass the username along with the SSH public key to configure SSH access.</p> <p>The SSH public key can be passed to <code>.spec.config.ssh.publicKey</code> (in the case of a hosted control plane) or <code>.spec.config.controlPlane.ssh.publicKey</code> and <code>.spec.config.worker.ssh.publicKey</code> (in the case of a standalone control) of the <code>ClusterDeployment</code> object.</p> <p>The SSH public key must be passed literally as a string.</p> <p>You can pass the username to <code>.spec.config.controlPlane.ssh.user</code>, <code>.spec.config.worker.ssh.user</code> or <code>.spec.config.ssh.user</code>, depending on you deployment model.</p>"},{"location":"reference/template/template-vsphere/#vm-resources","title":"VM resources","text":"<p>The following parameters are used to define VM resources:</p> Parameter Example Description <code>.rootVolumeSize</code> <code>50</code> Root volume size in GB (can't be less than the one defined in the image) <code>.cpus</code> <code>2</code> Number of CPUs <code>.memory</code> <code>4096</code> Memory size in MB <p>The resource parameters are the same for hosted and standalone CP deployments, but they are positioned differently in the spec, which means that they're going to:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"reference/template/template-vsphere/#vm-image-and-network","title":"VM Image and network","text":"<p>To provide image template path and network path the following parameters must be used:</p> Parameter Example Description <code>.vmTemplate</code> <code>/DC/vm/template</code> Image template path <code>.network</code> <code>/DC/network/Net</code> Network path <p>As with resource parameters the position of these parameters in the <code>ClusterDeployment</code> depends on deployment type and these parameters are used in:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"release-notes/","title":"Mirantis k0rdent Enterprise Release Notes","text":"<p>Mirantis k0rdent Enterprise provides release notes for each official release.</p> <ul> <li>Mirantis k0rdent Enterprise v1.0.0</li> <li>Mirantis k0rdent Enterprise v1.1.0</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/","title":"Mirantis k0rdent Enterprise v1.0.0 Release Notes","text":"<p>Released: June 17, 2025</p> <p>Mirantis k0rdent Enterprise is the enterprise-grade, commercially supported version of the k0rdent platform. It delivers a comprehensive, composable control plane for managing Kubernetes clusters, services, and observability across public clouds, private data centers, bare metal, and edge environments with enhanced security, support, and enterprise features.</p>"},{"location":"release-notes/release-notes-v1.0.0/#component-breakdown","title":"Component Breakdown","text":"<ul> <li> <p>KCM (k0rdent Cluster Management)     Manages provisioning, upgrade, scaling, and lifecycle of Kubernetes clusters via Cluster API with enhanced bare metal and airgapped support.</p> </li> <li> <p>KSM (k0rdent State Management)     Uses declarative, templated <code>ServiceTemplate</code> objects to manage consistent deployment of services like Istio, Flux, and cert-manager.</p> </li> <li> <p>KOF (k0rdent Observability &amp; FinOps)     Provides comprehensive metrics, logs, dashboards, and cost visibility using VictoriaMetrics, OpenCost integration.</p> </li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#major-highlights","title":"Major Highlights","text":"<p>This initial release of Mirantis k0rdent Enterprise includes:</p> <ul> <li>Fully stabilized v1beta1 APIs across components</li> <li>Production-grade multi-cluster support</li> <li>Bare metal deployment capabilities</li> <li>Enterprise-ready service templating and orchestration</li> <li>Integrated observability stack with AI/ML workload support</li> <li>Airgapped installation and deployment capabilities</li> <li>Commercial support and enterprise-grade security features</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#core-features","title":"Core Features","text":""},{"location":"release-notes/release-notes-v1.0.0/#cluster-lifecycle-management-kcm","title":"Cluster Lifecycle Management (KCM)","text":"<ul> <li> <p>Multi-Cloud Cluster Provisioning     Declarative cluster creation and management across AWS, Azure, GCP, vSphere, OpenStack, and bare metal environments using Cluster API</p> </li> <li> <p>Cluster Templates     Reusable, parameterized templates for consistent cluster provisioning with customizable node pools, networking, and security configurations</p> </li> <li> <p>Automated Cluster Scaling     Dynamic horizontal and vertical scaling of worker nodes based on resource demands and policies</p> </li> <li> <p>Cluster Upgrades &amp; Maintenance     Rolling updates of Kubernetes versions with automated health checks and rollback capabilities</p> </li> <li> <p>Bare Metal Cluster Deployment     Native support for provisioning and managing Kubernetes clusters on bare metal infrastructure with automated node discovery and configuration</p> </li> <li> <p>Airgapped Installation     Complete offline installation capabilities with pre-packaged container images and Helm charts for secure, disconnected environments</p> </li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#service-management-ksm","title":"Service Management (KSM)","text":"<ul> <li> <p>ServiceTemplates     Declarative templates for deploying and managing applications and platform services consistently across multiple clusters</p> </li> <li> <p>ServiceTemplateChains     Enables conditional, chained service deployments with upgrade paths for complex service topologies</p> </li> <li> <p>Multi-Cluster Service Orchestration     Coordinate service deployments across multiple clusters with dependency management and ordering</p> </li> <li> <p>GitOps Integration     Native integration with ArgoCD and FluxCD for continuous deployment workflows</p> </li> <li> <p>Provider Templates     Infrastructure provider configurations that can be reused across multiple cluster deployments</p> </li> <li> <p>Global Values Support     Reuse consistent variables across ClusterTemplates and ProviderTemplates ensuring DRY, repeatable infrastructure definitions</p> </li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#networking-connectivity","title":"Networking &amp; Connectivity","text":"<ul> <li> <p>IPAM Controller Integration     Automates IP address management across environments, removing manual allocation and avoiding IP conflicts</p> </li> <li> <p>Multi-Cluster Networking     Service mesh integration with Istio for secure, observable communication between clusters</p> </li> <li> <p>Load Balancer Integration     Automated configuration of cloud and on-premises load balancers for cluster ingress</p> </li> <li> <p>Network Policy Management     Centralized network security policy enforcement across clusters</p> </li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#observability-monitoring-kof","title":"Observability &amp; Monitoring (KOF)","text":"<ul> <li> <p>NVIDIA GPU Monitoring     Native Grafana dashboards for AI/ML workloads providing visibility into GPU utilization without additional configuration</p> </li> <li> <p>Kube API Server Metrics     OpenTelemetry-based monitoring of API server health with immediate insights into control plane performance</p> </li> <li> <p>VictoriaMetrics Log Cluster     Scalable logging stack with high-retention support, improved storage efficiency and performance</p> </li> <li> <p>Cluster Annotation Support for Promxy/Datasource Config     Allows per-cluster HTTP configuration customization</p> </li> <li> <p>Custom Resource Limits for Observability Stack     Fine-tune Grafana and VictoriaMetrics components to optimize memory and CPU usage across environments</p> </li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#enterprise-solutions","title":"Enterprise Solutions","text":"<p>Mirantis provides validated and fully-supported solutions for Mirantis k0rdent Enterprise, addressing a range of use cases. Mirantis k0rdent Enterprise v1.0.0 introduces three integrated solutions designed to work together:</p> <ul> <li> <p>Mirantis k0rdent Virtualization</p> <p>A commercially supported virtualization platform based on KubeVirt that enable you to run virtual machines alongside containers in Kubernetes clusters. It provides VM lifecycle management, live migration, storage integration with Ceph, and enterprise-grade support for hybrid workloads.</p> </li> <li> <p>Ceph Storage</p> <p>Automated deployment and management of Ceph distributed storage clusters on k0rdent child clusters using <code>ServiceTemplate</code> objects. Ceph povides scalable, reliable storage that is ideal for the virtual machine workloads in Mirantis k0rdent Virtualization, with support for both block and object storage needs, enterprise-grade data protection, and performance optimization.</p> </li> <li> <p>Mirantis StackLight LMA</p> <p>Comprehensive monitoring solution originally designed for Kubernetes clusters running on bare metal or VMs, now distributed as a Kubernetes Operator and installed as a Helm chart. Provides enterprise-grade observability with prebuilt dashboards, advanced analytics, and intelligent alerting capabilities specifically optimized for Mirantis k0rdent Virtualization environments. Includes log aggregation, metric collection, distributed tracing, and anomaly detection.</p> </li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#component-provider-versions","title":"Component &amp; Provider Versions","text":"Component / Provider Version Cluster API v1.9.7 CAPI Provider AWS v2.8.2 CAPI Provider Azure v1.19.4 CAPI Provider Docker v1.9.6 CAPI Provider GCP v1.8.1 CAPI Provider Infoblox v0.1.0-alpha.8 CAPI Provider IPAM v0.18.0 CAPI Provider k0smotron v1.5.2 CAPI Provider OpenStack (ORC) v0.12.3 / v2.1.0 CAPI Provider vSphere v1.13.0 CAPI Provider Metal3 v1.8.0 Project Sveltos v0.54.0 Mirantis k0rdent Virtualization v1.0.0 Ceph v18.2.0 Mirantis StackLight LMA v1.0.0"},{"location":"release-notes/release-notes-v1.0.0/#platform-benefits","title":"Platform Benefits","text":"<ul> <li>Declarative, template-driven provisioning of clusters and services across multiple infrastructure types</li> <li>Comprehensive observability built-in, including GPU, API metrics, and enterprise-grade monitoring</li> <li>GitOps-ready: compatible with ArgoCD, FluxCD, Velero, and other CNCF tools</li> <li>Unified Kubernetes-native APIs with long-term schema stability</li> <li>Enterprise support with SLA guarantees and professional services</li> <li>Enhanced security features for regulated environments</li> <li>Bare metal and airgapped deployment capabilities for edge and secure environments</li> <li>Integrated virtualization platform for hybrid container/VM workloads</li> <li>Enterprise-grade storage solutions with Ceph integration</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#installation-requirements","title":"Installation Requirements","text":""},{"location":"release-notes/release-notes-v1.0.0/#standard-installation","title":"Standard Installation","text":"<ul> <li>Kubernetes 1.28+ management cluster</li> <li>Minimum 8GB RAM, 4 CPU cores</li> <li>100GB storage for management cluster</li> <li>Network connectivity to target infrastructure providers</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#airgapped-installation","title":"Airgapped Installation","text":"<ul> <li>Pre-configured container registry accessible from management cluster</li> <li>Downloaded k0rdent Commercial airgap bundle (contact Mirantis for access)</li> <li>Offline Helm chart repository</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#upgrade-notes","title":"Upgrade Notes","text":"<ul> <li>Ensure all <code>ClusterTemplate</code>, <code>ServiceTemplate</code>, and <code>ProviderTemplate</code> definitions are updated to use <code>apiVersion: v1beta1</code>.</li> <li>Custom tooling or integrations built on v1alpha1 resources must be updated.</li> <li>We recommend creating a Velero backup of your management cluster before upgrade.</li> <li>Use helm upgrade <code>--reuse-values</code> to preserve your existing configuration.</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#deprecations","title":"Deprecations","text":"<ul> <li>All <code>v1alpha1</code> APIs are now deprecated and will be removed in a future release.</li> <li>Support for the legacy <code>loki-stack</code> logging backend has been removed. Migrate to the <code>victoria-log-cluster</code> stack.</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#known-issues","title":"Known Issues","text":"<ul> <li>Grafana dashboards may take up to 60 seconds to initialise after cluster deployment.</li> <li>MultiClusterService priority conflicts may require manual resolution if priorities are equal.</li> <li>In high-latency networks, IPAM reconciliation can be delayed.</li> <li>Velero restore across cloud providers may require exclusion of specific resources (see docs).</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#notable-fixes","title":"Notable Fixes","text":"<ul> <li>Fixed invalid CR references in OpenStack/IPAM providers      [#1522, #1496]</li> <li>Improved e2e test reliability and configuration fetching      [#1517, #1463]</li> <li>Fixed resource tuning for VM services      [#279]</li> <li>Corrected Istio remote secret creation      [#270]</li> <li>Addressed Helm/YQ compatibility issue (2-arg enforcement)      [#282]</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#release-metadata","title":"Release Metadata","text":"Key Value Helm Charts kcm: 1.0.0, kof: 1.0.0, ksm: 1.0.0 OCI Registry registry.mirantis.com/k0rdent SBOM Included with commercial license OCI Signature Support Included Release Tags v1.0.0 across all components Support Level Commercial with SLA"},{"location":"release-notes/release-notes-v1.0.0/#contributors","title":"Contributors","text":"<p>Special thanks to the Mirantis engineering team and the broader k0rdent community for making this release possible: @gmlexx, @denis-ryzhkov, @ramessesii2, @aglarendil, @kylewuolle, @a13x5, @eromanova, @zerospiel, @BROngineer, @cdunkelb</p>"},{"location":"release-notes/release-notes-v1.0.0/#resources","title":"Resources","text":"<ul> <li>Documentation</li> <li>Mirantis Support Portal</li> <li>Professional Services</li> <li>Enterprise Support: support@mirantis.com</li> </ul>"},{"location":"release-notes/release-notes-v1.0.0/#get-started","title":"Get Started","text":"<p>You can try Mirantis kordent Enterprise easily using one of our QuickStarts.</p>"},{"location":"release-notes/release-notes-v1.1.0/","title":"Mirantis k0rdent Enterprise v1.1.0 Release Notes","text":"<p>Released: July 29, 2025</p> <p>Mirantis k0rdent Enterprise builds on the upstream, community-driven k0rdent OSS project to provide a commercially supported, enterprise-grade environment for managing Kubernetes clusters, services, and observability. While the open source k0rdent delivers core functionality under the Apache 2.0 license, Mirantis k0rdent Enterprise adds hardened components, tested integrations, and enterprise-only features\u2014including a fully-featured UI, the ability to add a custom certificate authority, and bare metal provisioning.</p>"},{"location":"release-notes/release-notes-v1.1.0/#component-provider-versions","title":"Component &amp; Provider Versions","text":"Component / Provider Version Cluster API v1.10.3 CAPI Provider AWS v2.8.2 CAPI Provider Azure v1.19.4 CAPI Provider Docker v1.9.6 CAPI Provider GCP v1.9.0 CAPI Provider Infoblox v0.1.0-alpha.8 CAPI Provider IPAM v1.0.2 CAPI Provider k0smotron v1.5.4 CAPI Provider OpenStack (ORC) v0.12.3 / v2.1.0 CAPI Provider vSphere v1.13.0 Project Sveltos v0.57.2"},{"location":"release-notes/release-notes-v1.1.0/#new-features","title":"\ud83d\ude80 New Features \ud83d\ude80","text":"<ul> <li>Include a new, fully-featured User Interface</li> <li>Provide the ability to specify a custom Certificate Authority</li> <li>Support for OpenStack-based Hosted Control Plane</li> <li>Pass certificate secret to the default helm repository (#1558)</li> <li>Ability to set k0s dl url arch (#1682)</li> <li>K0s cp/worker flags (#1681)</li> <li>Adapt openstack template to mount CA cert (#1624)</li> <li>Add support for GitRepository via fluxcd source. (#1631)</li> <li>Add support for mounting registry and k0s URL certificates (#1595)</li> <li>Mount registry certificate secret in CAPI operator (#1580)</li> <li>Pass registry CA cert to hosted CP components (#1659)</li> <li>StateManagementProvider API and controller (#1489)</li> <li>Add non-root volumes to AWS Templates (#1567)</li> <li>Automatically update stuck sveltos tokens (#1588)</li> <li>Disable v1alpha1 api (#1678)</li> <li>Propagate registry-credentials across cld (#1598)</li> <li>Switching to upstream PrometheusRules at promxy and regional with patches for all/specific clusters (#248)</li> <li>Add server to kof-operator for prometheus observability (#275)</li> <li>Add configurable UI port setting (#314)</li> <li>ContainerHighMemoryUsage alert for CAPI Operator and others (#317)</li> <li>Configure Grafana SSO using Dex (#319)</li> <li>Add autoinstrumentation to kof operator to collect metrics and traces (#344)</li> <li>Custom image registries PRs and resolved conflicts (#348)</li> <li>Sync kof operator resources when cluster annotation changes (#340)</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#improvements","title":"\u2728 Improvements \u2728","text":"<ul> <li>Add tests for kof operator UI (#299)</li> <li>Bump <code>vite</code> from 6.2.0 to 6.2.7 (#306)</li> <li>Use latest KCM release for CI (#308)</li> <li>Add adopted regional cluster deployment (#309)</li> <li>Add kof-ui docs (#313)</li> <li>Align chart versions; sveltos dashboard bump to 0.54.0 (#316, #318)</li> <li>Add adopted child cluster deployment (#321)</li> <li>Replace collector event receiver (#327)</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#upgrade-notes","title":"\u2757 Upgrade Notes \u2757","text":"<ul> <li>After upgrading KOF, please run:</li> </ul> <pre><code>kubectl apply --server-side --force-conflicts \\\n  -f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre> <ul> <li>And run the same for each regional cluster:</li> </ul> <pre><code>kubectl get secret -n kcm-system $REGIONAL_CLUSTER_NAME-kubeconfig \\\n  -o=jsonpath={.data.value} | base64 -d &gt; regional-kubeconfig\n\nKUBECONFIG=regional-kubeconfig kubectl apply --server-side --force-conflicts \\\n  -f https://github.com/grafana/grafana-operator/releases/download/v5.18.0/crds.yaml\n</code></pre> <p>This is noted as required in the grafana-operator release notes.</p>"},{"location":"release-notes/release-notes-v1.1.0/#notable-changes","title":"\u2728 Notable Changes \u2728","text":"<ul> <li>Remove namespace from AWSClusterStaticIdentity reference (#1585)</li> <li>Align vsphere-standalone tpl, fn names, logs, packages (#1683)</li> <li>Auto-update all configs (#1650)</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#notable-fixes","title":"\ud83d\udc1b Notable Fixes \ud83d\udc1b","text":"<ul> <li>Flapping delete conditions (#1653)</li> <li>CAPI operator registry cert mount volume mismatch (#1621)</li> <li>Correct the k0s version of aws-standalone-cp (#1623)</li> <li>Ensure services are removed before ClusterDeployment removal (#1584)</li> <li>IPAM claim inline processing (#1652)</li> <li>Rework the helm-push Makefile target (#1520, #1524)</li> <li>Set configure-cloud-routes=false in gcp ccm parameters (#1540)</li> <li>Add CertSecretRef to registry configuration (#1630)</li> <li>Check if services from self are running before deleting ClusterDeployment (#1648)</li> <li>Don't expect operator provider in ProviderTemplates (#1638)</li> <li>Global values for azure, capi, k0smotron providers (#1593)</li> <li>Helm values for servicetemplates not showing in status.config (#1544)</li> <li>Image paths inaccuracies (#1615)</li> <li>Properly preserve meta during copying (#1604)</li> <li>Use global.registry in orc templates (#1566)</li> <li>Addon-controller ServiceMonitor watches wrong namespace (#290)</li> <li>Filter out projectsveltos_* metrics from kcm cm (#291)</li> <li>Errors on upgrade of kof ServiceTemplates (#295)</li> <li>Logs sorting order (#301)</li> <li>Remove shadcn add command causing unwanted file updates (#307)</li> <li>Temporary adaptation of new alerts to current metrics (#310)</li> <li>Workaround for <code>generatorURL</code> in alerts and \"See source\" in Grafana (#312)</li> <li>Too many open files in <code>sveltos-dashboard</code> (#320)</li> <li>Use correct namespace name for MCS (#332)</li> <li>Update grafana to fix CVE-2025-4123 (#339)</li> <li>Allow to parametrise operators and ingress-nginx values (#337)</li> <li>Make Sveltos follow cluster updates (#333)</li> <li>Pinned versions of Grafana plugins and Promxy as part of aig-gap solution (#349)</li> <li>Duplicate <code>version</code> field in Grafana (#350)</li> <li>Pattern dist not found on <code>kof-operator-release</code> (#353, #354)</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#deprecations-and-removals","title":"Deprecations and Removals","text":"<ul> <li>All <code>v1alpha1</code> APIs are have previously been deprecated v1.0.0 and have now been removed.</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#known-issues","title":"Known Issues","text":"<ul> <li>None</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#release-metadata","title":"Release Metadata","text":"Key Value Helm Charts kcm: 1.1.0, kof: 1.1.0, ksm: 1.1.0 OCI Registry registry.mirantis.com/k0rdent-enterprise/ SBOM Included OCI Signature Support Included Release Tags v1.1.0 across all components"},{"location":"release-notes/release-notes-v1.1.0/#contributors","title":"Contributors","text":"<p>Huge thanks to the following contributors for making this release possible: @gmlexx, @denis-ryzhkov, @aglarendil, @kylewuolle, @a13x5, @eromanova, @zerospiel, @BROngineer, @Kshatrix, @dis-xcom, @wahabmk, @AndrejsPon00</p>"},{"location":"release-notes/release-notes-v1.1.0/#resources","title":"Resources","text":"<ul> <li>Documentation</li> </ul>"},{"location":"release-notes/release-notes-v1.1.0/#try-it-out","title":"Try It Out","text":"<p>QuickStart guide: https://docs.k0rdent-enterprise.io/v1.1.0/quickstarts/</p>"},{"location":"support/","title":"Mirantis Support and Services","text":"<p>Mirantis stands behind Mirantis k0rdent Enterprise with Enterprise support offerings including:</p> <ul> <li>Mirantis k0rdent Enterprise fully-managed support</li> <li>Mirantis k0rdent Enterprise 24x7 support</li> </ul> <p>... Plus Deployment and Onboarding services for implementations from PoCs to complex enterprise production setups.</p> <p>Existing customers requiring support can log in to the Mirantis CloudCare Portal.</p> <p>To inquire about support, please visit our Enterprise-Grade Cloud Native and Kubernetes Support page or contact us for more information.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>There are some situations in which you will need to take extra care to make sure  things run smoothly.</p> <ul> <li>Inspecting k0rdent events</li> <li>AWS VPCs</li> <li>EKS</li> <li>GCP</li> <li>Custom CA Certificates</li> </ul>"},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/","title":"Troubleshooting AWS VPCs","text":""},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/#aws-vpc-not-removed-when-deleting-eks-cluster","title":"AWS VPC Not Removed When Deleting EKS Cluster","text":"<p>A bug has been fixed in CAPA (Cluster API Provider AWS) for VPC removal: kubernetes-sigs/cluster-api-provider-aws#5192</p> <p>If you find that a VPC has not been deleted, you can deal with it in three different ways:</p>"},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/#applying-ownership-information-on-vpcs","title":"Applying ownership information on VPCs","text":"<p>When VPCs have owner information, all AWS resources will be removed when the Mirantis k0rdent Enterprise EKS cluster is deleted. So after provisioning an EKS cluster, the operator can go and set tags (for example, <code>tag:Owner</code>) and it will be  sufficient for CAPA to manage them.</p>"},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/#guardduty-vpce","title":"GuardDuty VPCE","text":"<p>Another way to prevent an issue with non-deleted VPCs is to disable GuardDuty. GuardDuty creates an extra VPCE (VPC Endpoint) not managed by CAPA and when CAPA  starts EKS cluster removal, this VPCE does not get removed.</p>"},{"location":"troubleshooting/admin-troubleshooting-aws-vpcs/#manual-removal-of-vpcs","title":"Manual removal of VPCs","text":"<p>When it is impossible to turn off GuardDuty or applying ownership tags is not permitted, you need to remove VPCs manually. Follow these steps.</p> <ol> <li> <p>Look for the affected VPC. The sign of \u201cstuck\u201d VPC looks like a hidden \u201cDelete\u201d button. </p> </li> <li> <p>Open \u201cNetwork Interfaces\u201d and attempt to detach an interface. You will see a disabled \u201cDetach\u201d button: </p> </li> <li> <p>Go to the VPC endpoints screen and remove the end-point:  </p> </li> <li> <p>OK the Endpoint deletion: </p> </li> <li> <p>Wait until the VPCE is completely removed and all network interfaces disappear. </p> </li> <li> <p>Now you can finally remove the VPC: </p> </li> </ol>"},{"location":"troubleshooting/events/","title":"Events","text":"<p>Kubernetes <code>events</code> are important system-level messages that record state changes, warnings, or other significant occurrences related to cluster components. They provide valuable insight into what is happening inside your cluster and are often essential for troubleshooting and debugging.</p> <p>In the context of Mirantis k0rdent Enterprise, the KCM (k0rdent Controller Manager) generates events for all major operations and reflects the current system state. If a Mirantis k0rdent Enterprise installation fails or behaves unexpectedly, these events can help identify the root cause.</p> <p>To retrieve all events generated by the KCM controller using the <code>kubectl</code> CLI, run:</p> <pre><code>kubectl get events --all-namespaces --field-selector reportingComponent=kcm-controller-manager\n</code></pre>"},{"location":"troubleshooting/events/#event-locations","title":"Event Locations","text":"<ul> <li>Events related to cluster-scoped objects (e.g., <code>Management</code>, <code>AccessManagement</code>, <code>Release</code>, etc.) are stored   in the <code>default</code> namespace.</li> <li>Events related to namespace-scoped objects (e.g., <code>ClusterDeployment</code>) are stored in the same namespace as the   corresponding object.</li> </ul>"},{"location":"troubleshooting/events/#viewing-events-for-specific-resources","title":"Viewing Events for Specific Resources","text":"<p>To view events for a Management:</p> <pre><code>kubectl events --for management/kcm\n</code></pre> <p>To view events for a specific ClusterDeployment:</p> <pre><code>kubectl events -n &lt;cluster-deployment-namespace&gt; --for clusterdeployment/&lt;cluster-deployment-name&gt;\n</code></pre>"},{"location":"troubleshooting/known-issues-custom-ca/","title":"Helm Extensions Cannot Be Pulled from Private Registry on Hosted ClusterDeployments","text":"<p>Related issue: KCM #1612</p> <p>Note</p> <p> The issue is resolved in k0s versions <code>v1.32.6+k0s.0</code>, <code>v1.33.2+k0s.0</code>.</p> <p>When deploying Hosted ClusterDeployments on a management cluster configured with a custom container registry, and the registry uses a certificate signed by an unknown certificate authority, Helm extensions (e.g., CCM or CSI drivers) may fail to install.</p> <p>You may encounter an error similar to the following in the logs of the hosted cluster controller pods:</p> <pre><code>can''t locate chart `oci://172.19.125.101:5001/charts/vsphere-csi-driver-0.0.3`:\nfailed to do request: Head \"https://172.19.125.101:5001/v2/charts/vsphere-csi-driver/manifests/0.0.3\":\ntls: failed to verify certificate: x509: certificate signed by unknown authority\n</code></pre> <p>As a result, the Helm extension fails to install.</p> <p>The controller cannot verify the registry\u2019s TLS certificate because the required custom CA certificate is not trusted by the k0s environment.</p> <p>Support for supplying custom CA certificates for Helm extensions from OCI registries is being added in Allow providing CA certificate for helm extensions that use OCI registries. When using a k0s version that includes this fix, the workaround is no longer required.</p> <p>Workaround</p> <p>You can work around this issue by manually mounting the registry CA certificate into the hosted cluster controller\u2019s StatefulSet. The secret with the registry CA should already be present in your system namespace (as you configured <code>registryCertSecret</code> parameter):</p> <ol> <li>Patch the StatefulSet for the hosted cluster controller:</li> <li>Add a volume from the Secret</li> <li>Mount it inside the container at a known location (e.g., <code>/etc/ssl/certs/registry-ca.pem</code>)</li> </ol> <pre><code>kubectl patch statefulset kmc-&lt;CLUSTER_DEPLOYMENT_NAME&gt; \\\n  -n &lt;CLUSTER_DEPLOYMENT_NAMESPACE&gt; \\\n  --type='json' \\\n  -p='[\n    {\n      \"op\": \"add\",\n      \"path\": \"/spec/template/spec/volumes/-\",\n      \"value\": {\n        \"name\": \"registry-ca\",\n        \"secret\": {\n          \"secretName\": \"&lt;REGISTRY_SECRET_NAME&gt;\",\n          \"items\": [\n            {\n              \"key\": \"ca.crt\",\n              \"path\": \"registry-ca.pem\"\n            }\n          ]\n        }\n      }\n    },\n    {\n      \"op\": \"add\",\n      \"path\": \"/spec/template/spec/containers/0/volumeMounts/-\",\n      \"value\": {\n        \"name\": \"registry-ca\",\n        \"mountPath\": \"/etc/ssl/certs/registry-ca.pem\",\n        \"subPath\": \"registry-ca.pem\",\n        \"readOnly\": true\n      }\n    }\n  ]'\n</code></pre>"},{"location":"troubleshooting/known-issues-eks/","title":"EKS Machines Are not Created: ControlPlaneIsStable Preflight Check Failed","text":"<p>Related issue: KCM #907</p> <p>The deployment of the EKS cluster is stuck waiting for the machines to be provisioned. The <code>MachineDeployment</code> resource is showing the following conditions:</p> <pre><code>Type: MachineSetReady\nStatus: False\nReason: PreflightCheckFailed\nMessage: ekaz-eks-dev-eks-md: AWSManagedControlPlane kcm-system/ekaz-eks-dev-eks-cp is provisioning (\"ControlPlaneIsStable\" preflight check failed)\nType: Available\nStatus: False\nReason: WaitingForAvailableMachines\nMessage: ekaz-eks-dev-eks-md: Minimum availability requires 1 replicas, current 0 available\nType: Ready\nStatus: False\nReason: WaitingForAvailableMachines\nMessage: ekaz-eks-dev-eks-md: Minimum availability requires 1 replicas, current 0 available\n</code></pre> <p>As a result, the cluster was successfully created in EKS but no nodes are available.</p> <p>Workaround</p> <ol> <li> <p>Edit the <code>MachineDeployment</code> object: <pre><code>kubectl --kubeconfig &lt;management-kubeconfig&gt; edit MachineDeployment -n &lt;cluster-namespace&gt; &lt;cluster-name&gt;-md\n</code></pre></p> </li> <li> <p>Add <code>machineset.cluster.x-k8s.io/skip-preflight-checks: \"ControlPlaneIsStable\"</code> annotation to skip the <code>ControlPlaneIsStable</code> preflight check: <pre><code>apiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: aws-eks-dev\n    meta.helm.sh/release-namespace: kcm-system\n    machineset.cluster.x-k8s.io/skip-preflight-checks: \"ControlPlaneIsStable\" # add new annotation\n  name: aws-eks-dev-md\n</code></pre></p> </li> <li> <p>Save and exit</p> </li> </ol>"},{"location":"troubleshooting/known-issues-gcp/","title":"GCP ClusterDeployment Deletion Stuck: Resource Is in Use by Another Resource","text":"<p>Related issue: KCM #1476</p> <p>When deleting a GCP cluster, the process can become stuck during the VPC deletion phase. The following error may appear in the <code>cluster-api-provider-gcp</code> logs:</p> <pre><code>E0505 13:33:29.573486       1 controller.go:324] \"Reconciler error\" err=\"googleapi:\nError 400: RESOURCE_IN_USE_BY_ANOTHER_RESOURCE - The network resource 'projects/k0rdent-dev/global/networks/network-name'\nis already being used by 'projects/k0rdent-dev/global/routes/kubernetes-f85faa80-9d44-4a73-a679-97aa072ecb4e'\"\ncontroller=\"gcpcluster\" controllerGroup=\"infrastructure.cluster.x-k8s.io\" controllerKind=\"GCPCluster\"\nGCPCluster=\"kcm-system/gcp-ekaz\" namespace=\"kcm-system\" name=\"gcp-ekaz\" reconcileID=\"74d04828-3892-4ad5-80c9-dd9187857c6e\"\n</code></pre> <p>This typically occurs when orphaned routes remain in the VPC and reference deleted compute instances.</p> <p>As a result, the <code>ClusterDeployment</code> object cannot be fully deleted until these route resources are removed.</p> <p>Workaround</p> <p>Manually delete all orphaned routes using the <code>gcloud</code> CLI or the Google Cloud Console. For example, using the <code>gcloud</code> CLI:</p> <ol> <li> <p>Identify the orphaned route from the <code>cluster-api-provider-gcp</code> logs (e.g., <code>kubernetes-f85faa80-9d44-4a73-a679-97aa072ecb4e</code>).</p> </li> <li> <p>List all routes in the affected network:</p> <pre><code>gcloud compute routes list --filter=\"network:&lt;networkName&gt;\"\n</code></pre> <p>Example output for the network <code>test-net</code>: <pre><code>NAME                                             NETWORK   DEST_RANGE     NEXT_HOP                                                           PRIORITY\ndefault-route-r-e39cfb79ee7071c0                 test-net  10.128.0.0/20  test-net                                                           0\ndefault-route-r-ffb0b56ff2dc7b8a                 test-net  10.212.0.0/20  test-net                                                           0\nkubernetes-24a016be-92d0-4298-abec-772bf45ba189  test-net  10.244.4.0/24  europe-west4-a/instances/test-ksi-gcp-18apr-20-n2d-cp-2            1000\nkubernetes-3ec1f296-768b-445d-b280-23560bed3ed1  test-net  10.244.2.0/24  europe-west4-a/instances/test-ksi-gcp-18apr-20-n2d-md-rqr5q-968gx  1000\n</code></pre></p> </li> <li> <p>Delete orphaned routes where the instance in the NEXT_HOP column no longer exists:</p> <pre><code>gcloud compute routes delete &lt;routeName&gt;\n</code></pre> <p>Repeat for each orphaned route to allow the VPC to be deleted successfully.</p> </li> </ol>"},{"location":"user/","title":"Mirantis k0rdent Enterprise User Guide","text":"<p>This guide walks you through the basics of using Mirantis k0rdent Enterprise, such as spinning up your own Kubernetes clusters, and then gets into adding extras, such as tools, applications, and services, to those clusters. We'll show you how to use templates to make that easier, add those extras to your deployments, and keep an eye on everything. Plus, we'll cover how to make sure your clusters avoid configuration drift. Basically, it's all about getting your apps running smoothly with Mirantis k0rdent Enterprise.</p> <ul> <li>Creating clusters as a Mirantis k0rdent Enterprise user</li> <li>Creating and adding services to clusters as a Mirantis k0rdent Enterprise user </li> <li>Enabling drift detection</li> </ul>"},{"location":"user/user-create-cluster/","title":"Deploying a Cluster","text":"<p>Mirantis k0rdent Enterprise simplifies the process of deploying and managing Kubernetes clusters across various cloud platforms through the use of <code>ClusterDeployment</code> objects, which include all of the information Mirantis k0rdent Enterprise needs to know in order to create the cluster you want. This <code>ClusterDeployment</code> system relies on predefined templates and credentials. </p> <p>A cluster deployment typically involves:</p> <ol> <li>Credentials for the infrastructure provider (such as AWS, vSphere, and so on).</li> <li>A template that defines the desired cluster configuration (for example, number of nodes or instance types).</li> <li>Submitting the configuration for deployment and monitoring the process.</li> </ol> <p>Follow these steps to deploy a standalone Kubernetes cluster:</p> <ol> <li> <p>Obtain the <code>Credential</code> object</p> <p>Mirantis k0rdent Enterprise needs credentials to communicate with the infrastructure provider (for example, AWS, Azure, or vSphere). These credentials enable Mirantis k0rdent Enterprise to provision resources such as virtual machines, networking components, and storage.</p> <p><code>Credential</code> objects are generally created ahead of time and made available to users. You can see all of the existing <code>Credential</code> objects by querying the management cluster:</p> <p><pre><code>kubectl get credentials -n accounting\n</code></pre> When you find a <code>Credential</code> that looks appropriate, you can get more information by <code>describe</code>-ing it, as in:</p> <pre><code>kubectl describe credential accounting-cluster-credential -n accounting\n</code></pre> <p>You'll see the YAML for the <code>Credential</code> object, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: Credential\nmetadata:\n  name: accounting-cluster-credential\n  namespace: accounting\nspec:\n  description: \"Credentials for Accounting AWS account\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: accountingd-cluster-identity\n</code></pre> <p>As you can see, the <code>.spec.description</code> field gives more information about the <code>Credential</code>.</p> <p>If the <code>Credential</code> you need doesn't yet exist, you can ask your cloud administrator to create it, or you can follow the instructions in the Credential System, as well as the specific instructions for your target infrastructure, to create it yourself.</p> <p>Tip</p> <p>Double-check to make sure that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Select a Template</p> <p>Templates in Mirantis k0rdent Enterprise are predefined configurations that describe how to set up the cluster. Templates include details such as:</p> <ul> <li>The number and type of control plane and worker nodes.</li> <li>Networking settings.</li> <li>Regional deployment preferences.</li> </ul> <p>Templates act as a blueprint for creating a cluster. To see the list of available templates, use the following command:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-1-0-1           true\nkcm-system   aws-eks-1-0-2                   true\nkcm-system   aws-hosted-cp-1-0-11             true\nkcm-system   aws-standalone-cp-1-0-12         true\nkcm-system   azure-aks-1-0-1                 true\nkcm-system   azure-hosted-cp-1-0-13           true\nkcm-system   azure-standalone-cp-1-0-13       true\nkcm-system   docker-hosted-cp-1-0-2          true\nkcm-system   gcp-gke-1-0-3                   true\nkcm-system   gcp-hosted-cp-1-0-12             true\nkcm-system   gcp-standalone-cp-1-0-12         true\nkcm-system   openstack-standalone-cp-1-0-11   true\nkcm-system   remote-cluster-1-0-11            true\nkcm-system   vsphere-hosted-cp-1-0-10         true\nkcm-system   vsphere-standalone-cp-1-0-11     true\n</code></pre></p> <p>You can then get information on the actual template by describing it, as in:</p> <pre><code>kubectl describe clustertemplate aws-standalone-cp-1-0-12 -n kcm-system\n</code></pre> </li> <li> <p>Create a ClusterDeployment YAML Configuration</p> <p>Once you have the <code>Credential</code> and the <code>ClusterTemplate</code> you can create the <code>ClusterDeployment</code> object configuration.  It includes:</p> <ul> <li>The template to use.</li> <li>The credentials for the infrastructure provider.</li> <li>Optional customizations such as instance types, regions, and networking.</li> </ul> <p>Create a <code>ClusterDeployment</code> configuration in a YAML file, following this structure:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>You will of course want to replace the placeholders with actual values. (For more information about <code>dryRun</code> see Understanding the Dry Run) For example, this is a simple AWS infrastructure provider <code>ClusterDeployment</code>:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-1-0-12\n  credential: aws-credential\n  config:\n    clusterLabels: {}\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> Note that the <code>.spec.credential</code> value should match the <code>.metadata.name</code> value of a created <code>Credential</code> object.</p> </li> <li> <p>Apply the Configuration</p> <p>Once the <code>ClusterDeployment</code> configuration is ready, apply it to the Mirantis k0rdent Enterprise management cluster:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits your deployment request to Mirantis k0rdent Enterprise. </p> </li> <li> <p>Verify Deployment Status</p> <p>After submitting the configuration, verify that the <code>ClusterDeployment</code> object has been created successfully:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output shows the current status and any errors.</p> </li> <li> <p>Monitor Provisioning</p> <p>Mirantis k0rdent Enterprise will now start provisioning resources (for example, VMs or networks) and setting up the cluster. To monitor this process, run:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> </li> <li> <p>Retrieve the Kubernetes Configuration</p> <p>When provisioning is complete, you can retrieve the kubeconfig file for the new cluster so you can interact with the cluster using <code>kubectl</code>:</p> <p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> You can then use this file to access the cluster, as in:</p> <pre><code>export KUBECONFIG=kubeconfig\nkubectl get pods -A\n</code></pre> <p>Store the kubeconfig file securely, as it contains authentication details for accessing the cluster.</p> </li> </ol>"},{"location":"user/user-create-cluster/#cleanup","title":"Cleanup","text":"<p>When you're finished you'll want to remove the cluster. Because the cluster is represented by the <code>ClusterDeployment</code> object, deleting the cluster is a simple matter of deleting that object.  For example:</p> <pre><code>kubectl delete clusterdeployment &lt;cluster-name&gt; -n kcm-system\n</code></pre> <p>Note that even though the Kubernetes object is deleted immediately, it will take a few minutes for the actual resources to be removed.</p>"},{"location":"user/user-enable-drift-detection/","title":"Detecting and Correcting Drift","text":""},{"location":"user/user-enable-drift-detection/#how-drift-detection-and-correction-works","title":"How Drift Detection and Correction Works","text":"<p>The drift-detection-manager watches for the deployed helm chart resources (that is, the resources deployed via a <code>ServiceTemplate</code>) and if it detects any changes  in the spec of the resources based on hash value, it updates the status of the <code>ResourceSummary</code> object. This change triggers the addon-controller in the <code>projectsveltos</code> namespace in the management cluster to update the status of the associated <code>ClusterSummary</code> object, which then triggers a reconcile to  re-deploy the spec to the target cluster.</p> <p>Note</p> <p>The <code>ResourceSummary</code> and <code>ClusterSummary</code> are CRDs provided by Sveltos.</p>"},{"location":"user/user-enable-drift-detection/#enabling-drift-detection","title":"Enabling Drift Detection","text":"<p>Set <code>.spec.serviceSpec.syncMode=Continuous</code> in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object to enable drift detection and correction. Sveltos will then automatically deploy the drift-detection-manager on the targeted clusters:</p> <p><pre><code>kubectl -n projectsveltos get deployments.apps \n</code></pre> <pre><code>NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ndrift-detection-manager   1/1     1            1           152m\nsveltos-agent-manager     1/1     1            1           152m\n</code></pre></p>"},{"location":"user/user-enable-drift-detection/#using-drift-ignore","title":"Using Drift Ignore","text":"<p>Certain resources can be completely opted out of drift correction by using this feature. In the following example, the \"ingress-nginx/ingress-nginx-controller\" deployment is ignored for drift correction on the target cluster.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n      . . .\n    priority: 100\n    syncMode: ContinuousWithDriftDetection\n    driftIgnore:\n      - target:\n        group: apps\n        version: v1\n        kind: Deployment\n        name: ingress-nginx-controller\n        namespace: ingress-nginx\n  . . .\n</code></pre> <p>If we manually remove the <code>app.kubernetes.io/managed-by=Helm</code> label, we can observe that the drift is not corrected as can be seen in the following watch output.</p> <p><pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller --show-labels -w\n</code></pre> <pre><code>NAME                       READY   UP-TO-DATE   AVAILABLE   AGE     LABELS\ningress-nginx-controller   3/3     3            3           3h58m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx,app.kubernetes.io/version=1.11.0,helm.sh/chart=ingress-nginx-4.11.0\ningress-nginx-controller   3/3     3            3           3h59m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx,app.kubernetes.io/version=1.11.0,helm.sh/chart=ingress-nginx-4.11.0\n</code></pre></p> <p>This can also be verified by observing that <code>ignoreForConfigurationDrift: true</code> is set for the targeted resource in the <code>ResourceSummary</code> spec on the target cluster.</p> <pre><code>kind: ResourceSummary\nmetadata:\n  . . .\nspec:\n  chartResources:\n  - chartName: ingress-nginx\n    group:\n    . . .\n    - group: apps\n      ignoreForConfigurationDrift: true\n      kind: Deployment\n      name: ingress-nginx-controller\n      namespace: ingress-nginx\n      version: v1\n    releaseName: ingress-nginx\n    releaseNamespace: ingress-nginx\nstatus:\n  helmResourceHashes:\n  . . .\n</code></pre> <p>Yet another way to check if a resource is being ignored for drift is by verifying that the <code>projectsveltos.io/driftDetectionIgnore: ok</code> annotation has been applied to it, as in:</p> <pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller -o=jsonpath='{.metadata.annotations}'\n{\"deployment.kubernetes.io/revision\":\"1\",\"meta.helm.sh/release-name\":\"ingress-nginx\",\"meta.helm.sh/release-namespace\":\"ingress-nginx\",\"projectsveltos.io/driftDetectionIgnore\":\"ok\"}%\n</code></pre>"},{"location":"user/user-enable-drift-detection/#removing-drift-ignore","title":"Removing Drift Ignore","text":"<p>The drift ignore setting can be removed by removing the <code>.spec.serviceSpec.driftIgnore</code> field.</p>"},{"location":"user/user-enable-drift-detection/#using-drift-exclusions","title":"Using Drift Exclusions","text":"<p>Certain fields of a resource can be excluded from drift detection using this feature. In the following example, the <code>.spec.replicas</code> field of the <code>ingress-nginx/ingress-nginx-controller</code> deployment on the target cluster is excluded from drift detection.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n      . . .\n    priority: 100\n    syncMode: ContinuousWithDriftDetection\n    driftExclusions:\n      - paths:\n        - \"/spec/replicas\"\n        target:\n          kind: Deployment\n          name: ingress-nginx-controller\n          namespace: ingress-nginx\n  . . .\n</code></pre> <p>If we manually edit the replicas to be 1, the number of replicas is not corrected back to 3 as is indicated by the following watch output.</p> <pre><code>kubectl -n ingress-nginx get deployments.apps ingress-nginx-controller -o=jsonpath='{.spec.replicas}' -w\n3111\n</code></pre> <p>We can also verify that this is the case by observing that the <code>ResourceSummary</code> object has the following patch in its spec now.</p> <pre><code>kind: ResourceSummary\nmetadata:\n  . . .\nspec:\n  chartResources:\n  - chartName: ingress-nginx\n    group:\n    . . .\n    releaseName: ingress-nginx\n    releaseNamespace: ingress-nginx\n  patches:\n  - patch: |-\n      - op: remove\n        path: /spec/replicas\n    target:\n      kind: Deployment\n      name: ingress-nginx-controller\n      namespace: ingress-nginx\nstatus:\n  helmResourceHashes:\n  . . .\n</code></pre>"},{"location":"user/user-enable-drift-detection/#removing-drift-exclusion","title":"Removing Drift Exclusion","text":"<p>The drift exclusion can be removed by removing the <code>.spec.serviceSpec.driftExclusion</code> field and re-triggering the drift correction by editing any field in the \"ingress-nginx/ingress-nginx-controller\" deployment. This will force a drift correction and since the drift exclusion has been removed, it will restore the deployment to it's original spec.</p>"},{"location":"user/services/","title":"Deploy Services to a Managed Cluster","text":"<p>At its heart, everything in Mirantis k0rdent Enterprise is based on templates that help define Kubernetes objects. For clusters, these are <code>ClusterTemplate</code> objects. For applications and services, these are <code>ServiceTemplate</code> objects.</p> <p>You can find numerous useful applications in the k0rdent Catalog, or you can create them yourself.</p> <ul> <li>Understanding ServiceTemplates</li> <li>Adding a Service to a ClusterDeployment</li> <li>Beach Head Services</li> <li>Checking Status</li> <li>Remove Beach Head Services</li> <li>ServiceTemplate Parameters</li> <li>Service Upgrades and ServiceTemplateChain</li> </ul>"},{"location":"user/services/add-service-to-clusterdeployment/","title":"Adding a <code>Service</code> to a <code>ClusterDeployment</code>","text":"<p>To add the service defined by this template to a cluster, you simply add it to the <code>ClusterDeployment</code> object when you create it, as in:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-1-0-12\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: project-ingress-nginx-4.11.0\n        name: ingress-nginx\n        namespace: tenant42\n    priority: 100\n</code></pre> As you can see, you're simply referencing the template in the <code>.spec.serviceSpec.services[].template</code> field of the <code>ClusterDeployment</code> to tell Mirantis k0rdent Enterprise that you want this service to be part of this cluster.</p> <p>If you wanted to add this service to an existing cluster, you would simply patch the definition of the <code>ClusterDeployment</code>, as in:</p> <pre><code>kubectl patch clusterdeployment my-cluster-deployment -n tenant42 --type='merge' -p '\nspec:\n  serviceSpec:\n    services:\n      - template: project-ingress-nginx-4.11.0\n        name: ingress-nginx\n        namespace: tenant42\n</code></pre> <p>Let's look at a more complex case, involving deploying beach-head services on a single cluster.</p>"},{"location":"user/services/beach-head/","title":"Deployment of beach-head services","text":"<p>Beach-head services can be installed on a cluster deployment (that is, a target cluster) using the <code>ClusterDeployment</code> object, just as with a single service. Consider the following example of a <code>ClusterDeployment</code> object for AWS Infrastructure Provider with beach-head services.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: kcm-system\nspec:\n  config:\n    clusterLabels: {}\n    clusterIdentity:\n      name: aws-cluster-identity\n      namespace: kcm-system\n    controlPlane:\n      amiID: ami-0eb9fdcf0d07bd5ef\n      instanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: t3.small\n      controlPlaneNumber: 1\n      publicIP: true\n      region: ca-central-1\n    worker:\n      amiID: ami-0eb9fdcf0d07bd5ef\n      instanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: t3.small\n    workersNumber: 1\n  credential: aws-cluster-identity-cred\n  serviceSpec:\n    services:\n      - template: kyverno-3-2-6\n        name: kyverno\n        namespace: kyverno\n      - template: ingress-nginx-4-11-3\n        name: ingress-nginx\n        namespace: ingress-nginx\n    priority: 100\n  template: aws-standalone-cp-1-0-12\n</code></pre> <p>In the example above, the fields under <code>serviceSpec</code> are relevant to the deployment of beach-head services.</p> <p>Note</p> <p> Refer to the Template Guide for more detail about these fields.</p> <p>This example <code>ClusterDeployment</code> object deploys kyverno and ingress-nginx, as referred to by their service templates respectively, on the target cluster.  As before, the <code>ServiceTemplate</code> includes information on the service. For example, here is the <code>ServiceTemplate</code> for kyverno:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  annotations:\n    helm.sh/resource-policy: keep\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      version: 3.2.6\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-catalog\n</code></pre> <p>The <code>k0rdent-catalog</code> helm repository hosts the actual kyverno chart version 3.2.6. For more details see the Bring your own Templates guide.</p>"},{"location":"user/services/beach-head/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Helm values can be passed to each beach-head service with the <code>.spec.serviceSpec.services[].values</code> field in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object. For example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - template: ingress-nginx-4-11-3\n      name: ingress-nginx\n      namespace: ingress-nginx\n      values: |\n        ingress-nginx:\n          controller:\n            replicaCount: 3\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n      values: |\n        kyverno:\n          admissionController:\n            replicas: 3\n    - name: motel-regional\n      namespace: motel\n      template: motel-regional-0-1-1\n      values: |\n        victoriametrics:\n          vmauth:\n            ingress:\n              host: vmauth.kcm0.example.net\n            credentials:\n              username: motel\n              password: motel\n        grafana:\n          ingress:\n            host: grafana.kcm0.example.net\n   . . .\n</code></pre> </p> <p>Note</p> <p>The values for ingress-nginx and kyverno start with the \"ingress-nginx:\" and \"kyverno:\" keys respectively because the helm charts used by the ingress-nginx-4-11-3 and kyverno-3-2-6 <code>ServiceTemplate</code> objects use the official upstream helm charts for ingress-nginx and kyverno as dependencies.</p>"},{"location":"user/services/beach-head/#templating-custom-values","title":"Templating Custom Values","text":"<p>Using the Sveltos templating feature, we can also write templates that can be useful for automatically fetching pre-existing information within the cluster. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - template: motel-1-1-0\n      name: motel\n      namespace: motel\n    - template: myappz-0-3-0\n      name: myappz\n      namespace: myappz\n      values: |\n        controlPlaneEndpointHost: {{ .Cluster.spec.controlPlaneEndpoint.host }}\n        controlPlaneEndpointPort: \"{{ .Cluster.spec.controlPlaneEndpoint.port }}\"\n    priority: 100\n    . . .        \n</code></pre> <p>In this case, the host and port information will be fetched from the spec of the CAPI cluster that hosts this <code>ClusterDeployment</code>.</p>"},{"location":"user/services/checking-status/","title":"Checking status","text":"<p>The <code>.status.services</code> field of the <code>ClusterDeployment</code> object shows the status for each of the beach-head services. For example, if you were to <code>describe</code> the <code>ClusterDeployment</code> with these services, you would see conditions that show status information, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 1\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    - name: kyverno\n      namespace: kyverno\n      template: kyverno-3-2-6\n    . . .\nstatus:\n  . . .\n  observedGeneration: 1\n  services:\n  - clusterName: my-cluster-deployment\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre> <p>Based on the information above both kyverno and ingress-nginx are installed in their respective namespaces on the target cluster. You can check to see for yourself:</p> <p><pre><code>kubectl get pod -n kyverno\n</code></pre> <pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\nkyverno-admission-controller-96c5d48b4-sg5ts     1/1     Running   0          2m39s\nkyverno-background-controller-65f9fd5859-tm2wm   1/1     Running   0          2m39s\nkyverno-cleanup-controller-848b4c579d-ljrj5      1/1     Running   0          2m39s\nkyverno-reports-controller-6f59fb8cd6-s8jc8      1/1     Running   0          2m39s\n</code></pre> <pre><code>kubectl get pod -n ingress-nginx \n</code></pre> <pre><code>NAME                                       READY   STATUS    RESTARTS   AGE\ningress-nginx-controller-cbcf8bf58-zhvph   1/1     Running   0          24m\n</code></pre></p> <p>Youc an get more information on how to access the child cluster in the create a cluster deployment chapter, and more on <code>ServiceTemplate</code> objects in the Template Guide.</p>"},{"location":"user/services/remove-beach-head/","title":"Removing beach-head services","text":"<p>To remove a beach-head service simply remove its entry from <code>.spec.serviceSpec.services</code>. The example below removes <code>kyverno-3-2-6</code>, so its status also removed from <code>.status.services</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 2\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  serviceSpec:\n    services:\n    - name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n    priority: 100\n    . . .\nstatus:\n  . . .\n  observedGeneration: 2\n  services:\n  - clusterName: wali-aws-dev\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre>"},{"location":"user/services/service-upgrade/","title":"Upgrading and rolling back deployed services","text":""},{"location":"user/services/service-upgrade/#service-version-upgrade","title":"Service Version Upgrade","text":"<p>When you deploy a service to a cluster, you can specify a <code>ServiceTemplateChain</code> that will be used to define available upgrade path for the service. </p> <p>Info</p> <p> Before you begin, make sure all templates you're going to add to <code>ServiceTemplateChain</code> exist in system namespace (normally <code>kcm-system</code>). Templates can be propagated to other namespaces using Template Life Cycle Management.</p> <p>First, you need to create a <code>ServiceTemplateChain</code> object:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplateChain\nmetadata:\n  name: ingress-nginx-chain\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: ingress-nginx-4-11-3\n      availableUpgrades:\n      - name: ingress-nginx-4-11-5\n    - name: ingress-nginx-4-11-5\n</code></pre> <p>This object defines a chain of templates that can be used to upgrade the service.</p> <p>Warning</p> <p> The <code>ServiceTemplateChain</code> has immutable spec. You can't change it after it's created.</p> <p>After <code>ServiceTemplateChain</code> is created, you can use it in <code>ClusterDeployment</code> or <code>MutliClusterService</code> objects to define the available upgrade path for the service:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-1-0-12\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: ingress-nginx-4-11-3\n        templateChain: ingress-nginx-chain\n        name: ingress-nginx\n        namespace: tenant42\n    priority: 100\n</code></pre> <p>Warning</p> <p> If no <code>templateChain</code> is specified for the service, the service cannot be upgraded because no path is availble. If you try to change the service template, in the logs, you'll see an error message such as:</p> <pre><code>service ingress-nginx/ingress-nginx can't be upgraded from ingress-nginx-4-11-3 to ingress-nginx-4-11-5\n</code></pre> <p>After the <code>ClusterDeployment</code> or <code>MultiClusterService</code> has been reconciled, you will see available upgrade paths for the service in the status:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  ...\nstatus:\n  servicesUpgradePaths:\n    - availableUpgrades:\n        - upgradePaths:\n            - ingress-nginx-4-11-5\n      name: ingress-nginx\n      namespace: ingress-nginx\n      template: ingress-nginx-4-11-3\n</code></pre> <p>Now you can update the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object to upgrade the service to the available version:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster-deployment\n  namespace: tenant42\nspec:\n  config:\n    clusterLabels: {}\n  template: aws-standalone-cp-1-0-12\n  credential: aws-credential\n  serviceSpec:\n    services:\n      - template: ingress-nginx-4-11-5 # &lt;-- upgrade to the latest version\n        templateChain: ingress-nginx-chain\n        name: ingress-nginx\n        namespace: tenant42\n    priority: 100\n</code></pre>"},{"location":"user/services/service-upgrade/#service-version-rollback","title":"Service Version Rollback","text":"<p>In general, the process of rolling back a service to the previous version is the same as upgrading the service in the first place. You'll need to create a separate <code>ServiceTemplateChain</code>, which defines the downgrade path:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplateChain\nmetadata:\n  name: ingress-nginx-chain\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: ingress-nginx-4-11-3\n    - name: ingress-nginx-4-11-5\n      availableUpgrades:\n        - name: ingress-nginx-4-11-3\n</code></pre> <p>After the <code>ServiceTemplateChain</code> has been created, you can use it in a <code>ClusterDeployment</code> or <code>MutliClusterService</code> object to define the available rollback path for the service. Follow these steps:</p> <ol> <li>Update the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object with the rollback <code>ServiceTemplateChain</code>.</li> <li>Wait for the <code>ClusterDeployment</code> or <code>MultiClusterService</code> to be reconciled.</li> <li>Update the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object with the previous version of the service.</li> </ol>"},{"location":"user/services/servicetemplate-parameters/","title":"Parameter List","text":"<p>Here is an idea of the parameters involved.</p> Parameter Example Description <code>.spec.serviceSpec.syncMode</code> <code>Continuous</code> Specifies how beach-head services are synced i the target cluster (default:<code>Continuous</code>) <code>.spec.serviceSpec.DriftIgnore</code> specifies resources to ignore for drift detection <code>.spec.serviceSpec.DriftExclusions</code> specifies specific configurations of resources to ignore for drift detection <code>.spec.serviceSpec.priority</code> <code>100</code> Sets the priority for the beach-head services defined in this spec (default: <code>100</code>) <code>.spec.serviceSpec.stopOnConflict</code> <code>false</code> Stops deployment of beach-head services upon first encounter of a conflict (default: <code>false</code>) <code>.spec.serviceSpec.services[].template</code> <code>kyverno-3-2-6</code> Name of the <code>ServiceTemplate</code> object located in the same namespace <code>.spec.serviceSpec.services[].name</code> <code>my-kyverno-release</code> Release name for the beach-head service <code>.spec.serviceSpec.services[].namespace</code> <code>my-kyverno-namespace</code> Release namespace for the beach-head service (default: <code>.spec.services[].name</code>) <code>.spec.serviceSpec.services[].values</code> <code>replicas: 3</code> Helm values to be used with the template while deployed the beach-head services <code>.spec.serviceSpec.services[].valuesFrom</code> `` Can reference a ConfigMap or Secret containing helm values <code>.spec.serviceSpec.services[].disable</code> <code>false</code> Disable handling of this beach-head service (default: <code>false</code>)"},{"location":"user/services/understanding-servicetemplates/","title":"Understanding ServiceTemplates","text":"<p><code>ServiceTemplate</code> objects are meant to let Mirantis k0rdent Enterprise know where to find a Helm chart with instructions for installing an application. In many cases, these charts will be in a private repository.  For example, consider this template for installing Nginx Ingress:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplate\nmetadata:\n  name: project-ingress-nginx-4.11.0\n  namespace: tenant42\nspec:\n  helm:\n    chartSpec:\n      chart: demo-ingress-nginx\n      version: 4.11.0\n      interval: 10m0s\n      sourceRef:\n        kind: HelmRepository\n        name: k0rdent-demos\n---\napiVersion: k0rdent.mirantis.com/v1beta1\nkind: ServiceTemplateChain\nmetadata:\n  name: project-ingre`ServiceTemplate` objects are a representation of the source where Mirantis k0rdent Enterprise can find a resource or set of resources to\nbe deployed as a complete application.\n\n`ServiceTemplate` supports the following types as a source:\n\n- [`HelmChart`](https://fluxcd.io/flux/components/source/helmcharts/)\n- [`GitRepository`](https://fluxcd.io/flux/components/source/gitrepositories/)\n- [`Bucket`](https://fluxcd.io/flux/components/source/buckets/)\n- [`OCIRepository`](https://fluxcd.io/flux/components/source/ocirepositories/)\n- `Secret`\n- `ConfigMap`\n\n### Helm-based ServiceTemplate\n\nHelm-based `ServiceTemplate` can be created in two ways:\n\n- by defining Helm chart right in the template object\n\n  ```yaml\n  apiVersion: k0rdent.mirantis.com/v1beta1\n  kind: ServiceTemplate\n  metadata:\n    name: foo\n    namespace: bar\n  spec:\n    helm:\n      chartSpec:\n        chart: ingress-nginx\n        version: 4.11.0\n        interval: 10m\n        sourceRef:\n          kind: HelmRepository\n          name: foo-repository\n  ```\n\n  In this case the corresponding `HelmChart` object will be created by the controller.\n\n- by referring the existing Helm chart\n\n  ```yaml\n  apiVersion: k0rdent.mirantis.com/v1beta1\n  kind: ServiceTemplate\n  metadata:\n    name: foo\n    namespace: bar\n  spec:\n    helm:\n      chartRef:\n        kind: HelmChart\n        name: foo-chart\n  ```\n\n### Kustomize-based ServiceTemplate\n\nKustomize-based `ServiceTemplate` can be created with either local or remote source:\n\n- by using existing flux source object - `GitRepository`, `Bucket` or `OCIRepository` - or using existing `ConfigMap` or `Secret`\n\n  ```yaml\n  apiVersion: k0rdent.mirantis.com/v1beta1\n  kind: ServiceTemplate\n  metadata:\n    name: foo\n    namespace: bar\n  spec:\n    kustomize:\n      localSourceRef:\n        kind: Bucket  # also can be GitRepository, OCIRepository, ConfigMap or Secret\n        name: foo-bar\n      deploymentType: Remote\n      path: \"./base\"\n  ```\n\n  `ConfigMap` or `Secret` in this case must embed the tar-gzipped archive containing the kustomization files. This can be done by the following command, assuming the the archive was already created:\n\n  ```shell\n  kubectl create configmap foo-bar --from-file=/path/to/kustomization/archive.tar.gz\n  ```\n\n- by defining remote source right in the template object\n\n  ```yaml\n  apiVersion: k0rdent.mirantis.com/v1beta1\n  kind: ServiceTemplate\n  metadata:\n    name: kustomization-app\n    namespace: kcm-system\n  spec:\n    kustomize:\n      remoteSourceSpec:\n        oci:\n          url: oci://ghcr.io/org/project-x\n          reference:\n            tag: latest\n          interval: 10m\n      deploymentType: Remote\n      path: \"./overlays\"\n  ```\n\n  `.spec.kustomize.remoteSourceSpec` has mutual exclusive fields `.git`, `.bucket` and `.oci` which inline `GitRepositorySpec`, `BucketSpec` and `OCIRepositorySpec` respectively.\n\n### Raw-resources-based ServiceTemplate\n\nSimilar to kustomize-based `ServiceTemplate`, raw-resources-based `ServiceTemplate` can be created with either local or remote source. Using the remote source has no difference with\nkustomize-based `ServiceTemplate`, however using local source slightly differ in case `ConfigMap` or `Secret` object is referred as a source:\n\n- `spec.resources.localSourceRef.path` will be ignored\n- referred `ConfigMap` or `Secret` must contain inlined resources' definitions instead of embedding tar-gzipped archive. \nss-nginx-4.11.0\n  namespace: tenant42\nspec:\n  supportedTemplates:\n    - name: project-ingress-nginx-4.11.0\n    - name: project-ingress-nginx-4.10.0\n      availableUpgrades:\n        - name: project-ingress-nginx-4.11.0\n</code></pre> <p>Here you see a template called <code>project-ingress-nginx-4.11.0</code> that is meant to be deployed in the <code>tenant42</code> namespace. The <code>.spec.helm.chartSpec</code> specifies the name of the Helm chart and where to find it, as well as the version and other  important information. The <code>ServiceTemplateChain</code> shows that this template is also an upgrade path from version 4.10.0.</p> <p>If you wanted to deploy this as an application, you would first go ahead and add the template to the cluster in which you were working, so if you were to save this YAML to a file called <code>project-ingress.yaml</code> you could run this command on the management cluster:</p> <pre><code>kubectl apply -f project-ingress.yaml -n tenant42\n</code></pre>"}]}